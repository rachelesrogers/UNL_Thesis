<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>CHAPTER 3 Text analysis with Transcripts | JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE</title>
  <meta name="description" content="CHAPTER 3 Text analysis with Transcripts | JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="CHAPTER 3 Text analysis with Transcripts | JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="CHAPTER 3 Text analysis with Transcripts | JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE" />
  
  
  

<meta name="author" content="Rachel Edie Sparks Rogers" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-study1.html"/>
<link rel="next" href="4-study2.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"></a></li>
<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Literature Review</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#bullet-matching-background"><i class="fa fa-check"></i><b>1.2</b> Bullet Matching Background</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#issues-in-pattern-analysis"><i class="fa fa-check"></i><b>1.3</b> Issues in Pattern Analysis</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#subjectivity-of-comparisons"><i class="fa fa-check"></i><b>1.3.1</b> Subjectivity of Comparisons</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#scientific-validity"><i class="fa fa-check"></i><b>1.3.2</b> Scientific Validity</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#scale-of-conclusions"><i class="fa fa-check"></i><b>1.3.3</b> Scale of Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#quantitative-methods"><i class="fa fa-check"></i><b>1.4</b> Quantitative Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#bullet-matching-algorithm"><i class="fa fa-check"></i><b>1.4.1</b> Bullet Matching Algorithm</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#quantitative-results"><i class="fa fa-check"></i><b>1.4.2</b> Quantitative Results</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#explainability-in-the-courtroom"><i class="fa fa-check"></i><b>1.4.3</b> Explainability in the Courtroom</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#demonstrative-evidence"><i class="fa fa-check"></i><b>1.4.4</b> Demonstrative Evidence</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#response-methods"><i class="fa fa-check"></i><b>1.5</b> Response Methods</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#conclusion"><i class="fa fa-check"></i><b>1.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-study1.html"><a href="2-study1.html"><i class="fa fa-check"></i><b>2</b> Jury Perception of Bullet Matching Algorithms and Demostrative Evidence</a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-study1.html"><a href="2-study1.html#background"><i class="fa fa-check"></i><b>2.1</b> Background</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2-study1.html"><a href="2-study1.html#firearms-examiners"><i class="fa fa-check"></i><b>2.1.1</b> Firearms Examiners</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-study1.html"><a href="2-study1.html#bullet-matching-algorithm-1"><i class="fa fa-check"></i><b>2.1.2</b> Bullet Matching Algorithm</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-study1.html"><a href="2-study1.html#explainable-machine-learning---previous-research"><i class="fa fa-check"></i><b>2.1.3</b> Explainable Machine Learning - Previous Research</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-study1.html"><a href="2-study1.html#demonstrative-evidence-1"><i class="fa fa-check"></i><b>2.1.4</b> Demonstrative Evidence</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-study1.html"><a href="2-study1.html#methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-study1.html"><a href="2-study1.html#study-format"><i class="fa fa-check"></i><b>2.2.1</b> Study Format</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-study1.html"><a href="2-study1.html#prolific"><i class="fa fa-check"></i><b>2.2.2</b> Prolific</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-study1.html"><a href="2-study1.html#results"><i class="fa fa-check"></i><b>2.3</b> Results</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-study1.html"><a href="2-study1.html#participants"><i class="fa fa-check"></i><b>2.3.1</b> Participants</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-study1.html"><a href="2-study1.html#overview"><i class="fa fa-check"></i><b>2.3.2</b> Overview</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-study1.html"><a href="2-study1.html#probability"><i class="fa fa-check"></i><b>2.3.3</b> Probability</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-study1.html"><a href="2-study1.html#credibility"><i class="fa fa-check"></i><b>2.3.4</b> Credibility</a></li>
<li class="chapter" data-level="2.3.5" data-path="2-study1.html"><a href="2-study1.html#reliability"><i class="fa fa-check"></i><b>2.3.5</b> Reliability</a></li>
<li class="chapter" data-level="2.3.6" data-path="2-study1.html"><a href="2-study1.html#scientificity"><i class="fa fa-check"></i><b>2.3.6</b> Scientificity</a></li>
<li class="chapter" data-level="2.3.7" data-path="2-study1.html"><a href="2-study1.html#understanding"><i class="fa fa-check"></i><b>2.3.7</b> Understanding</a></li>
<li class="chapter" data-level="2.3.8" data-path="2-study1.html"><a href="2-study1.html#uniqueness"><i class="fa fa-check"></i><b>2.3.8</b> Uniqueness</a></li>
<li class="chapter" data-level="2.3.9" data-path="2-study1.html"><a href="2-study1.html#strength"><i class="fa fa-check"></i><b>2.3.9</b> Strength</a></li>
<li class="chapter" data-level="2.3.10" data-path="2-study1.html"><a href="2-study1.html#mistakes"><i class="fa fa-check"></i><b>2.3.10</b> Mistakes</a></li>
<li class="chapter" data-level="2.3.11" data-path="2-study1.html"><a href="2-study1.html#comparing-algorithm-values-to-examiner-values"><i class="fa fa-check"></i><b>2.3.11</b> Comparing Algorithm Values to Examiner Values</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-study1.html"><a href="2-study1.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-study1.html"><a href="2-study1.html#summary-of-results"><i class="fa fa-check"></i><b>2.4.1</b> Summary of Results</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-study1.html"><a href="2-study1.html#limitations"><i class="fa fa-check"></i><b>2.4.2</b> Limitations</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-study1.html"><a href="2-study1.html#future-research"><i class="fa fa-check"></i><b>2.4.3</b> Future Research</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-textcolor.html"><a href="3-textcolor.html"><i class="fa fa-check"></i><b>3</b> Text analysis with Transcripts</a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-textcolor.html"><a href="3-textcolor.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="3-textcolor.html"><a href="3-textcolor.html#literature-review"><i class="fa fa-check"></i><b>3.1.1</b> Literature Review</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-textcolor.html"><a href="3-textcolor.html#implementation"><i class="fa fa-check"></i><b>3.2</b> Implementation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3-textcolor.html"><a href="3-textcolor.html#data-cleaning"><i class="fa fa-check"></i><b>3.2.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-textcolor.html"><a href="3-textcolor.html#note-analysis"><i class="fa fa-check"></i><b>3.2.2</b> Note Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-textcolor.html"><a href="3-textcolor.html#case-study"><i class="fa fa-check"></i><b>3.3</b> Case Study</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3-textcolor.html"><a href="3-textcolor.html#the-data"><i class="fa fa-check"></i><b>3.3.1</b> The Data</a></li>
<li class="chapter" data-level="3.3.2" data-path="3-textcolor.html"><a href="3-textcolor.html#data-cleaning-1"><i class="fa fa-check"></i><b>3.3.2</b> Data Cleaning</a></li>
<li class="chapter" data-level="3.3.3" data-path="3-textcolor.html"><a href="3-textcolor.html#final-output"><i class="fa fa-check"></i><b>3.3.3</b> Final Output</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-textcolor.html"><a href="3-textcolor.html#discussion-1"><i class="fa fa-check"></i><b>3.4</b> Discussion</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-textcolor.html"><a href="3-textcolor.html#conclusion-1"><i class="fa fa-check"></i><b>3.4.1</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-study2.html"><a href="4-study2.html"><i class="fa fa-check"></i><b>4</b> An Investigation of Response Types</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-study2.html"><a href="4-study2.html#background-1"><i class="fa fa-check"></i><b>4.1</b> Background</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="4-study2.html"><a href="4-study2.html#study-1-and-scale-compression"><i class="fa fa-check"></i><b>4.1.1</b> Study 1 and Scale Compression</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-study2.html"><a href="4-study2.html#study-visualization"><i class="fa fa-check"></i><b>4.1.2</b> Study Visualization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-study2.html"><a href="4-study2.html#methods-1"><i class="fa fa-check"></i><b>4.2</b> Methods</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4-study2.html"><a href="4-study2.html#study-format-1"><i class="fa fa-check"></i><b>4.2.1</b> Study Format</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-study2.html"><a href="4-study2.html#prolific-1"><i class="fa fa-check"></i><b>4.2.2</b> Prolific</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-study2.html"><a href="4-study2.html#results-1"><i class="fa fa-check"></i><b>4.3</b> Results</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-study2.html"><a href="4-study2.html#participants-1"><i class="fa fa-check"></i><b>4.3.1</b> Participants</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-study2.html"><a href="4-study2.html#questions-from-study-1"><i class="fa fa-check"></i><b>4.3.2</b> Questions from Study 1</a></li>
<li class="chapter" data-level="4.3.3" data-path="4-study2.html"><a href="4-study2.html#new-study-questions"><i class="fa fa-check"></i><b>4.3.3</b> New Study Questions</a></li>
<li class="chapter" data-level="4.3.4" data-path="4-study2.html"><a href="4-study2.html#scale-comparison"><i class="fa fa-check"></i><b>4.3.4</b> Scale Comparison</a></li>
<li class="chapter" data-level="4.3.5" data-path="4-study2.html"><a href="4-study2.html#demographic-comparison"><i class="fa fa-check"></i><b>4.3.5</b> Demographic Comparison</a></li>
<li class="chapter" data-level="4.3.6" data-path="4-study2.html"><a href="4-study2.html#notepad-analysis"><i class="fa fa-check"></i><b>4.3.6</b> Notepad Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-study2.html"><a href="4-study2.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-finalstudy.html"><a href="5-finalstudy.html"><i class="fa fa-check"></i><b>5</b> Conclusion - Initial Study Revisited</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-finalstudy.html"><a href="5-finalstudy.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-finalstudy.html"><a href="5-finalstudy.html#methods-2"><i class="fa fa-check"></i><b>5.2</b> Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5-finalstudy.html"><a href="5-finalstudy.html#study-format-2"><i class="fa fa-check"></i><b>5.2.1</b> Study Format</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-testimony-transcripts.html"><a href="A-testimony-transcripts.html"><i class="fa fa-check"></i><b>A</b> Testimony Transcripts</a>
<ul>
<li class="chapter" data-level="A.1" data-path="A-testimony-transcripts.html"><a href="A-testimony-transcripts.html#firearm-examiner"><i class="fa fa-check"></i><b>A.1</b> Firearm Examiner</a></li>
<li class="chapter" data-level="A.2" data-path="A-testimony-transcripts.html"><a href="A-testimony-transcripts.html#algorithm-expert"><i class="fa fa-check"></i><b>A.2</b> Algorithm Expert</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html"><i class="fa fa-check"></i><b>B</b> Study 2 Changes</a>
<ul>
<li class="chapter" data-level="B.1" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#cautions-against-expert-witnesses"><i class="fa fa-check"></i><b>B.1</b> Cautions Against Expert Witnesses</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#jury-instructions"><i class="fa fa-check"></i><b>B.1.1</b> Jury Instructions</a></li>
<li class="chapter" data-level="B.1.2" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#opinion-witness"><i class="fa fa-check"></i><b>B.1.2</b> Opinion Witness</a></li>
<li class="chapter" data-level="B.1.3" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#cross-examination"><i class="fa fa-check"></i><b>B.1.3</b> Cross Examination</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#clarifying-sides"><i class="fa fa-check"></i><b>B.2</b> Clarifying Sides</a></li>
<li class="chapter" data-level="B.3" data-path="B-study-2-changes.html"><a href="B-study-2-changes.html#images"><i class="fa fa-check"></i><b>B.3</b> Images</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="colophon.html"><a href="colophon.html"><i class="fa fa-check"></i>Colophon</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="textcolor" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">CHAPTER 3</span> Text analysis with Transcripts<a href="3-textcolor.html#textcolor" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction<a href="3-textcolor.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note taking is commonly used when individuals are provided with a large amount of information that they may want to reference later.
These notes can act as a guide in terms of what items of information the note taker found most important.
By fluidly connecting notes from multiple note takers to the information presented, one may be able to discover the most ‘noteworthy’ pieces of information, and compare the perception of the note takers to the goal of the presenter.
To this end, we have designed a method for highlighting presented information, in the form of a transcript, to correspond to the notes of participants in order to form a ‘heatmap’ for the information that participants found worth recording. This method can provide valuable information about what participants chose to, or not to, record.
There were two important steps to this process: the text cleaning of cumulative notes, and the mapping of phrase frequency to form a ‘heatmap’.
In this case, participants took notes on a single notepad, but their notes were saved multiple times throughout the study as they were presented with new information.
By isolating the notes taken at each time point through text cleaning, we can isolate notes corresponding to the presentation of specific information.
This was accomplished through a combination of two methods (First N Character and Longest Common Substring) used to identify which notes have been added, presumably corresponding to the new information.
These individualized notes can then be mapped to the information presented through the use of collocations, or phrases of a set length.
Once the frequency with which each phrase of information appears in participant notes is calculated, this frequency can be mapped to a corresponding color to create a highlighting gradient that illuminates areas of participant focus.
This method was developed and tested on a trial transcript study designed to assess juror perceptions of the use of algorithms and demonstrative evidence in the courtroom.</p>
<div id="literature-review" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Literature Review<a href="3-textcolor.html#literature-review" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="first-n-character-method" class="section level4 hasAnchor" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> First N Character Method<a href="3-textcolor.html#first-n-character-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The First n Character method relies on the concept of edit distance for matching and removing previous notes from sequential study documents.
I developed this method specifically for the cleaning of sequential notes.
In this process, a previous page’s notes are matched to the next page’s notes in order to remove duplicate text.
This is a form of pattern matching.
In a reference to algorithmic detection of plagiarism in computer programs, <span class="citation">Parker &amp; Hamblen (1989)</span> describe an algorithm that uses the character differences in order to compare how similar programs are.
Other algorithms they discuss use code-specific features such as number of operators, lines of comments, and number of various loop statements.
Because we are only concerned with the content of the notes, there is little information to be gained from formatting, as there is in this plagiarism detection in programs.
There are plenty of examples of pattern matching in text analysis - such as <span class="citation">Baeza-Yates &amp; Gonnet (1992)</span> and <span class="citation">Landau &amp; Vishkin (1988)</span>.
These papers rely on a set pattern to be found in the reference text, with a defined number of differences allowed between the pattern and the text.
According to <span class="citation">Baeza-Yates &amp; Gonnet (1992)</span>, many algorithmic methods have been developed to solve this problem, with allowances for mismatches between the reference text and the pattern.
A similar allowance for differences is present in <span class="citation">Landau &amp; Vishkin (1988)</span>, where differences are defined as either insertions, deletions, or substitutions.
This definition of differences is the same definition that is used in computing Levenshtein distance, also known as edit distance <span class="citation">(Levenshtein, 1966)</span>.
Levenshtein considered the issue of insertions, deletions, and substitutions with binary code.
This concept has later been extended to include more extensive strings, as demonstrated by <span class="citation">Konstantinidis (2005)</span>.
The edit distance can be used to determine the extent of matching text when comparing two sequential note sheets, in order to identify if a portion of notes should be removed.</p>
<table>
<caption>Transformations used to calculate Edit Distance <span id="tab:edist">Table 3.1: </span></caption>
<colgroup>
<col width="14%" />
<col width="15%" />
<col width="28%" />
<col width="15%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Word 1</th>
<th align="center">Word 2</th>
<th align="center">Transformation</th>
<th align="center">Method</th>
<th align="center">Edit Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Hat</td>
<td align="center">Hot</td>
<td align="center">a → o</td>
<td align="center">substitution</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">Hello there</td>
<td align="center">there</td>
<td align="center">Hello</td>
<td align="center">deletion</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">meow</td>
<td align="center">homeowner</td>
<td align="center">meow</td>
<td align="center">insertion</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">Shire</td>
<td align="center">Shareholder</td>
<td align="center">i → a + </td>
<td align="center">substitution and insertion</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<p>The edit distance is the number of characters that would need to be changed in order to transform the first string into the second string.
This includes insertions, deletions, and substitutions, in the case of the “adist” function <span class="citation">(R Core Team, 2023)</span>.
These different transformations are shown in Table <a href="3-textcolor.html#tab:edist">3.1</a>.
For example, “Hat” and “Hot” would have an edit distance of 1, because the strings match if the “a” is turned into an “o”.
This is an example of substitution.
Similarly, “Hello there” and “there” would have an edit distance of 6, since 6 characters are deleted (the letters in “Hello” plus an additional space).
Finally, “meow” and “homeowner” would have an edit distance of 5, because 5 characters are added.
These operations can also be combined in edit distance calculations.
For example, going from “Shire” to “Shareholder” would require both a substitution and insertion.</p>
<p>Although edit distance can be used to calculate the similarity between notes, there is no set pattern that guarantees removal.
While the analysis is based on the previous page of notes, individuals do not always keep the previous page of notes the same - some delete parts, and some add new information in the middle.
Instead of only aiming to remove exact matches of the previous note sheet, we must factor in partial and discontinuous notes as well.</p>
</div>
<div id="longest-common-substring-method" class="section level4 hasAnchor" number="3.1.1.2">
<h4><span class="header-section-number">3.1.1.2</span> Longest Common Substring Method<a href="3-textcolor.html#longest-common-substring-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Whereas calculating edit distance between two pages of notes is useful when finding notes that wholly match the previous page, the Longest Common Substring (LCS) method can be used to find pieces of notes that match between pages, so that this repeated text can be removed.
LCS accomplishes this goal through searching for common text between two sequential pages to be removed.
For example, in Figure <a href="3-textcolor.html#fig:lcs">3.1</a>, the LCS between the two strings is “the cat enjoys napping”.
In the process of note cleaning, this would be the string that is removed from the second page of notes, resulting in “When it is quiet” as the cleaned (non-repeated) notes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lcs"></span>
<img src="images/svg_graph.png" alt="Longest Common Substring Diagram" width="\linewidth" />
<p class="caption">
Figure 3.1: Longest Common Substring Diagram
</p>
</div>
<!-- @landau1988's discussion of pattern matching with k differences includes both a dynamic programming approach, as well as the construction of suffix trees. -->
<!-- \authorcol{In the dynamic programming approach, results are recorded in a table or matrix form, while the suffix tree approach organizes text into a tree format for performing calculations} [@dynamicvstree]. -->
<!-- These methods correspond to those proposed for finding the Longest Common Substring between two sequences. -->
<p>Methods for computing the longest common substring between two sequences have been used to compare biological sequences <span class="citation">(Crochemore, Iliopoulos, Langiu, &amp; Mignosi, 2017)</span>.
Solutions for finding the longest common substring usually the construction of a suffix tree <span class="citation">(Charalampopoulos, Kociumaka, Pissis, &amp; Radoszewski, 2021)</span> or table <span class="citation">(Landau &amp; Vishkin, 1988)</span>.
The suffix tree method is described by <span class="citation">Gusfield (1997)</span>; in short, each unique suffix of a string would contribute a new branch to the suffix tree, where leaves consist of the string’s terminal character: $ is used avoid recurrence with previous characters in the string.</p>
<p>A solution implemented by <span class="citation">Bielow, Mastrobuoni, &amp; Kempa (2016)</span> in a CRAN R package constructs a matrix that records the length of a matching string at character <span class="math inline">\(i\)</span> for string 1 (in the <span class="math inline">\(i\)</span>th row), and character <span class="math inline">\(j\)</span> for string 2 (in the <span class="math inline">\(j\)</span>th column).
Because this matrix method has an R implementation, it is utilized for this analysis.
However, <span class="citation">Bielow et al. (2016)</span> warns that this LCS method is inefficient, and meant to be used in small instances.
<!-- The comparison between suffix tree and dynamic programming methods will be evaluated in a future analysis. -->
This method creates an empty matrix, where the number of rows correspond to the length of the first string, and the number of columns correspond to the length of the second string.
Thus, the first cell would correspond to the first character of both strings, and so on.
If the characters match and it is the first row or column, then the cell gets a value of 1.
If the characters match and it is not the first row or column, the the cell receives the value of the (i-1,j-1) cell plus one.
The first string’s index for the substring of the longest length is saved.
This process is shown in Table <a href="3-textcolor.html#tab:dylcs">3.2</a>, with two strings: “cabbacced” and “bbacccade”.
The diagonal of numbers in red indicate the longest substring, beginning with ‘b’ and ending with ‘c’
Thus, “bbacc” is identified as the LCS.</p>
<table>
<caption>Dynamic Programming for LCS <span id="tab:dylcs">Table 3.2: </span></caption>
<thead>
<tr class="header">
<th>_</th>
<th>c</th>
<th>a</th>
<th>b</th>
<th>b</th>
<th>a</th>
<th>c</th>
<th>c</th>
<th>e</th>
<th>d</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>b</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>b</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>a</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>c</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>c</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>c</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>a</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>d</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>e</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<div id="collocation-analysis-and-fuzzy-matching" class="section level4 hasAnchor" number="3.1.1.3">
<h4><span class="header-section-number">3.1.1.3</span> Collocation Analysis and Fuzzy Matching<a href="3-textcolor.html#collocation-analysis-and-fuzzy-matching" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After the notes are cleaned, sequences of 5 words (collocations) are used to find areas of the testimony that participants focus on.
For indirect matches, such as typos, weights are applied to these fuzzy matches to contribute to the total count.
While collocations traditionally refer to words that occur more frequently together - such as “white house”(<span class="citation">Merriam-Webster (2023)</span>), tools for collocation analysis helpfully provide the frequency of occurrence for strings of a specified length (<span class="citation">Schweinberger (2022)</span>).</p>
<p>Character differences can effectively be used to indicate similarity between participants’ notes and the text transcript.
This is useful in situation where individuals do not copy the notes directly.
In these cases, fuzzy matching can be used to find the transcript collocation that is the most similar to the participants’ written notes.
This approximate string matching allows for matching strings that do not correspond directly (<span class="citation">Gusfield (1997)</span>), which allows for matching up participants’ inexact notes with the closest transcript collocation.</p>
<p>In order to perform fuzzy matching on the indirect collocations from participants’ notes, I used ‘stringdist_join’ from the ‘fuzzyjoin’ R package <span class="citation">(Robinson, 2020)</span>.
This function conducts a join with the best matching observations from the second dataset.
For example, consider asking young children to spell their birth month.
This may result in a dataset like Table <a href="3-textcolor.html#tab:months">3.3</a>.</p>
<table>
<caption>Dataset of misspelled months <span id="tab:months">Table 3.3: </span></caption>
<thead>
<tr class="header">
<th>Name</th>
<th>Month</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Billy</td>
<td>Mach</td>
</tr>
<tr class="even">
<td>Jimmy</td>
<td>Apil</td>
</tr>
<tr class="odd">
<td>Frances</td>
<td>Mae</td>
</tr>
<tr class="even">
<td>Gary</td>
<td>Febary</td>
</tr>
<tr class="odd">
<td>Steve</td>
<td>Octobr</td>
</tr>
<tr class="even">
<td>Bobby</td>
<td>Agust</td>
</tr>
<tr class="odd">
<td>Tammy</td>
<td>Febrey</td>
</tr>
<tr class="even">
<td>Greg</td>
<td>Agast</td>
</tr>
<tr class="odd">
<td>Benny</td>
<td>Jun</td>
</tr>
</tbody>
</table>
<p>We may then want to match the children’s written months to their correctly spelled counterparts.
While ‘stringdist_join’ will give values for all matches between the two datasets, we can use group_by and slice_min to find the month that is closest to the misspelled value.</p>
<table>
<caption><span id="tab:fuzzymonth">Table 3.4: </span>Fuzzy Match by Month</caption>
<thead>
<tr class="header">
<th align="left">month.x</th>
<th align="left">month.y</th>
<th align="left">name</th>
<th align="right">dist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">June</td>
<td align="left">Jun</td>
<td align="left">Benny</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">March</td>
<td align="left">Mach</td>
<td align="left">Billy</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">August</td>
<td align="left">Agust</td>
<td align="left">Bobby</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">May</td>
<td align="left">Mae</td>
<td align="left">Frances</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">February</td>
<td align="left">Febary</td>
<td align="left">Gary</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="left">August</td>
<td align="left">Agast</td>
<td align="left">Greg</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">April</td>
<td align="left">Apil</td>
<td align="left">Jimmy</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">October</td>
<td align="left">Octobr</td>
<td align="left">Steve</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">February</td>
<td align="left">Febrey</td>
<td align="left">Tammy</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<p>In Table <a href="3-textcolor.html#tab:fuzzymonth">3.4</a>, “month.x” is the correctly spelled month, “month.y” is the children’s spellings, and “dist” is the edit distance between the two words.
This same concept is applied to matching collocations between a transcript and notepad.
In this case, the transcript is considered the “correct” version, and the participants’ collocations that do not directly match are considered the misspellings.
This method finds the closest transcript collocation to the written collocation.</p>
</div>
</div>
</div>
<div id="implementation" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Implementation<a href="3-textcolor.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="data-cleaning" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Data Cleaning<a href="3-textcolor.html#data-cleaning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The FNC and LCS methods for sequential notepad cleaning were first tested on a subset of notes resulting from a jury perception survey.
In this study, participants were provided with a notepad to record information while reading through mock trial testimony.
Because participants are provided with the same continuous notepad throughout the study, the database records each page as a “screenshot” of what is written on their notes when they advance to the next page - this provides cumulative notes.</p>
<table>
<caption><span id="tab:noteexample">Table 3.5: </span>Participant Note Example</caption>
<colgroup>
<col width="7%" />
<col width="92%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">page_count</th>
<th align="left">notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
<td align="left">discharge firearm in business- felony, not guilty</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="left">discharge firearm in business- felony, not guilty</td>
</tr>
</tbody>
</table>
<p>ski mask. no money, no injuries. 9mm, arrested after confiscated and testing. |</p>
<p>In order to compare the FNC and LCS methods described above, I cleaned a subset of the survey response notes to have a “correct” baseline with which to calculate error rates.
This dataset consists of 561 pages of ‘notes’ (including 66 blanks) with 35 unique participants.
30 participants were randomly selected from the larger dataset, while 5 participants were selected based on demonstrated issues with data cleaning.
The 5 selected participants included an individual who deleted almost all of the beginning notes; two individuals who pasted new notes in the middle of old notes; an individual who duplicated notes; and an individual who included the algorithm expert’s trial confirmation as well as the forensic scientist’s - duplicate text that should be included as the new notes.
Table <a href="3-textcolor.html#tab:noteexample">3.5</a> is an example of what sequential notes may look like, taken from the second and third pages of a study participant’s notes.</p>
<p>In order to appropriately clean the notes, the duplicated text (“discharge firearm in business- felony, not guilty”) needs to be removed from the third page, to leave only notes that correspond to the third page’s testimony.
A generalized version of the functions used in this data cleaning are included in the <code>seqstrclean</code> package <span class="citation">(Rogers &amp; VanderPlas, 2024b)</span>.</p>
<div id="first-n-character-fnc-method" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> First n Character (FNC) Method<a href="3-textcolor.html#first-n-character-fnc-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In scenarios such as Table <a href="3-textcolor.html#tab:noteexample">3.5</a>, a straightforward method for note cleaning would be to measure the length of the previous page’s notes (n), and compare the previous notes to the first n characters in the current pages’ notes via edit distance.
If the edit distance is small (indicating a correspondence to the previous page’s notes), the first n characters can be removed.
If we consider Table <a href="3-textcolor.html#tab:noteexample">3.5</a>, it is clear to see how this method can be applied.
The page 2 notes perfectly line up with the first line in the page 3 notes, resulting in an edit distance of 0 when comparing the first n characters of the third page.
Thus, by removing this beginning section, we would be left with the notes that are unique to page 3 - namely, “ski mask. no money, no injuries. 9mm, arrested after confiscated and testing.”
The threshold of allowable disagreement between two strings can be controlled through setting the maximum allowable edit distance.
For this method comparison, I changed the threshold with values of 0 character difference, 5 character difference, and 15 character difference (set as less than 1, less than 6, and less than 16).</p>
<p>This method works well when participants take notes sequentially.
In some instances, however, participants will delete portions of their previous notes, add new notes either before or in the middle of their old notes, or duplicate their old notes.
When participants delete a portion of their notes or add new notes before/in the middle of their old notes, the edit distance between the old and new notes could be large.
The first n character method cannot appropriately clean these notes, as shown in Table <a href="3-textcolor.html#tab:issues">3.6</a>.</p>
</div>
<div id="longest-common-substring-lcs-method" class="section level4 hasAnchor" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Longest Common Substring (LCS) Method<a href="3-textcolor.html#longest-common-substring-lcs-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Longest Common Substring method is able to handle the issues found in the First n Character method, which is demonstrated in Table <a href="3-textcolor.html#tab:issues">3.6</a>.
If notes are deleted, the LCS method can identify the substring from the previous notes, despite the large edit distance that would prevent removal in the First n Character method.
If new notes are inserted in the middle of previous notes, the First n Character method would compare the Page 1’s notes with the red text of the Page 2 notes shown in the second row, resulting in an edit distance that would prevent removal.
With the LCS method, the first substring “The cat ran” could be identified and removed, then in a second iteration, the second substring of “up the tree” can also be removed, leaving only the new text.
In the case of duplication, the First n Character method would be able to identify the first iteration of text that matches Page 1 and remove it; but it would not be able to identify the second occurrence of the text.
As before, the LCS method would be able to identify both instances through two iterations, and remove all Page 1 text.
In the case of LCS, a threshold must be picked for the minimum matching substring length to be considered ‘previous notes’ and removed.
If the threshold is too small, the LCS method can identify words like “the” as matching previous testimony to be removed.
If the threshold is too large, the LCS method may be unable to remove substrings when new text is inserted in the middle of the old text.
Here, I tried several different character lengths: 40 characters, half the previous note length, a third of the previous note length, and a fourth of the previous note length plus 5 (to compensate for shorter strings).</p>
<table>
<caption>LCS Comparison for FNC Issues <span id="tab:issues">Table 3.6: </span></caption>
<colgroup>
<col width="18%" />
<col width="18%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th>Issue</th>
<th>Page 1</th>
<th>Page 2</th>
<th>LCS</th>
<th>Edit Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deletion</td>
<td>The cat ran up the tree</td>
<td>The cat ran</td>
<td>The cat ran</td>
<td>12</td>
</tr>
<tr class="even">
<td>Insertion</td>
<td>The cat ran up the tree</td>
<td> dog, up the tree</td>
<td>(The cat ran)(up the tree)</td>
<td>11</td>
</tr>
<tr class="odd">
<td>Duplication</td>
<td>The cat ran up the tree</td>
<td> The cat ran up the tree</td>
<td>(The cat ran up the tree)(The cat ran up the tree)</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>While the LCS method is able to solve the issues related to the First N Character method, it has some issues of its own.
If the testimony itself contains duplicate text, such as the text related to swearing in witnesses, the LCS method would remove the new text that corresponds to the later testimony.
The other issue with the LCS method is the greater computational complexity.</p>
</div>
<div id="hybrid-method" class="section level4 hasAnchor" number="3.2.1.3">
<h4><span class="header-section-number">3.2.1.3</span> Hybrid Method<a href="3-textcolor.html#hybrid-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>I also investigated the use of a hybrid method in the hope that the FNC method could be used as a quick way to clean the simplest cases of notes, while the LCS method could be used to clean up the notes that are less clear-cut.
In order to determine which observations would need the second cleaning method, I created Figure <a href="3-textcolor.html#fig:errorplot">3.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:errorplot"></span>
<img src="thesis_files/figure-html/errorplot-1.png" alt="Error Comparison" width="\linewidth" />
<p class="caption">
Figure 3.2: Error Comparison
</p>
</div>
<p>Here, the yellow circles indicate the FNC method with a 16 character threshold, while the black circles represent the LCS method with a threshold of 1/3 the previous character length.
Larger circles indicate a larger amount of error when compared to the hand-cleaned notes.
The y-axis shows the edit distance computed between the current and previous notes for the FNC method, while the x-axis indicates the length of the clean notes.
The horizontal line is the 16 character threshold for the FNC method.
As can be seen, most observations are located in the bottom left of the graph, below the FNC threshold, and have relatively small circles.
These are notes where the FNC method was effectively applied, meaning that the edit distance was small enough to remove duplicated text at the beginning of the document.
In the observations above the threshold, most have a larger error when the FNC method is applied compared to the LCS method, with the exception of one observation in the top left.
In that case, the error appeared to be equal for both methods.
There is also a larger FNC method error for the observation on the bottom right of the graph.
This was a case of duplicate text, resulting in abnormally long notes.
This graph shows that and edit distance the FNC method’s threshold and the length of the cleaned notes can be used as criterion for determining when the LCS method should be applied.
I chose to set a threshold of 4 standard deviations above the mean note length for cleaned notes of the page of interest to constitute “unusually long” notes to apply the LCS method to.
I chose 1/3 of the previous notes as the LCS threshold because the mean edit distance to the correct notes was comparable to the 1/4 + 5 character threshold, but it is more conservative for larger notes (as can be seen in Table <a href="3-textcolor.html#tab:cleancompare">3.7</a>).</p>
<table>
<caption><span id="tab:cleancompare">Table 3.7: </span>Note Cleaning Method Comparison</caption>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="center">Time (Minutes)</th>
<th align="center">Error</th>
<th align="center">SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">FNC 1 Character</td>
<td align="center">7.17</td>
<td align="center">498.96</td>
<td align="center">2769.83</td>
</tr>
<tr class="even">
<td align="left">FNC 6 Character</td>
<td align="center">3.74</td>
<td align="center">201.35</td>
<td align="center">1890.01</td>
</tr>
<tr class="odd">
<td align="left">FNC 16 Character</td>
<td align="center">3.43</td>
<td align="center">36.91</td>
<td align="center">411.25</td>
</tr>
<tr class="even">
<td align="left">LCS 40 Character</td>
<td align="center">79.51</td>
<td align="center">6.34</td>
<td align="center">35.50</td>
</tr>
<tr class="odd">
<td align="left">LCS 1/2 Previous Notes</td>
<td align="center">71.25</td>
<td align="center">7.29</td>
<td align="center">104.69</td>
</tr>
<tr class="even">
<td align="left">LCS 1/3 Previous Notes</td>
<td align="center">71.33</td>
<td align="center">3.06</td>
<td align="center">26.77</td>
</tr>
<tr class="odd">
<td align="left">LCS 1/4 Previous Notes + 5</td>
<td align="center">71.05</td>
<td align="center">2.82</td>
<td align="center">26.14</td>
</tr>
<tr class="even">
<td align="left">Hybrid (FNC 16 and LCS 1/3)</td>
<td align="center">4.52</td>
<td align="center">2.13</td>
<td align="center">24.62</td>
</tr>
</tbody>
</table>
<p>The results of this study are shown in Table <a href="3-textcolor.html#tab:cleancompare">3.7</a>.
Notably, the mean edit distance to the correct notes (or the error) is large for the FNC methods, with a high level of variation.
However, the computation time is, between 3 and 8 minutes.
In the case of the LCS method, the error is much lower, but the computation time is above an hour, taking approximately 17 times as long as the FNC method.
The hybrid method combines the benefits of both of these methods.
The error rate is on par with that of the LCS methods, while the time is only slightly above that of the FNC methods.
The flowchart in Figure <a href="3-textcolor.html#fig:flowchart">3.3</a> shows how this hybrid method was applied.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:flowchart"></span>
<img src="images/flowchart.jpg" alt="Hybrid Flowchart" width="\linewidth" />
<p class="caption">
Figure 3.3: Hybrid Flowchart
</p>
</div>
</div>
</div>
<div id="note-analysis" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Note Analysis<a href="3-textcolor.html#note-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="collocations" class="section level4 hasAnchor" number="3.2.2.1">
<h4><span class="header-section-number">3.2.2.1</span> Collocations<a href="3-textcolor.html#collocations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In order to analyze which areas of the text individuals focus on, we must first find a way to compare their notes to the written testimony.
One way to do this comparison is through collocation analysis, which uses the frequency of strings of sequentially-appearing words (n-grams) in order to identify words that tend to ‘stick together’.
For example, in the case of the phrase “The cat ran up the tree”, there would be three collocations of length 4, as shown in Table <a href="3-textcolor.html#tab:colex">3.8</a>).
By matching the collocations that appear in the testimony to a frequency of the collocation from the given notepad, we can get frequency values for all n-grams that occur in the testimony.
The functions for creating highlighted text can be found in the package <code>highlightr</code> <span class="citation">(Rogers &amp; VanderPlas, 2024a)</span>.</p>
<table>
<caption>Collocations in “The cat ran up the tree” <span id="tab:colex">Table 3.8: </span></caption>
<thead>
<tr class="header">
<th>Collocation</th>
<th>The</th>
<th>cat</th>
<th>ran</th>
<th>up</th>
<th>the</th>
<th>tree</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>The</td>
<td>cat</td>
<td>ran</td>
<td>up</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td></td>
<td>cat</td>
<td>ran</td>
<td>up</td>
<td>the</td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td></td>
<td></td>
<td>ran</td>
<td>up</td>
<td>the</td>
<td>tree</td>
</tr>
</tbody>
</table>
<p>In setting the size of the collocation, there are two important factors to keep in mind: the frequency with which a phrase appears in the transcript itself, and the number of words in a row that individuals may directly copy from the transcript.
If the number of words considered in a collocation is too short, a phrase may occur multiple times throughout the transcript, without a way to distinguish which specific sections participants were recording.
For example, if we used a collocation of length two, a common phrase in the courtroom testimony is “Richard Cole”, the name of the defendant.
Because this phrase occurs so frequently in the testimony, it does not by itself tell us if there are particular portions of the testimony that individuals find important, even if we add weights to calculate the relative frequency.
On the other hand, if we use a collocation of length 10, it is unlikely that a large amount of individuals copied the exact 10-word phrase, which results in fewer observations.
In considering these two issues, we decided on a collocation size of 5.
This would be long enough to avoid many common phrases such as “the bullet matching algorithm”, but short enough that individuals may copy the phrase down directly.
The implementation of collocations used the ‘quanteda’ package <span class="citation">(Benoit et al., 2018)</span>, with guidance from a tutorial by <span class="citation">Schweinberger (2022)</span>.</p>
<p>Collocations are identified in the transcript, and assigned ordered word numbers (so the first collocation of length 5 will have words 1-5, the second will have words 2-6, and so on).
This data structure is similar to that shown in Table <a href="3-textcolor.html#tab:colex">3.8</a>, where the collocation is length 4 instead of 5.
These are then bound with collocations from the participants’ notes based on the sequence of 5 words, in order to assign a frequency to each collocation.
We then take the average per word out of the possible collocations that the word could have appeared in.
For example, the first word can only appear in a single collocation (in Table <a href="3-textcolor.html#tab:colex">3.8</a>, “The” only apprears in the first collocation, while “ran” appears in all three), so its average will be equal to the frequency of the first collocation.
When multiple 5-grams appear in the same testimony page, the note frequency is divided by the number of appearances in the testimony in order to account for multiple occurrences.
The Figure <a href="#fig:colcount"><strong>??</strong></a> shows the highest frequencies for the second page of the testimony:</p>
<p>“In this case the defendant Richard Cole has been charged with willfully discharging a firearm in a place of business. This crime is a felony.
Mr. Cole has pleaded not guilty to the charge.
You will now hear a summary of the case.
This summary was prepared by an objective court clerk.
It describes all of the evidence that was presented at trial.”</p>
<table>
<caption><span id="tab:colcount">Table 3.9: </span>Collocation Count for Page 2</caption>
<thead>
<tr class="header">
<th align="right">count</th>
<th align="left">collocation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">134</td>
<td align="left">discharging a firearm in a</td>
</tr>
<tr class="even">
<td align="right">133</td>
<td align="left">firearm in a place of</td>
</tr>
<tr class="odd">
<td align="right">130</td>
<td align="left">a firearm in a place</td>
</tr>
<tr class="even">
<td align="right">129</td>
<td align="left">in a place of business</td>
</tr>
<tr class="odd">
<td align="right">121</td>
<td align="left">willfully discharging a firearm in</td>
</tr>
<tr class="even">
<td align="right">93</td>
<td align="left">with willfully discharging a firearm</td>
</tr>
<tr class="odd">
<td align="right">91</td>
<td align="left">charged with willfully discharging a</td>
</tr>
<tr class="even">
<td align="right">76</td>
<td align="left">been charged with willfully discharging</td>
</tr>
<tr class="odd">
<td align="right">75</td>
<td align="left">has been charged with willfully</td>
</tr>
<tr class="even">
<td align="right">74</td>
<td align="left">cole has been charged with</td>
</tr>
</tbody>
</table>
<p>The highlighted collocations in Figure <a href="#fig:colcount"><strong>??</strong></a> show all of the collocations for the word “willfully” from the testimony above.
The frequencies for ‘willfully’ can be averaged: <span class="math inline">\(\frac{121+93+91+76+75}{5}=91.2\)</span>.
This number will correspond to the shading of the word “willfully” in the highlighted testimony of Figure <a href="3-textcolor.html#fig:highlights">3.4</a>.</p>
<p>Note that the words have a gradient.
This is to ensure a smooth transition between words in order to give an overall “highlight” effect, because we are interested in the important phrases in the testimony as opposed to the importance of individual words.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:highlights"></span>
<img src="images/collocationanalysis.jpg" alt="Collocation Count for Page 2" width="\linewidth" />
<p class="caption">
Figure 3.4: Collocation Count for Page 2
</p>
</div>
<p>In order to create this effect, we first implemented a graph of the testimony in ‘ggplot2’ <span class="citation">(Wickham, 2016)</span> in order to assign color values to the frequencies based on a continuous gradient.
Then we used HTML gradient boxes, where the left color is that of the previous word, while the right color is based on the frequency of the current word.
This results in a smooth transition between words.</p>
</div>
<div id="fuzzy-matching" class="section level4 hasAnchor" number="3.2.2.2">
<h4><span class="header-section-number">3.2.2.2</span> Fuzzy Matching<a href="3-textcolor.html#fuzzy-matching" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>At times, participants do not directly copy a collocation, but the correspondence is clear enough to match their notes to the transcript.
In these cases, we can use fuzzy matching to account for indirect collocations.
Because the intent of the participant is more clear with simple typos (such as a single letter change) than with larger differences (such as the changing of multiple words), the count for these fuzzily matched observations are weighted based on the edit distance.
I set a maximum edit distance of 99 to be considered a match.
Specifically, a weight of <span class="math inline">\(\frac{1}{d+0.25}\)</span> is used, where <span class="math inline">\(d\)</span> represents the edit distance (between 1 and 99).
An edit distance of 0 would receive a weight of 1, since fuzzy matching need not be applied.
Another factor in the calculation is the number of matches per ‘misspelled’ collocation.
To return to the month example shown in Table <a href="3-textcolor.html#tab:fuzzymonth">3.4</a>, if an individual were to spell a month “Jur”, the edit distance to both “June” and “July” is 2.
It is then unclear which month the person was born in, but they most likely meant June or July, as opposed to the other months.
Similarly, some collocation notes had the same edit distance for multiple transcript collocations.
In these cases, the frequency of the ‘misspelled’ collocation notes are split evenly between the closest transcript collocations.
This results in a total fuzzy frequency per transcript collocation of:</p>
<p><span class="math display">\[
\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i}
\]</span>
where <span class="math inline">\(x_i\)</span> is the count for the participant’s note collocation <span class="math inline">\(i\)</span>, <span class="math inline">\(d_i\)</span> is the distance between the transcript collocation and the ‘misspelled’ collocation <span class="math inline">\(i\)</span>, and <span class="math inline">\(c_i\)</span> is the number of transcript collocation matches for note collocation <span class="math inline">\(i\)</span>.
This quantity is then added to the count of direct collocation matches calculated in the previous section.</p>
</div>
<div id="standardization" class="section level4 hasAnchor" number="3.2.2.3">
<h4><span class="header-section-number">3.2.2.3</span> Standardization<a href="3-textcolor.html#standardization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The courtroom transcript testimony analyzed comes from a 2x2x3 factorial design (presence or absence of images, presence or absence of the algorithm, and the conclusion: identification, inconclusive, or elimination).
Due to this design, the testimony diverges and merges in several places.
For clarity, when the testimony corresponds, the frequency for all scenarios are considered together.
However, when the testimony diverges, each scenario is considered separately.
In order to consistently evaluate the frequencies throughout the testimonies, the final frequency count was standardized by the number of scenarios included.
This would result in collocations with a final frequency weight of:</p>
<p><span class="math display">\[
(z+\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i})\frac{1}{ks}
\]</span></p>
<p>In this case, <span class="math inline">\(z\)</span> is the count of direct matches; <span class="math inline">\(\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i}\)</span> is the weighted fuzzy match count as defined above; <span class="math inline">\(k\)</span> is the number of times the collocation re-occurs in the transcript page itself; and <span class="math inline">\(s\)</span> is the number of scenarios included in the frequency count.
This standardization is included in Figure <a href="3-textcolor.html#fig:weightedhighlights">3.5</a>.
In comparing with the unweighted highlighting without fuzzy matching of Figure <a href="3-textcolor.html#fig:highlights">3.4</a>, the same pattern emerges in terms of phrase popularity: most individuals focused on the crime.
However, the numerical values of the gradient scale have changed.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:weightedhighlights"></span>
<img src="images/weightedcollocationanalysis.jpg" alt="Collocation Count for Page 2" width="\linewidth" />
<p class="caption">
Figure 3.5: Collocation Count for Page 2
</p>
</div>
</div>
</div>
</div>
<div id="case-study" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Case Study<a href="3-textcolor.html#case-study" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This method was applied to notes taken from a recent study on jury perception.
Participants were asked to read over a testimony transcript and answer some questions related to the scenarios.
Conditions varied based on the examiner conclusion, the presence of images, and the presence of a bullet matching algorithm.
When the algorithm was present, participants also read the testimony of an algorithm developer, resulting in more pages of notes.
Participants were supplied with a notepad in order to take notes throughout the testimony.
We are interested in comparing this notepad to the given testimony, in order to determine which portions of the testimony the participants found to be worth recording the most.
Portions of the testimony that bear the most similarity to the collective notes will be more highlighted than testimony that appears less frequently in the collective notes.
The method outlined above resulted in scenario testimonies that indicate where participants copied notes.</p>
<div id="the-data" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> The Data<a href="3-textcolor.html#the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our study consisted of 560 uniquely recorded participants.
Of these 560 participants, 430 recorded something on the notepad.
This has resulted in 7,185 pages of notes with at least something recorded, out of 10,329 total pages.
For individuals who clicked through the survey multiple times but submitted a single result, the notes correspond to the closest time less than their submission time for observation per page number.
Note pages that corresponded to the questions (and not the testimony) were removed; this was above page 10 in conditions without the algorithm and page 19 in conditions with the algorithm.</p>
</div>
<div id="data-cleaning-1" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Data Cleaning<a href="3-textcolor.html#data-cleaning-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>On the complete dataset, the hybrid method took a total of 19.33 minutes.
This demonstrates the time benefit of the hybrid method over the Longest Common Substring method - it took less time to clean the complete dataset with the hybrid method than it took to evaluate the reduced dataset with the LCS method.
The data cleaning resulted in a total of 2,372 note pages with some text recorded.
This is much smaller than the uncleaned total of 7,185 note pages, demonstrating a significant reduction in the note size (as is expected when removing the previous page’s notes).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:notecount"></span>
<img src="thesis_files/figure-html/notecount-1.png" alt="Number of Notes Taken Per Page" width="\linewidth" />
<p class="caption">
Figure 3.6: Number of Notes Taken Per Page
</p>
</div>
<p>Figure <a href="3-textcolor.html#fig:notecount">3.6</a> consists of the number of participants who took notes based on the page number.
In both the algorithm and the non-algorithm scenario, more individuals took notes on the first two pages of the testimony compared to the rest of the testimony.
However, the plot shows that individuals took notes throughout the testimony.</p>
</div>
<div id="final-output" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Final Output<a href="3-textcolor.html#final-output" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This final note analysis, which includes fuzzy matching, can be found at <a href="https://rachelesrogers.github.io/Presentations/Collocations/Collocation%20Comment%20Analysis%20Cleaned.html" class="uri">https://rachelesrogers.github.io/Presentations/Collocations/Collocation%20Comment%20Analysis%20Cleaned.html</a>.
In visually comparing the highlighted transcripts for fuzzy matches to those without fuzzy matching, the results appear to be fairly similar.
While the fuzzy matching did increase the frequency number, the overall trends remained the same for the dataset.
This can be seen in Table <a href="3-textcolor.html#tab:nonfuzzycount">3.10</a>.
The ranking of the collocations remains fairly consistent, aside from differences that can be seen in collocations that had the same non-fuzzy count.
<!-- There is a jump in the fuzzy matches for the charge (a felony) that is not seen in the non-fuzzy matching. --></p>
<table>
<caption><span id="tab:nonfuzzycount">Table 3.10: </span>Table of Collocation Frequencies</caption>
<thead>
<tr class="header">
<th align="left">collocation</th>
<th align="center">Nonfuzzy</th>
<th align="center">Fuzzy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">discharging a firearm in a</td>
<td align="center">134</td>
<td align="center">151.04</td>
</tr>
<tr class="even">
<td align="left">firearm in a place of</td>
<td align="center">133</td>
<td align="center">139.37</td>
</tr>
<tr class="odd">
<td align="left">a firearm in a place</td>
<td align="center">130</td>
<td align="center">136.98</td>
</tr>
<tr class="even">
<td align="left">in a place of business</td>
<td align="center">129</td>
<td align="center">136.43</td>
</tr>
<tr class="odd">
<td align="left">willfully discharging a firearm in</td>
<td align="center">121</td>
<td align="center">136.24</td>
</tr>
<tr class="even">
<td align="left">with willfully discharging a firearm</td>
<td align="center">93</td>
<td align="center">107.45</td>
</tr>
<tr class="odd">
<td align="left">charged with willfully discharging a</td>
<td align="center">91</td>
<td align="center">101.56</td>
</tr>
<tr class="even">
<td align="left">been charged with willfully discharging</td>
<td align="center">76</td>
<td align="center">83.05</td>
</tr>
<tr class="odd">
<td align="left">has been charged with willfully</td>
<td align="center">75</td>
<td align="center">78.65</td>
</tr>
<tr class="even">
<td align="left">cole has been charged with</td>
<td align="center">74</td>
<td align="center">76.12</td>
</tr>
</tbody>
</table>
<!-- collocation | nonfuzzy count | fuzzy count -->
<!-- ------------|----------------|------------ -->
<!-- discharging a firearm in a | 134 | 151.19 -->
<!-- firearm in a place of | 133 | 139.43  -->
<!-- a firearm in a place | 130 | 137.01  -->
<!-- in a place of business | 129 | 136.70  -->
<!-- willfully discharging a firearm in | 121 | 136.33  -->
<!-- with willfully discharging a firearm | 93 | 107.52  -->
<!-- charged with willfully discharging a | 91 | 101.56  -->
<!-- been charged with willfully discharging | 76 | 83.24  -->
<!-- has been charged with willfully | 75 | 78.75  -->
<!-- cole has been charged with | 74 | 76.12  -->
<!-- a place of business this | 74 | 88.34  -->
<!-- : Table of Collocation Frequencies (\#tab:nonfuzzycount) -->
<p>The standardized frequency values remain fairly consistent throughout the testimony.
Some areas that individuals focused on were the crime and its extent (in terms of injury), the qualifications/process of the firearms examiner, and the results from the analysis.
While this type of analysis may allow us to determine the areas that participants copied into their notes, it does not indicate why the participants found this testimony worth copying.
This may either indicate areas that participants saw as interesting or confusing, or a mixture of both.</p>
</div>
</div>
<div id="discussion-1" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Discussion<a href="3-textcolor.html#discussion-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="conclusion-1" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Conclusion<a href="3-textcolor.html#conclusion-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Overall, this process effectively cleans and displays notes with the desired highlight format.
For sequential notes, such as this case, the First N Character method manages to clean the notes in most cases.
In the cases where the FNC method is ineffective (when the edit distance is above the threshold or the clean notes are unusually long), the Longest Common Substring method can be applied.
This process was demonstrated in a dataset of 35 participants, and showed a significant note reduction on the full dataset.</p>
<p>After the notes are cleaned, collocations can be used to determine what phrases participants decided to copy into their notes.
The length of the collocation was chosen to be long enough to avoid commonly repeated phrases, but short enough that participants may directly copy the phrase into their notes.
Notes that were not directly copied were included with weights proportional to edit distance through the use of fuzzy matching.
The final output qualitatively indicates which portions of testimony participants found relevant to copy.</p>
<p>This new method opens up new avenues for user studies and analyses.
While this method was developed for a specific use case, it has diverse applications, including education, social science, and more.
These methods are being developed into R packages to be used by other
researchers in transcript analysis.
Future studies may investigate the application of this notepad in a variety of scenarios, such as course notes or collaborative note
taking.
Another application of this type of analysis may be used in the evaluation of student note taking or Wikipedia article edits.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-study1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-study2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"], ["thesis.epub", "EPUB"], ["thesis.docx", "Word"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"style": "style.css"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
