# Text analysis with Transcripts {#textcolor}

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(knitr)
library(tidyverse)
library(stringdist)
library(dplyr)
library(kableExtra)
library(ggplot2)
library(readtext)
require(quanteda)
require(quanteda.textstats)
require(quanteda.corpora)
library(purrr)
library(ggrepel)
library(stringi)
library(PTXQC)
library(DiagrammeR)
library(fuzzyjoin)

```

## Introduction

When conducting studies that rely on participants reading a longer document, researchers may be interested in determining what portions of the document participants find worth noting.
This could provide useful information about areas of interest by creating a type of heat map for the document.
When the study document consists of multiple pages and notes are recorded sequentially, the problem then becomes twofold: first the participants' notes must be cleaned so that the recorded text corresponds to the current page, then a method must be developed to match the frequency of the participants' notes to the study document.

In order to clean the notes, two different methods are considered and then combined: the First n Character method and the Longest Common Substring method.
The First n Character method relies on the concept of edit distance for matching and removing previous notes from sequential study documents.
This method was developed specifically for this document comparison.
The Longest Common Substring method, on the other hand, searches for common text between two sequential pages to be removed.
After the notes are cleaned, sequences of 5 words (collocations) are used to find areas of the testimony that participants focus on.
For indirect matches, such as typos, weights are applied to these fuzzy matches to contribute to the total count. 

This method was applied to notes taken from a recent study.
Dr. Vanderplas and I conducted a study on jury perception, in which participants were asked to read over a testimony transcript and answer some questions related to the scenarios. 
Participants were supplied with a notepad in order to take notes throughout the testimony. 
We are interested in comparing this notepad to the given testimony, in order to determine which portions of the testimony the participants found to be worth recording the most.
Portions of the testimony that bear the most similarity to the collective notes will be more highlighted than testimony that appears less frequently in the collective notes.
The method outlined above resulted in scenario testimonies that indicate where participants copied notes.

## Background

For data cleaning, a previous page's notes are matched to the next page's notes in order to remove duplicate text.
This can be thought of as a form of pattern matching.
There are plenty of examples of pattern matching in text analysis - such as Baeza-Yates and Gonnet, and Landau and Vishkin. 
These papers rely on a set pattern to be found in the reference text, with a defined number of differences allowed between the pattern and the text. 
According th Baeza-Yates and Gonnet, many algorithmic methods have been developed to solve this problem, with allowances for mismatches between the reference text and the pattern.
A similar allowance for differences is present in Landau and Vishkin, where differences are defined as either insertions, deletions, or substitutions.
This definition of differences is the same definition that is used in computing Levenshtein distance, also known as edit distance (Levenshtein).
Levenshtein considered the issue of insertions, deletions, and substitutions with binary code.
This concept has later been extended to include more extensive strings, as demonstrated by Konstantinidis.
The edit distance can be used to determine the extent of matching text when comparing two sequential note sheets, in order to identify if a portion of notes should be removed.

One thing that differentiates this problem from other text analysis methods is that there is no set in stone pattern - while the analysis is based on the previous page of notes, individuals do not always keep the previous page of notes the same - some delete parts, and some add new information in the middle. 
Thus, the goal is to not only find and remove only exact matches of the entire previous note sheet, but to also remove previous notes - whether they be shorter or discontinuous.
While evaluating edit distance is useful when finding direct matches, the Longest Common Substring (LCS) method can be used to find pieces of notes that match between pages, so that this repeated text can be removed.

Landau and Vishkin's discussion of pattern matching with k differences includes both a dynamic programming approach, as well as the construction of suffix trees.
These methods correspond to those proposed for finding the Longest Common Substring between two sequences.
This problem has been used to compare biological sequences (Crochemore et al.).
An early solution for finding the longest common substring involves the construction of a suffix tree (Charalampopoulos et al.).
The suffix tree method is described by Gusfield; in short, each unique suffix of a string would contribute a new branch to the suffix tree, where leafs consist of the string's terminal character: \$ is used avoid recurrence with previous characters in the string.
While I have located a package on Github that utilizes the suffix tree for LCS (Lang), I have been unable to successfully download the package.
Another solution implemented by Bielow et al. in a CRAN R package uses dynamic programming, which involves the construction of a matrix that records the length of a matching string at character $i$ for string 1 (in the $i$th row), and character $j$ for string 2 (in the $j$th column).
Because the dynamic programming method has an R implementation, it is utilized for this analysis.
The comparison between suffix tree and dynamic programming methods will be evaluated in a future analysis.

Once the participants' notes are cleaned, they must be compared to the study transcript.
In the realm of non-fixed pattern matching, there are plagiarism recognition methods (such as TurnItIn).
In these cases, a portion of a student's text is cross referenced with published or online references, in order to determine if the text was copied.
Because this type of correspondence is on a larger level than individual words, collocation analysis is used.
While collocations traditionally refer to words that occur more frequently together - such as "white house"(Merriam-Webster), tools for collocation analysis helpfully provide the frequency of occurrence for strings of a specified length (Schweinberger). 
In a reference to algorithmic detection of plagiarism in computer programs, Parker and Hamblen describe an algorithm that uses the character differences in order to compare how similar programs are.
Other algorithms they discuss use code-specific features such as number of operators, lines of comments, and number of various loop statements.
Because we are only concerned with the content of the notes, there is little information to be gained from formatting, as there is in plagiarism detection in programs.
However, character differences can effectively be used to indicate similarity between participants' notes and the study testimony.
This is useful in situation where individuals do not copy the notes directly.
In these cases, fuzzy matching can be used to find the testimony collocation that is the most similar to the participants' written notes.
This approximate string matching allows for matching strings that do not correspond directly (Gusfield), which would allow for matching up participants' inexact notes with the closest testimony collocation.

## Methods

### Data Cleaning

Table \@ref(tab:noteexample) is an example of what sequential notes may look like, taken from the second and third pages of a study participant's notes. 
Because participants are provided with the same continuous notepad throughout the study, the database records each page as a "screenshot" of what is written on their notes when they advance to the next page - this provides cumulative notes.

```{r noteexample, echo=FALSE, message=FALSE, warning=FALSE}
comments <- read.csv("data/clean_notes.csv")
# length(unique(subset(comments, notes != "")$clean_prints))
# val_prints <- sample(unique(subset(comments, notes != "")$clean_prints), size=30, replace=FALSE)
# 
# val_prints <- c(val_prints, 503, 359, 39, 368, 547)
# 
# val_dataset <- comments %>% subset(clean_prints %in% val_prints)
# 
# write.csv(val_dataset, "validation_dataset.csv")
# dim(val_dataset)

test_comments <-comments[1:4,]

kable(test_comments %>% filter(page_count==2 | page_count==3) %>% dplyr::select(page_count, notes), caption="Participant Note Example") %>% column_spec(2, width = "30em")
```

In order to appropriately clean the notes, the duplicated text ("Richard Cole - has been charted with willfully discharging a firearm in a place of business. This crime is a felony.") needs to be removed from the third page, to leave only notes that correspond to the third page's testimony.

#### First n Character (FNC) Method

In scenarios such as the table above, a straightforward method for note cleaning would be to measure the length of the previous page's notes (n), and compare the previous notes to the first n characters in the current pages notes via edit distance. 
The edit distance is the number of characters that would need to be changed in order to transform the first string into the second string.
This includes insertions, deletions, and substitutions(in the case of the "adist" function from the R Core Team).
For example, "Hat" and "Hot" would have an edit distance of 1, because the strings match if the "a" is turned into an "o".
Similarly, "Over there" and "there" would have an edit distance of 5, since 5 characters are deleted (the letters in "Over" plus an additional space).
If the edit distance is small (indicating a correspondence to the previous page's notes), the first n characters can be removed.
If we consider the table is above, it is clear to see how this method can be applied.
The page 2 notes perfectly line up with the first two lines in the page 3 notes, resulting in an edit distance of 0 when comparing the first n characters of the third page.
Thus, by removing this beginning section, we would be left with the notes that are unique to page 3 - namely, the last two lines.

This method works well when participants take notes sequentially.
In some instances, however, participants will delete portions of their previous notes, add new notes either before or in the middle of their old notes, or duplicate their old notes.
When participants delete a portion of their notes or add new notes before/in the middle of their old notes, the edit distance between the old and new notes could be large.
The first n character method would not be able to appropriately clean these notes.

#### Longest Common Substring (LCS) Method

Another method for matching text between two different sets of notes is the Longest Common Substring (LCS).
This method searches two strings for the longest sequence of sequential characters that they have in common, which is then returned. 
For example, in Figure \@ref(fig:lcs), the LCS between the two strings is "the cat enjoys napping".
In the process of note cleaning, this would be the string that is removed from the second page of notes, resulting in "When it is quiet" as the cleaned (non-repeated) notes.

```{r}
#| lcs,
#| fig.cap= "Longest Common Substring Diagram",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/svg_graph.png")
```

The PTXQC package (Bielow et al.) implements LCS using dynamic programming, as opposed to the suffix tree method (which has not been implemented in CRAN).
Their dynamic programming process is described below.
This method creates an empty matrix, where the number of rows correspond to the length of the first string, and the number of columns correspond to the length of the second string.
Thus, the first cell would correspond to the first character of both strings, and so on.
If the characters match and it is the first row or column, then the cell gets a value of 1.
If the characters match and it is not the first row or column, the the cell receives the value of the (i-1,j-1) cell plus one.
The first string's index for the substring of the longest length is saved.
This process is shown in Table \@ref(tab:dylcs), with two strings: "cabbacced" and "bbacccade".
The diagonal of numbers in red indicate the longest substring, beginning with 'b' and ending with 'c'
Thus, "bbacc" is identified as the LCS.

_ | c | a | b | b | a | c | c | e | d
--|---|---|---|---|---|---|---|---|---
b | 0 | 0 | \textcolor{red}{1} | 1 | 0 | 0 | 0 | 0 | 0
b | 0 | 0 | 1 | \textcolor{red}{2} | 0 | 0 | 0 | 0 | 0
a | 0 | 1 | 0 | 0 | \textcolor{red}{3} | 0 | 0 | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | \textcolor{red}{4} | 1 | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | 1 |\textcolor{red}{5} | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 0 | 0
a | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0
d | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1
e | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0

: Dynamic Programming for LCS (\#tab:dylcs)

The Longest Common Substring method is able to handle the issues found in the First n Character method, which is demonstrated in Table \@ref(tab:issues).
If notes are deleted, the LCS method can identify the substring from the previous notes, despite the large edit distance that would prevent removal in the First n Character method.
If new notes are inserted in the middle of previous notes, the First n Character method would compare the Page 1's notes with the red text of the Page 2 notes shown in the second row, resulting in an edit distance that would prevent removal.
With the LCS method, the first substring "The cat ran" could be identified and removed, then in a second iteration, the second substring of "up the tree" can also be removed, leaving only the new text.
In the case of duplication, the First n Character method would be able to identify the first iteration of text that matches Page 1 and remove it; but it would not be able to identify the second occurrence of the text.
As before, the LCS method would be able to identify both instances through two iterations, and remove all Page 1 text.

Issue | Page 1 | Page 2 | LCS | Edit Distance
--------|--------|---------|---------|---------
Deletion | The cat ran up the tree | The cat ran | The cat ran | 12
Insertion | The cat ran up the tree | \textcolor{red}{The cat ran, chased by a} dog, up the tree | (The cat ran)(up the tree) | 11
Duplication | The cat ran up the tree | \textcolor{red}{The cat ran up the tree} The cat ran up the tree | (The cat ran up the tree)(The cat ran up the tree) | 0

: LCS Comparison for FNC Issues (\#tab:issues)

While the LCS method is able to solve the issues related to the First N Character method, it has some issues of its own.
If the testimony itself contains duplicate text, such as the text related to swearing in witnesses, the LCS method would remove the new text that corresponds to the later testimony.
The other issue with the LCS method is the time required when it is applied to the entire testimony.
In the validation study described below, the LCS method took approximately 30 times as long as the FNC method.

#### Hybrid Method

In order to compare the two methods described above, I cleaned a subset of the survey response notes to have a "correct" baseline with which to calculate error rates.
This dataset consists of 561 pages of 'notes' (including blanks) with 35 unique participants. 
30 participants were randomly selected from the larger dataset, while 5 participants were selected based on demonstrated issues with data cleaning. 
The 5 selected participants included an individual who deleted almost all of the beginning notes; two individuals who pasted new notes in the middle of old notes; an individual who duplicated notes; and an individual who included the algorithm expert's trial confirmation as well as the forensic scientist's - duplicate text that should be included as the new notes.

For the FNC method, the length of the previous notes is compared to the beginning of the current page's notes. 
If the edit distance is below a certain threshold, these notes are removed. 
In this case, I changed the threshold with values of 0 character difference, 5 character difference, and 15 character difference (set as less than 1, less than 6, and less than 16).
In the case of LCS, a threshold must be picked for the minimum matching substring length to be considered 'previous notes' and removed.
If the threshold is too small, the LCS method can identify words like "the" as matching previous testimony to be removed.
If the threshold is too large, the LCS method may be unable to remove substrings when new text is inserted in the middle of the old text.
Here, I tried several different character lengths: 40 characters, half the previous note length, a third of the previous note length, and a fourth of the previous note length plus 5 (to compensate for shorter strings).

I also investigated the use of a hybrid method in the hope that the FNC method could be used as a quick way to clean the simplest cases of notes, while the LCS method could be used to clean up the notes that are less clear-cut.
In order to determine which observations would need the second cleaning method, I created Figure \@ref(fig:errorplot).

```{r}
#| errorplot,
#| fig.cap= "Error Comparison",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/cleaningerror.jpg")
```


Here, the yellow circles indicate the FNC method with a 16 character threshold, while the black circles represent the LCS method with a threshold of 1/3 the previous character length.
Larger circles indicate a larger amount of error when compared to the hand-cleaned notes.
The y axis shows the edit distance computed between the current and previous notes for the FNC method, while the x axis indicates the length of the clean notes.
The horizontal line is the 16 character threshold for the FNC method.
As can be seen, most observations are located in the bottom left of the graph, and have relatively small circles.
These are notes where the FNC method was effectively applied.
In the observations above the threshold, most have a larger error when the FNC method is applied compared to the LCS method, with the exception of one observation in the top left.
In that case, the error appeared to be equal for both methods.
There is also a larger FNC method error for the observation on the bottom right of the graph.
This was a case of duplicate text, resulting in abnormally long notes.
This graph appears to show that and edit distance above the threshold of 16 for the FNC method and the length of the cleaned notes can be used as criterion for determining when the LCS method should be applied.
I chose to set a threshold of 4 standard deviations above the mean note length for cleaned notes of the current page to constitute "unusually long" notes to apply the LCS method to.
I chose 1/3 of the previous notes as the LCS threshold because the mean edit distance to the correct notes was comparable to the 1/4 + 5 character threshold, but it is more conservative for larger notes.

Methods | Error | SD | Time (Minutes)
--------|--------|---------|---------
FNC 1 Character | 498.965 | 2769.826 | 2.4 
FNC 6 Character | 201.353 | 1890.009 | 2.5 
FNC 16 Character | 36.912 | 411.252 | 2.5 
LCS 40 Character | 6.337 | 35.497 | 75.8 
LCS 1/2 Previous Notes | 7.288 | 104.692 | 85.7 
LCS 1/3 Previous Notes | 3.064 | 26.769 | 76.3 
LCS 1/4 Previous Notes + 5 | 2.823 | 26.137 | 73.6 
Hybrid (FNC 16 and LCS 1/3) | 2.135 | 24.625 | 3.5 

: Comparison of Note Cleaning Methods (\#tab:cleancompare)

The results of this study are shown in Table \@ref(tab:cleancompare).
Notably, the mean edit distance to the correct notes (or the error) is large for the FNC methods, with a high level of variation.
However, the computation time is approximately 2.5 minutes.
In the case of the LCS method, the error is much lower, but the computation time is above an hour.
The hybrid method appears to combine the benefits of both of these methods. 
The error rate is on par with that of the LCS methods, while the time is only slightly above that of the FNC methods.
The hybrid method was therefore implemented for the larger database.
The flowchart in Figure \@ref(fig:flowchart) shows how this hybrid method can be applied.

```{r}
#| flowchart,
#| fig.cap= "Hybrid Flowchart",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/flowchart.jpg")
```

### Note Analysis

#### Collocations

In order to analyze which areas of the text individuals focus on, we must first find a way to compare their notes to the written testimony.
One way to do this comparison is through collocation analysis, which uses the frequency of strings of sequentially-appearing words (n-grams) in order to identify words that tend to 'stick together'.
For example, in the case of the phrase "The cat ran up the tree", there would be three collocations of length 4: "The cat ran up", "cat ran up the", and "ran up the tree".
By matching the collocations that appear in the testimony to a frequency of the collocation from the given notepad, we would be able to get frequency values for all n-grams that occur in the testimony.

In setting the size of the collocation, there are two important factors to keep in mind: the frequency with which a phrase appears in the testimony itself, and the number of words in a row that individuals may directly copy from the testimony.
If the number of words considered in a collocation is too short, we may run into an issue of phrases occurring multiple times throughout the testimony, without having a way to distinguish which sections are of importance.
For example, if we used a collocation of length two, a common phrase in the testimony would be "Richard Cole".
Because this phrase occurs so frequently in the testimony, it does not by itself tell us if there are particular portions of the testimony that individuals find important, even if we add weights to calculate the relative frequency.
On the other hand, if we use a collocation of length 10, it is unlikely that a large amount of individuals would copy the exact 10-word phrase, which would result in fewer observations.
In considering these two issues, we decided on a collocation size of 5.
This would be long enough to avoid many common phrases such as "the bullet matching algorithm", but short enough that individuals may copy the phrase down directly.
The implementation of collocations used the 'quanteda' package, with guidance from a tutorial by Schweinberger.

N-grams are identified in the testimony, and assigned ordered word numbers (so the first n-gram of length 5 will have words 1-5, the second will have words 2-6, and so on). 
These are then bound with collocations from the notes based on the sequence of 5 words, in order to assign a frequency to each n-gram. 
The average is then taken per word out of the possible words that the n-gram could have appeared in (for example, the first word can only appear in a single n-gram, so its average will be equal to the frequency of the first collocation). 
When multiple 5-grams appear in the same testimony page, the note frequency is divided by the number of appearances in the testimony in order to account for multiple occurrences.
The table below shows the highest frequencies for the second page of the testimony: 

"In this case the defendant Richard Cole has been charged with willfully discharging a firearm in a place of business. This crime is a felony. 
Mr. Cole has pleaded not guilty to the charge. 
You will now hear a summary of the case.
This summary was prepared by an objective court clerk.
It describes all of the evidence that was presented at trial."

```{r}
#| colcount,
#| fig.cap= "Collocation Count for Page 2",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/collocationcount.jpg")
```

The highlighted collocations in Figure \@ref(fig:colcount) show all of the collocations for the word "willfully" from the testimony above.
The frequencies for 'willfully' can be averaged: $\frac{121+93+91+76+75}{5}=91.2$.
This number will correspond to the shading of the word "willfully" in the highlighted testimony of Figure \@ref(fig:highlights).

Note that the words have a gradient.
This is to ensure a smooth transition between words in order to give an overall "highlight" effect, because we are interested in the important phrases in the testimony as opposed to the importance of individual words. 

```{r}
#| highlights,
#| fig.cap= "Collocation Count for Page 2",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/collocationanalysis.jpg")
```

In order to create this effect, we first implemented a graph of the testimony in 'ggplot2'(Wickham) in order to assign color values to the frequencies.
Then we used HTML gradient boxes, where the left color is that of the previous word, while the right color is based on the frequency of the current word.
This results in a smooth transition between words.


#### Fuzzy Matching

In order to perform fuzzy matching on the indirect collocations from participants' notes, I used 'stringdist_join' from the 'fuzzyjoin' R package (Robinson). 
This function conducts a join with the best matching observations from the second dataset. 
For example, consider asking young children to spell their birth month.
This may result in a dataset like Table 
The results of this study are shown in Table \@ref(tab:months).

Name | Month
--------|--------
Billy | Mach
Jimmy | Apil 
Frances | Mae
Gary | Febary
Steve | Octobr
Bobby | Agust
Tammy | Febrey
Greg | Agast
Benny | Jun

: Dataset of misspelled months (\#tab:months)

We may then want to match the children's written months to their correctly spelled counterparts. 
While 'stringdist_join' will give values for all matches between the two datasets, we can use group_by and slice_min to find the month that is closest to the misspelled value.

```{r fuzzymonth, echo=FALSE,message=FALSE, warning=FALSE}

months <- data.frame(month=c("January","February","March","April","May","June","July","August","September","October",
                             "November","December"))
classroom <- data.frame(month=c("Mach","Apil","Mae","Febary","Octobr","Agust","Febrey","Agast","Jun"), 
                    name=c("Billy","Jimmy","Frances","Gary","Steve","Bobby","Tammy","Greg","Benny"))

fuzzy_matches <-stringdist_join(months, classroom, 
                                  by='month', #match based on team
                                  mode='right', #use right join
                                  method = "lv", #use levenshtein distance metric
                                  max_dist=99, 
                                  distance_col='dist')%>%
    group_by(name) %>%
    slice_min(order_by=dist, n=1)

kable(fuzzy_matches) # %>%
#  kable_styling(position = "center")

```

In Table \@ref(tab:fuzzymonth) above, "month.x" is the correctly spelled month, "month.y" is the children's spellings, and "dist" is the edit distance between the two words. 
This same concept is applied to the notepad dataset.
In this case, the transcript of the testimony is considered the "correct" version, and the participants' collocations that do not directly match are considered the misspellings.
This method finds the closest transcript collocation to the written collocation, with a maximum edit distance of 99 for collocations to still be considered a match.

Because the intent of the participant is more clear with simple typos (such as a single letter change) than with larger differences (such as the changing of multiple words), the count for these fuzzily matched observations are weighted based on the edit distance.
Specifically, a weight of $\frac{1}{dist+0.25}$ is used, where $dist$ represents the edit distance (between 1 and 99).
An edit distance of 0 would receive a weight of 1, since fuzzy matching need not be applied.
Another factor in the calculation is the number of matches per 'misspelled' collocation.
To return to the month example, if an individual were to spell a month "Jur", the edit distance to both "June" and "July" is 2.
It is then unclear which month the person was born in, but they most likely meant June or July.
Similarly, some collocation notes had the same edit distance for multiple transcript collocations.
In these cases, the frequency of the 'misspelled' collocation notes are split evenly between the closest transcript collocations.
This results in a total fuzzy frequency per transcript collocation of:

$$
\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i}
$$
where $x_i$ is the count for note collocation $i$, $d_i$ is the distance between the transcript collocation and the 'misspelled' collocation $i$, and $c_i$ is the number of transcript collocation matches for note collocation $i$.
This quantity is then added to the count of direct collocation matches calculated in the previous section.

## Notes
- Participants of the first study were allowed to take notes while reading through the transcripts, which are saved by page number
- We would like to use this to highlight which parts of the testimonies the participants found to be important

- First attempt:
  - ggplot: allows for both a gradient and transparency to be added to words based on their frequency
  - graphing words of the transcript resulted in uneven spacing
  - Relative frequency was computed based on the number of times the word appeared on the page
- Second attempt:
  - Using CSS instead of ggplot: can set gradient through color outputted by ggplot
  - Words can be equally spaced
  <!-- - Currently assigning a new color to each word - this is not efficient -->
  <!-- - Now in CSS for tab format -->
  - Labels added with gradient scale
  - Change from relative frequency of individual words to average collocation frequency per word (with collocations of length 5)
     -Include fuzzy matching for indirect matches (typos/skipped words)
       