# Jury Perception of Bullet Matching Algorithms and Demostrative Evidence {#study1}

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache = F}
library(readr)
#library(ordinal)
library(tidyr)
library(plyr)
library(dplyr)
#library(gofcat)
library(ggplot2)
#library(forcats)
library(ggmosaic)
require(gridExtra)
library(GGally)
library(forcats)
library(ggpcp)
library(ggblend)
library(knitr)
library(patchwork)
#library(kableExtra)
library(scales)
library(tidyverse)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# demographics_fingerprints <-
#   read_csv("data/demographic_results.csv")
# demographics2_fingerprints<-
#   read_csv("data/demographic_results2.csv")
# dim(demographics_fingerprints)
# demographics_fingerprints$clean_prints <- as.numeric(as.factor(demographics_fingerprints$fingerprint))
# 
# fingerprint_matching <- demographics_fingerprints %>% select(clean_prints, fingerprint) %>% distinct()
# dim(fingerprint_matching)
# 
# merged_results_fingerprints <- read_csv("data/merged_results_wo_comments_w_allresults.csv")
# dim(merged_results_fingerprints)
# 
# merged_results_fingerprint_join <- left_join(merged_results_fingerprints, fingerprint_matching)
# 
# ## Identifying the print that had two sets of notes ##
# merged_results_fingerprint_join[merged_results_fingerprint_join$fingerprint=="642d497661c84dac328438a2bf0709e4",]$clean_prints
# 
# dim(merged_results_fingerprint_join)
# fingerprint_clean <- subset(demographics_fingerprints, select=-c(fingerprint))
# merged_results_clean <- subset(merged_results_fingerprint_join, select=-c(fingerprint))
# 
# write.csv(fingerprint_clean, "data/study1_demographic_clean.csv")
# write.csv(merged_results_clean, "data/study1_merged_results_clean.csv")

```

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache = F}

# merged_results <-
#   read_csv("../../merged_results_full_wo_comments.csv")
#View(merged_results)

merged_results_allresponses<-read_csv("data/study1_merged_results_clean.csv")
# dim(merged_results_allresponses)

# Removing person who took notes for on the first page that did not correspond to their scenario number
merged_results_allresponses <-merged_results_allresponses %>% subset(clean_prints!=231)

merged_results <- merged_results_allresponses %>% filter(check1 =="9mm" & check2 == "Moderately reliable")

merged_results$conclusion <-
  factor(merged_results$conclusion,
         levels = c("NoMatch", "Inconclusive", "Match"))

demographics <-
  read_csv("data/study1_demographic_clean.csv")

```

Due to the issues outlined in Chapter \@ref(litreview), we designed a study to investigate how potential jurors may perceive a bullet matching algorithm, when presented alongside a firearms examiner's testimony, as well as the \authorcol{e}ffect of demonstrative evidence.
This study utilizes a testimony transcript to familiarize participants with bullet matching evidence for a case involving armed robbery, and participants are asked to make judgements on the guilt of the defendant, as well as the credibility of the witnesses and the reliability and scientificity of the evidence presented.

## Background

### Firearms Examiners

The foundational belief in bullet comparisons as a form of identifying evidence revolves around the idea that guns can leave unique striation marks on a bullet as an artifact of the rifling process [@pcast]. 
<!-- [@pcast, p. 104] -->
Striation marks are left on portions of the bullets known as "lands", due to contact with the rifling as the bullet is fired. 
These marks are compared by examiners across bullets in order to identify if bullets were fired from the same source. 
These examinations are subjective, as they are based on the firearms examiner's experience and judgement [@nationalresearchcouncilusStrengtheningForensicScience2009].
<!-- [@nationalresearchcouncilusStrengtheningForensicScience2009, p. 153] -->
This can lead to issues of bias in analysis [@kassinForensicConfirmationBias2013]. In 2019, the PCAST report highlighted common issues with traditional bullet matching methods, such as the lack of appropriately designed error rate studies and the circular nature of AFTE's bullet matching guidelines [@pcast]. 
<!-- [@pcast, p. 104-112] -->
Issues in error rate studies have been discussed in several articles [@hofmannTreatmentInconclusivesAFTE2021;@drorMisUseScientific2020]. 
PCAST emphasizes the importance of the development of an objective method for firearms comparisons [@pcast]. 
<!-- [@pcast, p. 113] -->
The use of an automatic, objective method would contribute to quantifiable presentation of evidence and would lower the effort required to identify the method's error rate. 

### Bullet Matching Algorithm

Following PCAST's publication, many methods of statistical matching have been developed and evaluated, such as @hare2017automatic, @vanderplasComparisonThreeSimilarity2020, @songDevelopmentBallisticsIdentification2012, and @vorburgerTopographyMeasurementsApplications2015. 
In this study, the bullet matching algorithm used was developed by @hare2017automatic. 
The algorithm can briefly be described as follows:

1. A 3D scan is taken of each bullet land (containing striation marks to be compared), a stable cross-section is extracted, and shoulders (without relevant striation marks) are removed, as in Figure \@ref(fig:shoulder), an image from @hare2017automatic.

```{r shoulder, echo=FALSE, fig.cap="The land is shown in the center. Shoulders are shown outside of the vertical lines. Hare et al. (2017)"}
include_graphics(path = "images/shoulder.jpg")
```

2. A smoothing function is applied twice in order to extract the signature, a pattern of high and low points on the bullet's surface corresponding to the striation marks. 
The signature can then be compared to land signatures from other bullets, as in Figure \@ref(fig:signature).

```{r signature, echo=FALSE, fig.cap="Two aligned signatures for a known match. Image generated from Houston dataset by authors."}
include_graphics(path = "images/F526_Match_Signatures.png")
```

3. Traits, such as consecutively matching striae and depth of grooves, are then used in a random forest to produce a match score (ranging from 0 to 1) for the lands. 
The random forest consists of decision trees that consider a combination of variables and responses in order to predict if two signatures were created by the same gun. 
The decision trees in a random forest each consider a subset of variables and observations to decide whether or not the lands match.
The decisions of these trees are combined to produce the match score. 
Lands are then aligned across bullets in order to compute an overall match score for the bullets, as in Figure \@ref(fig:grid).

```{r grid, echo=FALSE, fig.cap="Diagonal correspondence (in orange) among lands indicate a match, as shown here. Image generated from Houston dataset by authors."}
include_graphics(path = "images/Test_Fire_F526.jpeg")
```

This algorithm was trained on Hamby's Consecutively Rifled Ruger Barrel Study data sets 252 and 173 [@vanderplasComparisonThreeSimilarity2020], and the final random forest was able to correctly predict all matches and non-matches [@hare2017automatic].
<!-- [@hare2017automatic, p. 2350].  -->
Three sets were then used to verify the algorithm: Hamby set 44, Phoenix PD, and Houston FSC [@vanderplasComparisonThreeSimilarity2020]. 
<!-- [@vanderplasComparisonThreeSimilarity2020, p. 5] -->
The algorithm performed well for undamaged bullets, and was able to completely distinguish between matches and non-matches when a cutoff value was individually chosen for each data set [@vanderplasComparisonThreeSimilarity2020].
<!-- [@vanderplasComparisonThreeSimilarity2020, p. 10] -->

### Explainable Machine Learning - Previous Research

Jurors' ability to interpret statistical methods and language is in doubt; in a study conducted by @koehler2001 with regards to the probability of a random match in DNA evidence, they found that jurors had different interpretations when the probability was presented as 1 out of 1,000 versus 0.1 out of 100, where those presented with a decimal number were more likely to view the probability of a random match as smaller. 
In another study, @garrettComparingCategoricalProbabilistic2018 asked jurors to evaluate evidence that used the following FRStat language, typically used in fingerprint analysis: "The probability of observing this amount of correspondence is approximately [XXX] times greater when the impressions are made by the same source rather than by different sources" [language from @DFSCLPInformation2018].
When asked for the likelihood the defendant committed the crime when presented with the above FRStat language, participants did not provide significantly different likelihoods for values ranging from 10 times greater to 100,000 times greater. 
This may demonstrate a lack of understanding when jurors are presented with numerical results for statistical evidence. 
@ENFSIGuidelineEvaluative2016 proposes a verbal scale to supplement likelihood ratios, which may alleviate some of the burden of statistical interpretation from potential jurors. 
This scale ranged from weak support, corresponding to a likelihood ratio between 2 and 10, to very strong support, corresponding to a likelihood ratio greater than 10,000 [@ENFSIGuidelineEvaluative2016]. 
<!-- [@ENFSIGuidelineEvaluative2016, p. 64].  -->
When interviewing judges, lawyers, forensic scientists, and forensic researchers, @swoffordProbabilisticReportingAlgorithms2022 found that many expressed concern regarding the interpretation of probabilistic language, and several suggested a combination of both match and probabilistic language. 
@hare2017automatic's algorithm differs from previous presentation methods in that its output is a match score, as opposed to a likelihood ratio. 
In this study, potential jurors may encounter both the algorithm's match score alongside the categorical identification language of the examiner.

### Demonstrative Evidence

Images are often used to assist individuals in understanding non-image information. 
However, images may have an effect of making a scenario more believable, as demonstrated in a study by @cardwellNonprobativePhotosRapidly2016 which involved "giving" or "taking" food from animals with or without images. 
Images are also mentioned as a factor that can influence how truthful individuals view statements in the courtroom - even if no new information is presented through the presence of the image [@kellermannTrialAdvocacyTruthiness2013].
Therefore, the use of images in a courtroom should be studied for an effect with regards to how subjects perceive the evidence presented - namely, if there is a difference in how reliable or credible they feel the experts are, based on the presence or absence of images.
Because the images are intended to aid only in interpretation, unbiased results would include an increase in understanding across all other conditions, while reliability and credibilty stay the same.
However, based on previous research, it seems probable that the presence of images would increase the perceived reliability and credibility of the evidence and expert, respectively.

## Methods

### Study Format

First, participants are presented with a short scenario based on @garrettMockJurorsEvaluation2020: a bullet is the only evidence recovered from an attempted convenience store robbery, and is tested against a gun found in Richard Cole's car in a routine traffic stop. 
Participants are informed that the store clerk was unable to identify the robber because they were wearing a ski mask, and that the testimony presented represented all relevant information.
They were also advised that they would be unable to re-read testimony, and a notepad was provided for their convenience.
Participants are then asked to read a transcript of mock court testimony, and rate their impression of the evidence presented, as well as their impression of the experts. 
This document included expert testimony, cross examination, and questions from the jury conveyed through the judge regarding error rates. 
The expert testified to their qualifications, as well as the process of bullet matching.
They then described comparing the fired evidence to a test fire from the defendant's gun, and the results of the comparison.
When the algorithm was included, the firearms examiner also described the algorithm's match score resulting from this comparison.
Cross examination included questions regarding the ability to uniquely identify the source of the bullet, as well as the subjectivity of the comparison.

When the algorithm was included, an algorithm expert then testified.
They also listed their qualifications and involvement in the development of the algorithm.
The expert would then describe the process for obtaining a match score (similar to the algorithm description in the previous section).
They also spoke to the publication history of the algorithm, the code availability, and the algorithm's applicability to the type of firearm considered in the case.
Cross examination included questions on the newness of the algorithm, as well as subjective aspects of calibration, and limitations with respect to the types of bullets that it can evaluate.
The testimony was based on actual court testimony provided by forensics experts lawyers, and judges, shown in Appendix \@ref(testimony-transcripts).

After reading the transcript, participants were directed to respond to some questions regarding the testimony.
These questions were largely based on @garrettMockJurorsEvaluation2020.
They were first given information about their responsibility as jurors to choose whether or not to convict, based on the 'beyond reasonable doubt' threshold.
Participants were also asked to estimate the probability that the defendant committed the crime, and the probability that the gun was involved in the crime.
These questions were followed by several Likert scales on the strength of evidence, the credibility of the examiners, reliability and scientificity of the evidence, and understanding of the procedures.
Two attention check questions were asked, in order to ensure that participants were reading both the testimony and the subsequent questions. 
The first attention check asked participants to identify the caliber of bullet recovered from the crime scene, while the second attention check asked participants to select a specific value from a Likert scale.  

The study includes three independent variables: presence or absence of the algorithm, presence or absence of demonstrative evidence (images), and conclusion (identification, elimination, or inconclusive). 
In the case of the algorithm, two testimonies were presented: that of the firearms examiner (Terry Smith), and that of the algorithm developer (Adrian Jones). 
The firearms examiner presented the algorithm results for the case alongside their own analysis, and suggested that the algorithm's results supported their conclusion. 
By presenting the algorithm results with the interpretation suggested by the firearms examiner, we hoped that potential jurors could use the firearms examiner's explanation and conclusion to guide their understanding of the algorithm's results. 
The algorithm expert then describes in greater detail the algorithm's process, and its validity for this particular case. 
When demonstrative evidence is present, images of rifling (Figure \@ref(fig:rifling)), a fired bullet (Figure \@ref(fig:fired)), a bullet comparison (Figure \@ref(fig:microscope)), and algorithm images (such as those shown above) were included alongside the testimony. 
In terms of the conclusion: an "identification" indicated agreement in individual and class characteristics; an "elimination" indicated an agreement of class characteristics, but disagreement in individual characteristics; and an "inconclusive" indicated an agreement in class characteristics, but not enough agreement in individual characteristics to state that there was an identification.  

The number of survey questions that respondents received depended on the scenario; participants who received the algorithm were asked more questions than those who did not receive the algorithm. 
For example, participants who did not receive the algorithm were asked about the reliability of the forensics examiner's bullet comparison and the reliability of the field of firearms as a whole. 
Those who received the algorithm were asked about the reliability of the forensics examiner's bullet comparison, the reliability of the algorithm's comparison, the overall reliability in the case (which includes both the algorithm comparison and the forensics expert's comparison), as well as the reliability of the field of firearms as a whole. 
This same format was used when asking participants about credibility and scientificity.

### Prolific

Participants were recruited using Prolific, an online survey-taking website. 
From the Prolific website, participants were directed to a link containing our survey, created using R Shiny.
We selected options to recruit a representative sample of individuals located in the United States, and asked that participants self-screen for jury eligibility before completing the survey. 
Jury eligibility was defined as US citizens over the age of majority in their state who had not been convicted of a felony, were not active law enforcement, military, emergency response, or a judge, and who did not have a disability that would prevent them from serving on a jury. 
They were also required to have normal or corrected to normal vision, due to the images used in the study. 
Participants were compensated with \$8.40 for completing the study, for an hourly compensation rate of about \$27.79 (median completion time of approximately 18 minutes).  
Individuals who did not include their Prolific identification number and an individual whose notes indicated that they had progressed far enough into the survey to get a separate scenario before restarting were excluded from analysis.

```{r}
#| completiontime,
#| fig.cap= "Completion Time by Condition for all unique fingerprints",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

fingerprint_freq<-table(merged_results_allresponses$clean_prints)

freq_df <- data.frame(fingerprint_freq)
single_fingerprint <- freq_df %>%
  filter(Freq == 1)
scenarios <- merged_results_allresponses %>% 
  dplyr::select(scenario_number, conclusion, clean_prints, 
                                         algorithm, picture, time) %>%
  filter(clean_prints %in% single_fingerprint$Var1) %>% 
  dplyr::rename(finish_time = time)
beginning_wend <- right_join(demographics, scenarios)
beginning_wend$total_time <- NA
beginning_wend$total_time <- beginning_wend$finish_time - beginning_wend$time
beginning_wend$total_time_minutes <- beginning_wend$total_time/60
dim_g_75 <- beginning_wend %>% filter(total_time_minutes > 75) %>% dim()
beginning_wend$truncated_minutes <- beginning_wend$total_time_minutes
beginning_wend[beginning_wend$total_time_minutes > 75,]$truncated_minutes <- Inf

# median(beginning_wend[beginning_wend$algorithm=="Yes",]$total_time_minutes, na.rm=TRUE)
# median(beginning_wend[beginning_wend$algorithm=="No",]$total_time_minutes, na.rm=TRUE)

conclusion_labs <- c("Identification", "Inconclusive", "Elimination")
names(conclusion_labs) <- c("Match", "Inconclusive", "NoMatch")

beginning_wend$conclusion <-
  factor(beginning_wend$conclusion,
         levels = c("NoMatch", "Inconclusive", "Match"))

png(filename="images/completiontime.png",type = "cairo-png")
ggplot(beginning_wend, aes(x = truncated_minutes, fill=algorithm)) +
    geom_density(alpha=0.75, color=NA) |> partition(vars(algorithm)) |> blend("multiply")+
  facet_grid(conclusion~picture, labeller =  labeller(picture = label_both, conclusion = conclusion_labs))+
  ggtitle("Histogram of Completion Time") +
  xlab("Completion Time in Minutes")+
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()
hide<-dev.off()


include_graphics(path = "images/completiontime.png")


```
Figure \@ref(fig:completiontime) depicts completion times (from completion of the demographics information to the end of the survey) by conditions for unique fingerprints, or identification values (n=`r dim(single_fingerprint)[1]`), excluding `r dim_g_75[1]` observations greater than 75 minutes. 
Those who received the algorithm condition took slightly more time on average than those who did not (median values of `r  round(median(beginning_wend[beginning_wend$algorithm=="Yes",]$total_time_minutes, na.rm=TRUE), digits=3)` minutes and `r round(median(beginning_wend[beginning_wend$algorithm=="No",]$total_time_minutes, na.rm=TRUE), digits=3)` minutes, respectively). This is unsurprising, given the increased length of the algorithm condition.


## Results

### Participants

```{r}
#| demographics,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

# dim(demographics)
 # table(demographics$gender)
 # sum(demographics$gender=="Female")
# median(demographics$age, na.rm=TRUE)

# dim(merged_results_allresponses)
# dim(merged_results)

ggplot(subset(demographics, !is.na(age) & age > 17), aes(x = gender, y=age)) +
    geom_violin(position = position_dodge(1),
              draw_quantiles=c(.25, .5, .75),
              color = "black",
              alpha = 0.5) +
  geom_jitter(width=0.15)+
  ggtitle("Age by Gender") +
  theme_bw()

```
Of the `r dim(demographics)[1]` participants with Prolific ID's who started the survey, there were `r sum(demographics$gender=="Female")` female participants, `r sum(demographics$gender=="Male")` male participants, and `r sum(demographics$gender=="Other/non-binary")` non-binary participants.
According to Prolific, these participants are a representative sample (in terms of age, sex and ethnicity, based on census data) of Prolific participants located in the United States.
The median age was `r median(demographics$age, na.rm=TRUE)`. 
Age and gender are shown in Figure \@ref(fig:demographics).
An age of 0.2 was excluded due to implausibility and lack of study completion.
In two instances, participants reported being removed from the survey website prior to survey completion, which may have happened in other cases where the participants did not complete the survey.
`r dim(merged_results_allresponses)[1]` participants eligible for analysis completed the survey, and `r dim(merged_results)[1]` participants correctly answered both attention check questions.
The division of correct answers on the attention check questions are shown in Table \@ref(tab:attentioncheck).
These `r dim(merged_results)[1]` participants who answered both attention check questions correctly were considered for the following analysis.

```{r}
#| attentioncheck,
#| echo= FALSE,
#| message= FALSE,
#| eval= TRUE
merged_results_allresponses$gun_caliber<-NA
merged_results_allresponses$gun_caliber[merged_results_allresponses$check1=="9mm"] <- "Caliber Correct"
merged_results_allresponses$gun_caliber[merged_results_allresponses$check1!="9mm"] <- "Caliber Incorrect"

merged_results_allresponses$question_reading<-NA
merged_results_allresponses$question_reading[merged_results_allresponses$check2=="Moderately reliable"] <- "Reading Correct"
merged_results_allresponses$question_reading[merged_results_allresponses$check2!="Moderately reliable"] <- "Reading Incorrect"

dem_table<-table(merged_results_allresponses$gun_caliber, merged_results_allresponses$question_reading)
kable(dem_table, caption="Attention Check Information")
# ggplot(merged_results_allresponses) +
#   ggmosaic::geom_mosaic(aes(x = product(gun_caliber), fill=question_reading))+
#   ggtitle("Correct Answers") +
#     scale_fill_manual(values = c("grey", "black"))+
#   theme_bw()

```
### Overview

Participants were asked a variety of questions in order to assess their thoughts and feelings on the case and the evidence presented. 
Two questions related to probability: respondents were asked to provide a value for the probability that the gun was at the crime scene, and to provide a value for the probability that Richard Cole (the defendant) committed the crime. 
Another question asked participants if they would convict Cole, based on the evidence. 
Questions of credibility, reliability, and scientificity were assessed using a 7-point Likert scale (eg. "Extremely unscientific" to "Extremely scientific"). 
Understanding was ranked on a 5-point Likert scale ("I understood nothing" to "I understood everything"). 
Participants were also asked about uniqueness of firearm toolmarks, as a simple yes/no question. 
Strength of evidence was ranked on a 9-point Likert scale ("Not at all strong" to "Extremely strong"). The perceived frequency with which firearms examiners made mistakes was assessed on a 7-point Likert scale ("Never" to "Always").  
Many of these response scales suffered from scale compression - individuals selected the top two categories on many of the Likert scales, indicating that participants overall believed in the credibility of the witnesses and the reliability and scientificity of the methods.
Due to this issue, no statistical analysis was performed on this data.

### Probability

There is a difference in the perceived probability that Cole, the defendant, committed the crime as well as the perceived probability that the gun was present at the crime scene based on the examiner's decision, as shown in Figure \@ref(fig:probalgorithm). 
The identification condition resulted in high probabilities, while the elimination condition resulted in low probabilities, indicating that participants reacted to the examiner's testimony. 
A wider range of probability values were observed for the inconclusive decision, without the high peaks that were present for conclusive decisions. 
Definite conclusions of identification or elimation resulted in higher peaks at more extreme values for the probability that the gun was at the crime scene, compared with the probability that Cole committed the crime. 
This may indicate that some individuals are distinguishing between evidence that the weapon was used and evidence against Cole. 
The inconclusive decision resulted in a more spread out distribution that is practically the same for both the probability that the gun was at the crime scene and the probability that Cole committed the crime, with participants generally selecting values below 50%.  

```{r}
#| probcompare,
#| fig.cap= "Comparison of Probabilities Selected for Cole and the Gun",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE
merged_prob <-
  dplyr::select(merged_results, c("conclusion", "probability", "gunprob"))

merged_problong <-
  merged_prob %>% gather(subject, probability,-conclusion)



ggplot(merged_problong, aes(x = probability, fill = subject)) +
  geom_density(alpha = 0.4, color=NA) +
  facet_grid(. ~ conclusion, labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  scale_fill_manual(values = c("orange", "purple"),
                    labels = c("The Gun", "Cole"))+
  theme_bw()

```

```{r}
#| probalgorithm,
#| fig.cap= "Probability the gun was used in the crime, or that Cole committed the crime. Black lines indicate bullet match scores for the algorithm.",
#| fig.width= 10,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

merged_probcombine<- merged_results %>% 
  select(algorithm, gunprob, conclusion, probability) %>%
  pivot_longer(cols=c(gunprob, probability), names_to = "Type", values_to="Probability")

merged_probcombine$Type <- factor(merged_probcombine$Type, levels = c("probability", "gunprob"), 
                  labels = c("Cole", "Gun"))

png(filename="images/probalgorithm.png",type = "cairo-png")
ggplot(merged_probcombine, aes(x = Probability, fill = algorithm)) +
  geom_density(alpha = 0.75, color=NA) |> partition(vars(algorithm)) |> blend("multiply") +
  geom_vline(data = filter(merged_probcombine, conclusion == "NoMatch"),
             aes(xintercept = 3.4)) +
  geom_vline(data = filter(merged_probcombine, conclusion == "Inconclusive"),
             aes(xintercept = 34)) +
  geom_vline(data = filter(merged_probcombine, conclusion == "Match"),
             aes(xintercept = 98.9)) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  facet_grid(Type ~ conclusion, labeller = labeller(Type = label_both, conclusion = conclusion_labs)) +
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()
hide<-dev.off()

include_graphics(path = "images/probalgorithm.png")

```

For both Cole and the gun, values are slightly more concentrated toward the lower end of the scale when the algorithm is absent for the elimination condition, and the inconclusive decision produced similar densities across both algorithm conditions. 
In the case of the gun's involvement, values are more concentrated toward the higher end of the scale when the algorithm is present for the identification condition. 
There is no real difference between identification distributions for the probability that Cole committed the crime. 
The vertical lines indicate the match score presented by the algorithm (0.034 or 3.4% for the elimination condition, 0.34 or 34% for the inconclusive condition, and 0.989 or 98.9% for the identification condition). 
These are displayed in order to visually assess whether the subjects are anchoring to the given match score when assessing the probability of the gun being present at the crime scene. 
Because the match score is on a scale of 0 to 1, this value could be misinterpreted as a probability for the gun being used in the crime. 
It does not appear that the participants are anchoring to this value, however, as the distributions do not correspond more to the line when the algorithm is present.


```{r}
#| Coleprobalgorithm,
#| fig.cap= "Probability Cole committed the crime. Black lines indicate values produced by the algorithm",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(merged_results, aes(x = probability, fill = algorithm)) +
  geom_density(alpha = 0.5, color=NA) +
  geom_vline(data = filter(merged_results, conclusion == "NoMatch"),
             aes(xintercept = 3.4)) +
  geom_vline(data = filter(merged_results, conclusion == "Inconclusive"),
             aes(xintercept = 34)) +
  geom_vline(data = filter(merged_results, conclusion == "Match"),
             aes(xintercept = 98.9)) +
  ggtitle("Probability Cole Committed the Crime") +
  facet_grid(. ~ conclusion, labeller =  purrr::partial(label_both, sep = ":\n")) +
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()

table(merged_results$guilty, merged_results$conclusion)
sum(merged_results$guilty=="Yes" & merged_results$conclusion=="NoMatch")
sum(merged_results$conclusion=="NoMatch")
```

#### Probability and Guilt

After reading the testimony, the participants were given the following question: 
"The State has the burden of proving beyond a reasonable doubt that the defendant is the person who committed the alleged crime. 
If you are not convinced beyond a reasonable doubt that the defendant is the person who committed the alleged crime, you must find the defendant not guilty. 
Would you convict this defendant, based on the evidence that you have heard?". 
`r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="NoMatch")` out of `r sum(merged_results$conclusion=="NoMatch")` individuals in the elimination category, `r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Inconclusive")` out of `r sum(merged_results$conclusion=="Inconclusive")` individuals in the inconclusive category, and `r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Match")` out of `r sum(merged_results$conclusion=="Match")` individuals in the identification category chose to convict, despite the fact that the bullet comparison testimony was the only evidence presented. 
As Figure \@ref(fig:probguilt) illustrates, across all categories individuals who chose to convict generally assigned a higher probability to Cole committing the crime than those who did not choose to convict. 
The same general trend of higher probability values for those who chose to convict and lower probability values for those who chose not to convict is also seen for elimination and inconclusive conditions when discussing the probability that the gun was used in the crime. 
However, in the case of the identification condition, those who did not convict gave a generally higher probability that the gun was used in the crime than the probability that Cole committed the crime, resulting in comparable values (albeit with more variability) to those who chose to convict.

```{r}
#| probguilt,
#| fig.cap= "Probabilities based on whether the participants thought the defendant was guilty",
#| fig.width= 10,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

probplot <-
  ggplot(merged_results, aes(x = conclusion, y = probability, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  scale_x_discrete(labels=conclusion_labs)+
  theme_bw()

gunplot <-
  ggplot(merged_results, aes(x = conclusion, y = gunprob, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Gun was used in the Crime") +
  ylab("probability")+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  scale_x_discrete(labels=conclusion_labs)+
  theme_bw()

grid.arrange(probplot, gunplot, ncol = 2)

```


### Credibility

Through all conditions, the level of credibility remained approximately the same for both the firearms examiner and the algorithm expert. 
As Figures \@ref(fig:cred) and \@ref(fig:algcred) demonstrate, "Extremely credible" was by far the most selected category for the firearms examiner and the algorithm expert, with some people selecting "Moderately credible", while the other categories quickly drop off in responses. 
This trend can be seen in most histograms resulting from this study (leading to a question of scale compression). 
The lack of difference based on examiner decision, image, or algorithm (in the case of the firearms examiner) provides a hopeful indication that the credibility of expert witnesses is not necessarily swayed by the examiner's decision or the presence of images, when their written testimony remains the same.

```{r}
#| cred,
#| fig.cap= "Histogram of firearms examiner credibility",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| warning= FALSE,
#| message= FALSE,
#| echo= FALSE


merged_results$firetestcred = factor(
  merged_results$firetestcred,
  levels = c(
    "Extremely noncredible",
    "Moderately noncredible",
    "Weakly noncredible",
    "Neither credible nor noncredible",
    "Weakly credible",
    "Moderately credible",
    "Extremely credible"
  )
)

levels(merged_results$firetestcred) <-
  gsub(" ", "\n", levels(merged_results$firetestcred))
# 
# examcred <-
#   ggplot(subset(merged_results,!is.na(firetestcred)),
#          aes(x = firetestcred, fill = conclusion)) +
#   geom_bar(
#     mapping = aes(y =after_stat(prop), group = conclusion),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #geom_histogram(stat="count", position="dodge")+
#   facet_grid(algorithm ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
#   ggtitle("How credible did you find the testimony of Terry Smith\n(the firearm examiner)?") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())
#   
# 
# levels(algorithm_results$algtestcred) <-
#   gsub(" ", "\n", levels(algorithm_results$algtestcred))
# 
# algcred <-
#   ggplot(subset(algorithm_results,!is.na(algtestcred)),
#          aes(x = algtestcred, fill = conclusion)) +
#   geom_bar(mapping = aes(y = after_stat(prop), group = conclusion),
#            position = "dodge", color="black") +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(picture ~ ., labeller = label_both) +
#   ggtitle("Algorithm Expert Testimony") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

#grid.arrange(examcred, algcred, ncol=2)
# examcred


merged_results %>% 
  filter(!is.na(firetestcred)) %>%
  group_by(conclusion, algorithm, picture, firetestcred) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = firetestcred)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How credible did you find the testimony of Terry Smith (the firearm examiner)?") +
  scale_fill_manual("", values = c("#8c510a", "#d8b365", "#f6e8c3", "#f5f5f5", "#c7eae5","#5ab4ac", "#01665e")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))
```


```{r}
#| algcred,
#| fig.cap= "Histogram of algorithm expert credibility",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| warning= FALSE,
#| message= FALSE,
#| echo= FALSE

algorithm_results <- merged_results %>% filter(algorithm == "Yes")
algorithm_results$algtestcred = factor(
  algorithm_results$algtestcred,
  levels = c(
    "Extremely noncredible",
    "Moderately noncredible",
    "Weakly noncredible",
    "Neither credible nor noncredible",
    "Weakly credible",
    "Moderately credible",
    "Extremely credible"
  )
)


levels(algorithm_results$algtestcred) <-
  gsub(" ", "\n", levels(algorithm_results$algtestcred))

algorithm_results %>% 
  filter(!is.na(algtestcred)) %>%
  group_by(conclusion, algorithm, picture, algtestcred) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = algtestcred)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How credible did you find the testimony of Adrian Jones (the algorithm expert)?") +
  scale_fill_manual("", values = c("#8c510a", "#d8b365", "#f6e8c3", "#f5f5f5", "#c7eae5","#5ab4ac", "#01665e")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))
```

### Reliability


In terms of reliability, participants appeared to find the case evidence less reliable when an inconclusive decision was given than they did when a conclusive decision was reached, as shown in Figure \@ref(fig:caserel). 
Note that this question was only answered by participants who received the algorithm condition and is meant to encompass both the examiner's comparison and the algorithm comparison. 
As with the credibility condition, the majority of individuals selected the two highest conditions, "Moderately reliable" and "Extremely reliable", across all categories of conclusions. 
In the case of an inconclusive decision, the highest proportion of participants selected "Moderately reliable", while for the conclusive decisions, the highest proportion of participants selected "Extremely reliable".

```{r}
#| caserel,
#| fig.cap= "Histogram of overall case reliability",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

# df.overrel <- ddply(
#   algorithm_results,
#   .(conclusion),
#   summarise,
#   prop = prop.table(table(overrel)),
#   overrel = names(table(overrel))
# )

merged_results$overrel = factor(
  merged_results$overrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

#Reorder factors (match/inconclusive/nomatch)
#gsub on axis (replace spaces with new lines)
levels(merged_results$overrel) <-
  gsub(" ", "\n", levels(merged_results$overrel))
# ggplot(df.overrel, aes(overrel, prop, fill = conclusion)) +
#   geom_bar(stat = "identity", position = position_dodge(preserve = "single"), color="black") +
#   ggtitle("How reliable do you think the firearm evidence in this case is?") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

merged_results %>% 
  filter(!is.na(overrel)) %>%
  group_by(conclusion, algorithm, picture, overrel) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = overrel)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How reliable do you think the firearm evidence in this case is?") +
  scale_fill_manual("", values = c("#d77227", "#fc8d59", "#fee090", "#ffffbf", "#e0f3f8","#91bfdb", "#4575b4")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))


```

Individuals were also asked to rate the general reliability of firearms evidence as a field (Figure \@ref(fig:genrel)). 
As with case reliability, those who received an inconclusive condition were more likely to select "Moderately reliable" than they were to select "Extremely reliable". 
This trend is also shown in the elimination condition when the algorithm is present. 
However, it is not reflected in the identification condition. 
When the algorithm is absent, both the identification and the elimination conditions appear to produce approximately equal proportions for the two highest categories of reliability. 
As with previous responses, all other categories are sparsely populated.

```{r}
#| genrel,
#| fig.cap= "Histogram of perceived firearm reliability as a field",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$firerel = factor(
  merged_results$firerel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(merged_results$firerel) <-
  gsub(" ", "\n", levels(merged_results$firerel))

# ggplot(subset(merged_results,!is.na(firerel)),
#        aes(x = firerel, fill = conclusion)) +
#   geom_bar(
#     mapping = aes(y = after_stat(prop), group = conclusion),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(algorithm ~ ., labeller = label_both) +
#   ggtitle("In general, how reliable do you think firearm evidence is?") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

merged_results %>% 
  filter(!is.na(firerel)) %>%
  group_by(conclusion, algorithm, picture, firerel) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = firerel)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("In general, how reliable do you think firearm evidence is?") +
  scale_fill_manual("", values = c("#d77227", "#fc8d59", "#fee090", "#ffffbf", "#e0f3f8","#91bfdb", "#4575b4")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))

```



In all conditions, participants were asked to rate the reliability of the examiner's subjective opinion of the firearm evidence (Figure \@ref(fig:examrel)).
Participants from both the algorithm and non-algorithm groups gave similar reliability ratings in the elimination condition: most chose "Moderately reliable" or "Extremely reliable", with more choosing "Extremely reliable". 
For inconclusive and identification conditions, there is a difference in proportions of which category is selected based on the presence or the absence of the algorithm. 
When the algorithm is absent, the trend is fairly similar to that in the elimination condition: between the two highest categories, the majority chose "Extremely reliable". 
However, in the case that the algorithm was present, this trend was flipped: more participants chose "Moderately reliable" over "Extremely reliable". 

```{r}
#| examrel,
#| fig.cap= "Histogram of perceived reliability of the firearms examiner's comparison",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$examrel = factor(
  merged_results$examrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(merged_results$examrel) <-
  gsub(" ", "\n", levels(merged_results$examrel))

# ggplot(subset(merged_results,!is.na(examrel)),
#        aes(x = examrel, fill = algorithm)) +
#   geom_bar(
#     mapping = aes(y = ..prop.., group = algorithm),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
#   ggtitle(
#     "How reliable do you think the firearms examiner's subjective opinion \n of the bullet comparison is, in this case?"
#   ) +
#   scale_fill_manual(values = c("grey20", "tomato")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

## Cannot center title: faceting variable error
#  theme(plot.title=element_text(hjust=0.5))+

merged_results %>% 
  filter(!is.na(examrel)) %>%
  group_by(conclusion, algorithm, picture, examrel) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = examrel)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How reliable do you think the firearms examiner's subjective opinion \n of the bullet comparison is, in this case?") +
  scale_fill_manual("", values = c("#d77227", "#fc8d59", "#fee090", "#ffffbf", "#e0f3f8","#91bfdb", "#4575b4")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))

```

Individuals who received the algorithm were also asked to rate algorithm reliability, responses are shown in Figure \@ref(fig:algrel). 
The responses were in many ways similar to those given in the case of general reliability: those who received an inconclusive decision were more likely to give a lower reliability rating, and the two highest categories were by far the most selected. 
Both also demonstrated a higher selection of "Weakly reliable" when individuals were presented with an inconclusive decision.

```{r}
#| algrel,
#| fig.cap= "Histogram of perceived algorithm reliability",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

# df.algrel <- ddply(
#   algorithm_results,
#   .(conclusion),
#   summarise,
#   prop = prop.table(table(algrel)),
#   algrel = names(table(algrel))
# )
# 
# df.algrel$algrel = factor(
#   df.algrel$algrel,
#   levels = c(
#     "Extremely unreliable",
#     "Moderately unreliable",
#     "Weakly unreliable",
#     "Neither reliable nor unreliable",
#     "Weakly reliable",
#     "Moderately reliable",
#     "Extremely reliable"
#   )
# )
# 
# levels(df.algrel$algrel) <-
#   gsub(" ", "\n", levels(df.algrel$algrel))

# ggplot(df.algrel, aes(algrel, prop, fill = conclusion)) +
#   geom_bar(stat = "identity", position = position_dodge(preserve = "single"), color="black") +
#   ggtitle("How reliable do you think the firearm algorithm evidence is,\n in this case?") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

merged_results$algrel = factor(
  merged_results$algrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(merged_results$algrel) <-
  gsub(" ", "\n", levels(merged_results$algrel))

merged_results %>% 
  filter(!is.na(algrel)) %>%
  group_by(conclusion, algorithm, picture, algrel) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = algrel)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() +
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How reliable do you think the firearm algorithm evidence is,\n in this case?") +
  scale_fill_manual("", values = c("#d77227", "#fc8d59", "#fee090", "#ffffbf", "#e0f3f8","#91bfdb", "#4575b4")) +
  theme_bw()+
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))


```

In most cases, individuals gave lower reliability ratings when an inconclusive decision was reached. 
They also tended to select the two highest reliability categories, "Moderately reliable" and "Extremely reliable", regardless of other conditions.
The presence or absence of images did not have a noticeable effect on reliability ratings. 
The presence of the algorithm is related to a slight reduction in reliability ratings for the firearms examiner's personal bullet comparison, in the case of an inconclusive or identification decision.

### Scientificity

Participants were also asked about how scientific they felt the process was, in a similar four-question format to reliability. 
These results bore some similarity in responses to reliability across questions. 
As previously stated, images did not appear to have a large effect and the two highest categories were by far the most selected. 
When asked about their rating of how scientific the evidence was in the case overall, those receiving the inconclusive condition were less likely to select "Extremely scientific" compared to their counterparts given conclusive conditions, as shown in Figure \@ref(fig:casesci).  

```{r}
#| casesci,
#| fig.cap= "Histogram of perceived scientificity in this case",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$oversci = factor(
  merged_results$oversci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$oversci) <-
  gsub(" ", "\n", levels(merged_results$oversci))

merged_results %>% 
  filter(!is.na(oversci)) %>%
  group_by(conclusion, algorithm, picture, oversci) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = oversci)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() + 
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How scientific do you think the firearm evidence in this case is?") +
  scale_fill_manual("", values = c("#b35806", "#f1a340", "#fee0b6", "#f7f7f7", "#d8daeb","#998ec3", "#542788")) +
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))


```

In terms of firearms evidence as a field, results are shown in Figure \@ref(fig:gensci). 
Here, as before, those who received the inconclusive decision were more likely to select "Moderately scientific" over "Extremely scientific" for both cases of the algorithm. 
A higher proportion of individuals selected "Extremely scientific" for conclusive decisions when the algorithm was present compared to when the algorithm was absent.

```{r}
#| gensci,
#| fig.cap= "Histogram of perceived firearm scientificity as a field",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$firesci = factor(
  merged_results$firesci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$firesci) <-
  gsub(" ", "\n", levels(merged_results$firesci))

# ggplot(subset(merged_results,!is.na(firesci)),
#        aes(x = firesci, fill = algorithm)) +
#   geom_bar(
#     mapping = aes(y = ..prop.., group = algorithm),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
#   ggtitle("In general, how scientific do you think the firearm evidence is?") +
#   scale_fill_manual(values = c("grey20", "tomato")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

merged_results %>% 
  filter(!is.na(firesci)) %>%
  group_by(conclusion, algorithm, picture, firesci) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = firesci)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() + 
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("In general, how scientific do you think the firearm evidence is?") +
  scale_fill_manual("", values = c("#b35806", "#f1a340", "#fee0b6", "#f7f7f7", "#d8daeb","#998ec3", "#542788")) +
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))


```

Regarding how scientific individuals found the examiner's comparison, the algorithm only seemed to have an effect when individuals were presented with an inconclusive decision (Figure \@ref(fig:examsci)). 
When the algorithm was not present, people were more likely to select "Extremely scientific", resulting in a proportion on par with conclusive results. 
When the algorithm was present, however, "Moderately scientific" became the most popular choice for those who received an inconclusive decision, which reflects the general trend of the majority of participants selecting the second highest category for inconclusive results. 
Results for conclusive categories are fairly similar across algorithm conditions, with a close split between "Moderately scientific" and "Extremely scientific".

```{r}
#| examsci,
#| fig.cap= "Histogram of perceived scientificity of the bullet comparison of the firearm examiner",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$examsci = factor(
  merged_results$examsci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$examsci) <-
  gsub(" ", "\n", levels(merged_results$examsci))

# ggplot(subset(merged_results,!is.na(examsci)),
#        aes(x = examsci, fill = algorithm)) +
#   geom_bar(
#     mapping = aes(y = ..prop.., group = algorithm),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
#   ggtitle(
#     "How scientific do you think the firearms examiner\'s subjective opinion \n of the bullet comparison evidence is, in this case?"
#   ) +
#   scale_fill_manual(values = c("grey20", "tomato")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())


merged_results %>% 
  filter(!is.na(examsci)) %>%
  group_by(conclusion, algorithm, picture, examsci) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = examsci)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() + 
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How scientific do you think the firearms examiner\'s subjective opinion \n of the bullet comparison evidence is, in this case?") +
  scale_fill_manual("", values = c("#b35806", "#f1a340", "#fee0b6", "#f7f7f7", "#d8daeb","#998ec3", "#542788")) +
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))

```

Participants gave the algorithm a high rating in scientificity across all categories of conclusions, as shown in Figure \@ref(fig:algsci). 
Here, all decisions had the highest proportion of respondents select "Extremely scientific". 
The proportion selecting "Moderately scientific" is noticeably lower than the proportion selecting "Extremely scientific".
The use of demonstrative evidence did not have a visible effect.

```{r}
#| algsci,
#| fig.cap= "Histogram of perceived algorithm scientificity in this case",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

# algorithm_results$algsci = factor(
#   algorithm_results$algsci,
#   levels = c(
#     "Extremely unscientific",
#     "Moderately unscientific",
#     "Weakly unscientific",
#     "Neither scientific nor unscientific",
#     "Weakly scientific",
#     "Moderately scientific",
#     "Extremely scientific"
#   )
# )
# 
# levels(algorithm_results$algsci) <-
#   gsub(" ", "\n", levels(algorithm_results$algsci))

# ggplot(subset(algorithm_results,!is.na(algsci)),
#        aes(x = algsci, fill = conclusion)) +
#   geom_bar(
#     mapping = aes(y = ..prop.., group = conclusion),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(picture ~ ., labeller = label_both) +
#   ggtitle("How scientific do you think the firearm algorithm evidence is, \n in this case?") +
#   scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())

merged_results$algsci = factor(
  merged_results$algsci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$algsci) <-
  gsub(" ", "\n", levels(merged_results$algsci))

merged_results %>% 
  filter(!is.na(algsci)) %>%
  group_by(conclusion, algorithm, picture, algsci) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
ggplot(aes(x = picture, fill = algsci)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() + 
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("How scientific do you think the firearm algorithm evidence is, \n in this case?") +
  scale_fill_manual("", values = c("#b35806", "#f1a340", "#fee0b6", "#f7f7f7", "#d8daeb","#998ec3", "#542788")) +
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))


```

In summary, the algorithm is related an an increase of perceived scientificity for the field of firearm evidence as a whole when individuals were presented with a conclusive decision, and "Extremely scientific" was the most selected category when evaluating the scientificity of the algorithm evidence regardless of conclusion. 
As for how individuals rated the scientificity of the algorithm expert's comparison, the algorithm only appeared to influence results for those who received the inconclusive condition. 
Individuals who received the algorithm were more likely to select "Moderately scientific", while those who did not receive the algorithm were more likeley to select "Extremely scientific". 

### Understanding

Individuals were asked to rate their understanding of both the algorithm and the examiner's personal bullet comparison on a 5-point Likert scale. Most responses ranged from 3 ("I understood about half of the method") to 5 ("I understood everything"), leading to less scale compression than was seen in scales relating to credibility, reliability, and scientificity.  

```{r}
#| expunder,
#| fig.cap= "Histogram of understanding for the explanation of the firearms examiner",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$expunder = factor(
  merged_results$expunder,
  levels = c(
    "1 <br/> I understood nothing",
    "2.0",
    "3 <br/> I understood about half of the method",
    "4.0",
    "5 <br/> I understood everything"
  )
)

levels(merged_results$expunder) <-
  gsub("<br/>", "", levels(merged_results$expunder))

# levels(merged_results$expunder) <-
#   gsub("  ", " ", levels(merged_results$expunder))


expunderdata <- merged_results %>% 
  filter(!is.na(expunder)) %>%
  group_by(conclusion, algorithm, picture, expunder) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
      mutate(expunder = stringr::str_remove(expunder, "<br/>|.0") %>%
           stringr::str_remove(., "I understood "))

expunderdata %>%  ggplot(aes(x = picture, fill = expunder)) +
  geom_col(
    mapping = aes(y =prop),
    position = position_stack(), color="black"
  ) +
  coord_flip() + 
  facet_grid(algorithm~conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs)) +
  ggtitle("Based on this testimony, how would you rate your understanding of \n the method described for the examiner's personal bullet comparison?") +
  scale_fill_manual("I understood", values = c("#d01c8b", "#f1b6da", "#f7f7f7", "#b8e186", "#4dac26")) + 
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
    guides(fill = guide_legend(nrow = 1, reverse = TRUE))

```

For the firearms examiner's personal comparison, few individuals selected that they understood less than half the method (Figure \@ref(fig:expunder)). 
Those who did not receive the algorithm were more likely to select "I understood everything" compared to those who did receive the algorithm, across all conclusions. 
There was not a discernible difference in responses based on the presence or absence of images.

```{r}
#| allunder,
#| fig.cap= "Histogram of understanding for the explanation of the firearms examiner",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

# merged_results$expunder = factor(
#   merged_results$expunder,
#   levels = c(
#     "1 <br/> I understood nothing",
#     "2.0",
#     "3 <br/> I understood about half of the method",
#     "4.0",
#     "5 <br/> I understood everything"
#   )
# )
# 
# levels(merged_results$expunder) <-
#   gsub("<br/>", "", levels(merged_results$expunder))
# 
# levels(merged_results$expunder) <-
#   gsub("  ", " ", levels(merged_results$expunder))
# 
# levels(merged_results$expunder) <-
#   gsub(" ", "\n", levels(merged_results$expunder))
# 
# ggplot(subset(merged_results,!is.na(expunder)),
#        aes(x = expunder, fill = algorithm)) +
#   geom_bar(
#     mapping = aes(y = ..prop.., group = algorithm),
#     position = position_dodge(preserve = "single"), color="black"
#   ) +
#   #  geom_histogram(stat="count", position="dodge")+
#   facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
#   ggtitle(
#     "Based on this testimony, how would you rate your understanding of \n the method described for the examiner's personal bullet comparison?"
#   ) +
#   scale_fill_manual(values = c("grey20", "tomato")) +
#   theme_bw()+
#   theme(axis.title.x = element_blank())+ 
#   scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))


histunderdata <- merged_results %>%
  pivot_longer(c(algunder, expunder), names_to = "Question", values_to = "Understanding") %>%
  mutate(Question = stringr::str_replace_all(Question, c("algunder" = "Algorithm", "expunder" = "Examiner"))) %>%
  filter(!(Question =="Algorithm" & algorithm == "No")) %>%
  group_by(conclusion, picture, Question, algorithm, Understanding) %>%
  count() %>%
  group_by(conclusion, picture, Question, algorithm) %>%
  mutate(prop = n/sum(n)) %>%
  mutate(Understanding = stringr::str_remove(Understanding, "<br/>|.0") %>%
           stringr::str_remove(., "I understood "))

histunderdata %>%
  filter(algorithm == "Yes") %>%
ggplot(aes(x = Question, fill = Understanding)) + 
  geom_col(aes(y = prop), position = "stack", color = "black") + 
  facet_grid(picture~conclusion, labeller = labeller(picture = label_both, conclusion = conclusion_labs)) + 
  coord_flip() + 
  scale_fill_manual("I understood", values = c("#d01c8b", "#f1b6da", "#f7f7f7", "#b8e186", "#4dac26")) + 
  theme_bw() + 
  ylab("Proportion") +
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
  guides(fill = guide_legend(nrow = 1, reverse = TRUE))

```

The results for the particpants' understanding of the algorithm description differs based on the presence or absence of images (Figure \@ref(fig:allunder)).
When images are absent, participants selected values from "I understood about half the method" to "I understood everything" with a fairly uniform frequency, with a few participants selecting values in the lower categories. 
When images were present, however, category selection appeared to relate to conclusion. 
Those receiving the identification condition were more likely to select 4 than other categories, meaning that they felt they understood more than half of the method but didn't understand everything. 
Those receiving the elimination condition were more likely to select that they understood half of the method compared to other categories. 
Those with an inconclusive condition had responses fairly evenly distributed across the top three categories, similar to when images were absent.
As in the case without images, few individuals indicated that they understood less than half of the method.
It is unclear what may have caused these differences in ratings of understanding.

```{r}
#| algunder,
#| fig.cap= "Histogram of understanding for the algorithm explanation",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE,
#| eval = FALSE

algorithm_results$algunder = factor(
  algorithm_results$algunder,
  levels = c(
    "1 <br/> I understood nothing",
    "2.0",
    "3 <br/> I understood about half of the method",
    "4.0",
    "5 <br/> I understood everything"
  )
)

levels(algorithm_results$algunder) <-
  gsub("<br/>", "", levels(algorithm_results$algunder))

levels(algorithm_results$algunder) <-
  gsub("  ", " ", levels(algorithm_results$algunder))

levels(algorithm_results$algunder) <-
  gsub(" ", "\n", levels(algorithm_results$algunder))

ggplot(subset(algorithm_results,!is.na(algunder)),
       aes(x = algunder, fill = conclusion)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = conclusion),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture ~ ., labeller = label_both) +
  ggtitle(
    "Based on this testimony, how would you rate your understanding of \n the method described for the bullet matching algorithm?"
  ) +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))

```

To investigate this potential relationship further, Table \@ref(tab:undertb) was created. Because the counts are so small for the individual cells, the resemblance to a relationship based on conclusion is possibly due to random variation.

```{r}
#| echo= FALSE,
#| message= FALSE,
#| eval= TRUE,
#| warning = FALSE

library(pander)
pander(table(subset(algorithm_results,!is.na(algunder) & picture=="Yes")$algunder,
      subset(algorithm_results,!is.na(algunder) & picture=="Yes")$conclusion),
      caption = "(\\#tab:undertb) Understanding Frequency")
# %>% 
#   column_spec(1, width = "10em")
```

```{r}
#| examunder,
#| fig.cap= "Histogram of understanding for the firearms examiner's explanation",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE,
#| eval = FALSE

merged_results <- merged_results %>%
  mutate(expunder = stringr::str_remove(expunder, "<br/>") %>%
           stringr::str_remove("\\n") %>%
           stringr::str_remove("I understood ") %>%
           stringr::str_remove("\\.0") %>%
           stringr::str_replace("\\s{2,}", " "))

merged_results %>%
  filter(!is.na(expunder)) %>%
  group_by(conclusion, algorithm, picture, expunder) %>%
  count() %>%
  group_by(conclusion, algorithm, picture) %>%
  mutate(prop = n/sum(n)) %>%
  ggplot(aes(x = picture, fill = expunder)) + 
  geom_col(aes(y = prop, position = "stack"),  color="black") + 
  coord_flip() + 
  scale_fill_manual("I understood", values = c("#d01c8b", "#f1b6da", "#f7f7f7", "#b8e186", "#4dac26")) + 
  theme_bw() + 
  theme(legend.position = "bottom", legend.direction = "horizontal") + 
  ylab("Proportion of responses") + 
  guides(fill = guide_legend(nrow = 1, reverse = TRUE)) + 
  facet_grid(algorithm ~ conclusion, labeller = labeller(algorithm = label_both, conclusion = conclusion_labs))
```


### Uniqueness

Individuals were asked whether or not they thought that guns left unique markings on discharged bullets and casings after reviewing the testimony. 
Of the `r dim(merged_results)[1]` responses, only `r sum(merged_results$unique=="No")` individuals indicated that they did not think guns left unique markings. 
These respondents were split across conditions. 
Thus, respondents in this study overwhelmingly believe in the uniqueness of markings on discharged bullets. 

```{r, eval=F, echo=F}

table(merged_results$unique)
sum(merged_results$unique=="No")
```

### Strength

```{r}
#| strength,
#| fig.cap= "Histogram of perceived strength of evidence against the defendant",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE
# How strong is the evidence that the defendant's gun was used to fire the shot in
             #       the convenience store, in your opinion?

merged_results$strength=factor(merged_results$strength, levels =c("1 <br/> Not at all strong","2.0", "3.0",
                                                                  "4.0","5 <br/> Moderately strong",
                                                                  "6.0", "7.0", "8.0", "9 <br/> Extremely strong"))

levels(merged_results$strength) <-
  gsub("<br/>", "", levels(merged_results$strength))

levels(merged_results$strength) <-
  gsub("  ", " ", levels(merged_results$strength))

levels(merged_results$strength) <-
  gsub(" ", "\n", levels(merged_results$strength))

ggplot(subset(merged_results, !is.na(strength)), aes(x=strength, fill = algorithm))+
  geom_bar(mapping = aes(y = ..prop.., group = algorithm), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion~., labeller = labeller(conclusion = conclusion_labs)) +
  ggtitle(
    "How strong would you say the case against the defendant is?"
  ) +
  scale_fill_manual("Algorithm", values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank(),
        legend.position = c(1, 1), legend.justification = c(1, 1), 
        legend.direction = "horizontal", legend.background = element_rect(fill = "transparent"))+ 
  scale_x_discrete(labels=c('1\nNot\nat all\nstrong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))


```

Individuals were also asked to rate the strength of evidence, both against Richard Cole and against the gun. 
Unlike questions of reliability, credibility, and scientificity, strength of evidence was measured on a 9-point Likert scale, without worded values for intermittent categories, following the procedure of @garrettMockJurorsEvaluation2020.

When asked about the case against the defendant, shown in Figure \@ref(fig:strength), there was a small difference in terms of the algorithm. 
When the examiner reached a elimination conclusion, individuals who also received the algorithm were more likely to select the lowest category ("Not at all strong") compared to those who did not receive the algorithm. 
Alternatively, in the case of an inconclusive decision, individuals who did not receive the algorithm were more likely to select "Not at all strong" compared to those who did receive the algorithm.
For the identification condition, responses were more widely distributed.

```{r}
#| gunstrength,
#| fig.cap= "Histogram of perceived strength of evidence against the gun",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$gunstrength=factor(merged_results$gunstrength, levels =c("1 <br/> Not at all strong","2.0", "3.0",
                                                                  "4.0","5 <br/> Moderately strong",
                                                                  "6.0", "7.0", "8.0", "9 <br/> Extremely strong"))

levels(merged_results$gunstrength) <-
  gsub("<br/>", "", levels(merged_results$gunstrength))

levels(merged_results$gunstrength) <-
  gsub("  ", " ", levels(merged_results$gunstrength))

levels(merged_results$gunstrength) <-
  gsub(" ", "\n", levels(merged_results$gunstrength))

ggplot(subset(merged_results, !is.na(gunstrength)), aes(x=gunstrength, fill = algorithm))+
  geom_bar(mapping = aes(y = ..prop.., group = algorithm), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion~., labeller = labeller(conclusion = conclusion_labs)) +
  ggtitle(
    "How strong is the evidence that the defendant's gun
    \nwas used to fire the shot in the convenience store, in your opinion?"
  ) +
  scale_fill_manual("Algorithm", values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank(),
        legend.position = c(1, 1), legend.justification = c(1, 1), 
        legend.direction = "horizontal", legend.background = element_rect(fill = "transparent"))+ 
  scale_x_discrete(labels=c('1\nNot at\nall strong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))


```

When asked about the strength of evidence against the defendant's gun, there was no real difference between those who received the algorithm and those who did not (Figure \@ref(fig:gunstrength)).
The identification condition resulted in a more concentrated distribution at higher strength values than were seen for the strength of evidence against Cole.

```{r}
#| probstrength,
#| fig.cap= "Probabilities based on perceived strength of evidence.",
#| fig.width= 10,
#| fig.height= 4.5,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

strengthdat <- merged_results %>% 
  select(strength, probability, conclusion, gunstrength, algorithm, picture) %>%
  pivot_longer(cols=c("strength", "gunstrength"), names_to="strength", values_to="scale") %>%
  mutate(strength = stringr::str_replace_all(strength, c("gunstrength" = "Gun", "^strength$" = "Defendant")))

levels(strengthdat$scale) <-
  gsub(" ", "\n", levels(strengthdat$scale))

 ggplot(strengthdat, aes(x = scale, y = probability, color = strength, fill = strength)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    alpha = 0.5,
    size = 0.5
  ) +
  geom_violin(position = position_dodge(1),
              draw_quantiles=c(.25, .5, .75),
              color = "black",
              alpha = 0.5,
              outlier.shape = NA) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  scale_color_manual("Strength of\nEvidence Against", values = c("orange", "purple"))+
  scale_fill_manual("Strength of\nEvidence Against", values = c("orange", "purple"))+
   theme_bw()+
  theme(axis.title.x = element_blank(), 
        legend.position = c(0, 1), legend.justification = c(0, 1), 
        legend.direction = "horizontal", legend.background = element_rect(fill = "transparent")) + 
  scale_x_discrete(labels=c('1\nNot at\nall strong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))
```

In comparing how individuals scored strength of evidence and the probability that they assigned to Cole committing the crime and the gun being present at the crime scene, the results were largely consistent (Figure \@ref(fig:probstrength)). 
In general, the probability assigned increases as the assigned strength of evidence increases, which is to be expected. 

### Mistakes

Individuals were asked how often firearms examiners make mistakes when determining whether bullets were fired through the same gun, with a scale from "Never" to "Usually". 
"Rarely" was by far the most selected category, as shown in Figure \@ref(fig:mistakes). 
There does not appear to be a strong relationship between how often individuals felt firearms examiners made mistakes and factors such as conclusion, images, or the algorithm.

```{r}
#| mistakes,
#| fig.cap= "Histogram of perceived frequency of mistakes made by firearms examiners",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$mistakes=factor(merged_results$mistakes, levels =c("Never","Rarely", "Occasionally",
                                                                  "Sometimes", "Frequently", "Usually", "Always"))


ggplot(subset(merged_results, !is.na(mistakes)), aes(x=mistakes, fill = conclusion))+
  geom_bar(mapping = aes(y = ..prop.., group = conclusion), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture~., labeller = label_both) +
  ggtitle(
    "How often do firearms examiners make mistakes when determining whether \n bullets were fired through the same gun?"
  ) +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7"), labels = conclusion_labs) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```


### Comparing Algorithm Values to Examiner Values 

Participants who received the algorithm condition were asked to evaluate their feelings on the reliability, credibility, scientificity, and their understanding of both the algorithm method as well as the traditional bullet analysis method, as discussed in previous sections. 
Results for both methods were compared on an individual level as well as on a group level. 
The group level results are represented in histograms, while the individual level results are represented in parallel coordinates plots. 
Note that these results only reflect the feelings of individuals who received the algorithm condition.

#### Group Level Results

```{r}
#| histcred,
#| fig.cap= "Histogram of perceived credibility of experts",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algtest_table<-table(merged_results$algtestcred, merged_results$conclusion)
alg_df<-data.frame(algtest_table, question="algorithm")
colnames(alg_df)[2] <- "conclusion"
firetest_table<-table(merged_results[merged_results$algorithm=="Yes",]$firetestcred,merged_results[merged_results$algorithm=="Yes",]$conclusion)
fire_df<- data.frame(firetest_table, question="firearm")
colnames(fire_df)[2] <- "conclusion"
test_df<-rbind(alg_df, fire_df)

levels(test_df$Var1) <- gsub(" ", "\n", levels(test_df$Var1))

test_df$Var1=factor(test_df$Var1, levels = c("Extremely\nnoncredible","Moderately\nnoncredible","Weakly\nnoncredible",
                                                         "Neither\ncredible\nnor\nnoncredible", "Weakly\ncredible", "Moderately\ncredible",
                                                         "Extremely\ncredible"))

ggplot(test_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How credible did you find the testimony of the ___ expert?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  labeller(conclusion = conclusion_labs)) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

In terms of credibility, as shown in Figure \@ref(fig:histcred), the algorithm and the expert scored fairly similarly, with the algorithm expert resulting in slightly more individuals selecting "Extremely credible" than the firearms expert. 
As mentioned before, most participants limited their selection to the two highest categories - "Moderately credible" and "Extremely credible".

```{r}
#| histrel,
#| fig.cap= "Histogram of perceived reliability of evidence",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algrel_table<-table(merged_results$algrel, merged_results$conclusion)
algrel_df<-data.frame(algrel_table, question="algorithm")
colnames(algrel_df)[2] <- "conclusion"
examrel_table<-table(merged_results[merged_results$algorithm=="Yes",]$examrel, merged_results[merged_results$algorithm=="Yes",]$conclusion)
examrel_df<- data.frame(examrel_table, question="examiner")
colnames(examrel_df)[2] <- "conclusion"

rel_df<-rbind(algrel_df, examrel_df)



levels(rel_df$Var1) <- gsub(" ", "\n", levels(rel_df$Var1))

rel_df$Var1=factor(rel_df$Var1, levels = c("Extremely\nunreliable","Moderately\nunreliable","Weakly\nunreliable",
                                            "Neither\nreliable\nnor\nunreliable", "Weakly\nreliable", "Moderately\nreliable",
                                            "Extremely\nreliable"))

ggplot(rel_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How reliable do you think the ____ evidence is, in this case?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  labeller(conclusion = conclusion_labs)) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```

For reliability, shown in Figure \@ref(fig:histrel), a similar number of individuals selected "Extremely reliable" for both the algorithm evidence and the examiner's comparison, with a larger proportion of individuals selecting "Extremely reliable" in the identification condition when the algorithm is present. 
There appears to be, however, a difference in the categories of "Moderately reliable" and "Weakly reliable". 
Individuals were more likely to select that the algorithm was "Weakly reliable", while they were less likely to select that the algorithm was "Moderately reliable", compared to the examiner.

```{r}
#| histsci,
#| fig.cap= "Histogram of perceived scientificity of evidence",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algsci_table<-table(merged_results$algsci, merged_results$conclusion)
algsci_df<-data.frame(algsci_table, question="algorithm")
colnames(algsci_df)[2] <- "conclusion"
examsci_table<-table(merged_results[merged_results$algorithm=="Yes",]$examsci,merged_results[merged_results$algorithm=="Yes",]$conclusion)
examsci_df<- data.frame(examsci_table, question="examiner")
colnames(examsci_df)[2] <- "conclusion"

sci_df<-rbind(algsci_df, examsci_df)

levels(sci_df$Var1) <- gsub(" ", "\n", levels(sci_df$Var1))

sci_df$Var1=factor(sci_df$Var1, levels = c("Extremely\nunscientific","Moderately\nunscientific","Weakly\nunscientific",
                                         "Neither\nscientific\nnor\nunscientific", "Weakly\nscientific", "Moderately\nscientific",
                                          "Extremely\nscientific"))

ggplot(sci_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How scientific do you think the ____ evidence is, in this case?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  labeller(conclusion = conclusion_labs)) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

Individuals generally saw the algorithm as more scientific than the expert, as shown in Figure \@ref(fig:histsci). 
They were more likely to select "Extremely scientific" in the case of the algorithm when compared to the expert. 
Accordingly, they were less likely to select "Moderately scientific" in the case of the algorithm when compared to the examiner. 
No real difference can be seen in the lower values on the scale, due to few individuals selecting these categories.

```{r}
#| histunder,
#| fig.cap= "Histogram of participants\' understanding",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algunder_table<-table(merged_results$algunder, merged_results$conclusion)
algunder_df<-data.frame(algunder_table, question="algorithm")
colnames(algunder_df)[2] <- "conclusion"
expunder_table<-table(merged_results[merged_results$algorithm=="Yes",]$expunder,merged_results[merged_results$algorithm=="Yes",]$conclusion)
expunder_df<- data.frame(expunder_table, question="examiner")
colnames(expunder_df)[2] <- "conclusion"

under_df<-rbind(algunder_df, expunder_df)

levels(under_df$Var1) <-
  gsub("<br/>", "", levels(under_df$Var1))

levels(under_df$Var1) <-
  gsub("  ", " ", levels(under_df$Var1))


levels(under_df$Var1) <- gsub(" ", "\n", levels(under_df$Var1))

ggplot(under_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("Based on this testimony, how would you rate your understanding of the method
                  described for the ______?") +
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  labeller(conclusion = conclusion_labs)) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))


```
Individuals rated their understanding of the examiner's bullet comparison higher than their understanding of the algorithm method, generally speaking (Figure \@ref(fig:histunder)). 
Participants were more likely to select a 4 or 5 with regards to their understanding of the examiner's comparison, while they were more likely to select a 3 or 4 with regards to their understanding of the algorithm method.
While these questions gauge how well participants believe they understand the given concepts, @dunningFlawedSelfAssessmentImplications2004 indicate that individuals are not unbiased judges of their own understanding.


#### Individual Level Results

```{r}
#| coordcred,
#| fig.cap= "Plots of understanding and perceived expert credibility",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algtestcred_reorder<- algorithm_results %>%
  mutate(algtestcred=fct_relevel(algtestcred, "Extremely noncredible",
                                 "Moderately noncredible",
                                 "Weakly noncredible",
                                 "Neither credible nor noncredible", 
                                 "Weakly credible", "Moderately credible",
                                 "Extremely credible"))

algtestcred_reorder<- algorithm_results %>%
  mutate(firetestcred=fct_relevel(firetestcred, 
                                  "Extremely noncredible","Moderately noncredible",
                                  "Weakly noncredible", 
                                  "Neither credible nor noncredible", 
                                  "Weakly credible", "Moderately credible",
                                  "Extremely credible"))


cred_data2 <- data.frame(
  conclusion = algtestcred_reorder$conclusion,
  Firearms = algtestcred_reorder$firetestcred,
  Algorithm = algtestcred_reorder$algtestcred
)

cred_tab2 <- table(cred_data2)
cred_df2 <- as.data.frame(cred_tab2)

cred_df2 <- cred_df2 %>%
  purrr::map(.f = function(x) rep(x, cred_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)

credplot <- cred_df2 %>%
  ggplot(aes(vars=vars(c(Algorithm, Firearms)))) +
  geom_pcp_box(boxwidth=0.1, fill=NA) +
  geom_pcp(aes(colour = conclusion), alpha = 0.5, 
            boxwidth=0.1) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7"), labels = conclusion_labs) +
  guides(colour=guide_legend(override.aes = list(alpha=1))) +
  theme_bw() +
  ggtitle("How credible did you find the\ntestimony of the ___ expert?")

under_data2 <- data.frame(
  conclusion = algtestcred_reorder$conclusion,
  algorithm = algtestcred_reorder$algunder,
  examiner = algtestcred_reorder$expunder
)

under_tab2 <- table(under_data2)
under_df2 <- as.data.frame(under_tab2)

under_df2 <- under_df2 %>%
  purrr::map(.f = function(x) rep(x, under_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)

underplot <- under_df2 %>%
  ggplot(aes(vars=vars(c(algorithm, examiner)))) +
  geom_pcp_box(boxwidth=0.1, fill=NA) +
  geom_pcp(aes(colour = conclusion), alpha = 0.5, 
            boxwidth=0.1) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7"), labels = conclusion_labs) +
  guides(colour=guide_legend(override.aes = list(alpha=1))) +
  theme_bw() +
  
  ggtitle("Based on this testimony, how would\nyou rate your understanding of the\nmethod described for the ______?")


grid.arrange(credplot, underplot, ncol = 2)


```
Figure \@ref(fig:coordcred) depicts parallel coordinate plots for participants' ratings of the credibility of the experts, as well as their understanding of the methods described in the testimony. 
Higher values indicate higher scores in understanding and credibility. These plots map the frequency that individuals selected the shown combination of values for credibility or understanding. 
In the case of credibility, we can see that most individuals selected the highest category of credibility for both the algorithm and the firearms expert. 
In the case of understanding, individuals tended to select the two highest categories the most. 
It can also be seen that there is a trend of individuals selecting a category lower for the algorithm compared to what they selected for the examiner by the thicker downward line between categories 4 and 5, as well as between categories 4 and 3. 
This corresponds to Figure \@ref(fig:histunder) in that individuals tend to give higher understanding ratings to the examiner's bullet comparison method than they did for the algorithm.

```{r}
#| coordscirel,
#| fig.cap= "Plots of perceived scientificity and reliability of methods",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

sci_reorder<- algorithm_results %>% mutate(algsci=fct_relevel(algsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))

sci_reorder<- sci_reorder %>% mutate(examsci=fct_relevel(examsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))

sci_data2 <- data.frame(
  conclusion = sci_reorder$conclusion,
  examiner = sci_reorder$examsci,
  algorithm = sci_reorder$algsci
)

sci_tab2 <- table(sci_data2)
sci_df2 <- as.data.frame(sci_tab2)

sci_df2 <- sci_df2 %>%
  purrr::map(.f = function(x) rep(x, sci_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)

sciplot <- sci_df2 %>%
  ggplot(aes(vars=vars(c(algorithm, examiner)))) +
  geom_pcp_box(boxwidth=0.1, fill=NA) +
  geom_pcp(aes(colour = conclusion), alpha = 0.5, 
            boxwidth=0.1) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7"), labels = conclusion_labs) +
  guides(colour=guide_legend(override.aes = list(alpha=1))) +
  theme_bw() +
  ggtitle("How scientific do you think the ____\nevidence is, in this case?")



# algorithm_results<- rbind.fill(algorithm_results, data.frame("examrel"="Extremely unreliable"))

rel_reorder<- algorithm_results %>% mutate(algrel=fct_relevel(algrel, "Extremely unreliable","Moderately unreliable","Weakly unreliable", "Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

rel_reorder<- rel_reorder %>% mutate(examrel=fct_relevel(examrel, "Extremely unreliable", "Moderately unreliable","Weakly unreliable","Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

rel_data2 <- data.frame(
  conclusion = rel_reorder$conclusion,
  examiner = rel_reorder$examrel,
  algorithm = rel_reorder$algrel
)

rel_tab2 <- table(rel_data2)
rel_df2 <- as.data.frame(rel_tab2)

rel_df2 <- rel_df2 %>%
  purrr::map(.f = function(x) rep(x, rel_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)

relplot <- rel_df2 %>%
  ggplot(aes(vars=vars(c(algorithm, examiner)))) +
  geom_pcp_box(boxwidth=0.1, fill=NA) +
  geom_pcp(aes(colour = conclusion), alpha = 0.5, 
            boxwidth=0.1) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7"), labels = conclusion_labs) +
  guides(colour=guide_legend(override.aes = list(alpha=1))) +
  theme_bw() +
  ggtitle("How scientific do you think the ____\nevidence is, in this case?")


grid.arrange(sciplot, relplot, ncol = 2)


```

Figure \@ref(fig:coordscirel) demonstrates the trends for selection for how scientific and reliable individuals felt the evidence was. 
In terms of how scientific individuals felt the evidence was, it appears that most participants selected the highest category for both the algorithm and the examiner. 
Some participants selected the second highest category for the examiner, and the highest category for the algorithm.
Others selected the second highest category in both cases.
In terms of reliability, participants tended to choose one of the two highest categories for both the algorithm and the examiner. 
Some participants switched between the two highest categories for the algorithm or the examiner, in similar numbers.  

```{r}
#| allpcp,
#| echo= FALSE,
#| warning= FALSE,
#| message= FALSE,
#| fig.width= 8,
#| fig.height= 9,
#| fig.cap = "Parallel Coordinate Plot for reliability, credibility, scientificity, and understanding",
#| eval = FALSE

algtestcred_reorder <- algorithm_results %>% mutate(conclusion = fct_relevel(conclusion, "NoMatch", "Inconclusive", "Match"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(algrel=fct_relevel(algrel, "Extremely unreliable","Moderately unreliable","Weakly unreliable", "Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(examrel=fct_relevel(examrel, "Extremely unreliable", "Moderately unreliable","Weakly unreliable","Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(examsci=fct_relevel(examsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(algsci=fct_relevel(algsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))


cred_data2 <- data.frame(
  algtestcred = algtestcred_reorder$algtestcred, 
  firetestcred = algtestcred_reorder$firetestcred, 
  conclusion = algtestcred_reorder$conclusion,
  examrel = algtestcred_reorder$examrel,
  algrel = algtestcred_reorder$algrel,
  algunder = algtestcred_reorder$algunder,
  expunder = algtestcred_reorder$expunder,
  algsci = algtestcred_reorder$algsci,
  examsci = algtestcred_reorder$examsci
)

testcred_tab2 <- table(cred_data2)
testcred_df2 <- as.data.frame(testcred_tab2)

testcred_df2 <- testcred_df2 %>%
  purrr::map(.f = function(x) rep(x, testcred_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)

# 
# testcred_df2 %>%
#   pcp_select(algunder, algsci, algtestcred, algrel, examrel, firetestcred, examsci, expunder) %>%
#   # pcp_scale() %>%
#   # pcp_arrange() %>%
#   ggplot(aes_pcp()) +
#   scale_y_continuous(labels=c("low scores", "", "", "", "high scores"),
#                      breaks=c(0,0.25, .5,.75, 1)) +
#   geom_pcp(aes(color=conclusion), alpha = 0.75) +
#   scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
#   guides(colour = guide_legend(override.aes = list(alpha = 1))) +
#   geom_pcp_boxes(fill = "transparent", color = "black", linewidth = 1)+
#   theme_bw()

testcred_df2 %>%
  ggplot(aes(vars=vars(c(algunder, algsci, algtestcred, algrel, examrel, firetestcred, examsci, expunder)))) +
  geom_pcp_box(boxwidth=0.1, fill=NA) +
  geom_pcp(aes(colour = conclusion), alpha = 0.5, 
            boxwidth=0.1, resort=2:3) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  guides(colour=guide_legend(override.aes = list(alpha=1))) +
  theme_bw() +
  ggtitle("Comparing Strength of Evidence to Multiple Choice Guilt Answer")

```

<!-- Figure \@ref(fig:allpcp) shows individual responses across all of the considered questions.  -->
<!-- There were some individuals who selected the highest category for responses across the board, while others selected the highest response for all except their understanding of the algorithm.  -->
<!-- In fact, algorithmic understanding seems to have the least consistent responses, based on the differences in the lines between algorithmic understanding and algorithmic scientificity.  -->
These coordinate plots indicate that individuals tended to choose the same categories for the algorithm and the examiner across variables, with most variation between categories resulting from individuals moving up or down a single category. 
Relatively few individuals changed their response by more than one category, when comparing between the algorithm and the expert. 
These results generally reflect the trends shown in the histograms in the previous section.

### Comments

Select comments are shown in Appendix \@ref(select-comments).
Many participants were concerned about the lawfulness of the arrest, and the lack of evidence tying the defendant specifically to the crime scene.
Several also indicate that they have pre-conceived notions, either about algorithms or about firearms evidence.
In particular, two comments were:

>"Random forest methods for machine learning and classification are not very robust.  They identify pictures of cats as hamburgers, for example.  Just use science." - Elimination, Algorithm, Demonstrative Evidence

>"Firearm evidence seems a lot like hair evidence which was debunked but I don't know that to be fact. From what was stated in this testimony, it seems to be mostly reliable but I wouldn't say it is strongly scientific." - Inconclusive, Algorithm, Demonstrative Evidence

The comments indicate that participants can have distrust for methods based on their previous ideas, which may not be factual.


## Discussion

### Summary of Results

As can be seen in most graphics, scale compression was an issue with this study. 
The vast majority of participants selected the two highest categories for questions rating how credible, reliable, and scientific the expert/method was, despite following the scale size and labeling recommendations of @grovesSurveyMethodology2009. 
This corresponds with @garrett2013's study of fingerprint match language, where they did not find a significant difference in the participants' feelings of guilt based on various match language.
@garrett2013 hypothesized that this may relate to strong feelings reliability for fingerprint evidence, resulting in individuals viewing any match as automatically reliable (without the need for stronger language).
In future studies, we will be evaluating different methods of response, aside from Likert scales, to study other formats that may yield less compressed results.
People generally seemed to have less faith in inconclusive decisions. 
They understood the examiner's comparison better than the algorithm comparison, which was expected given the statistical complexity of the algorithm procedure. 
In general, participants also found the algorithm to be more scientific than the examiner, when presented with both methods. 
The presence of images did not have a discernible effect in terms of credibility, reliability, or scientificity. 
Participants' views of the credibility of the experts and the frequency with which firearms examiners make mistakes did not depend on the algorithm, images, or conclusion.  

### Limitations

Some of the major limitations for this project relate to the format and distribution of the survey material: the pool of participants, the format of the testimony, the limited testimony, and the inability to deliberate. 
These limitations are also mentioned by @garrettMockJurorsEvaluation2020.

Participants were limited to those who participate in online survey-taking websites. 
These participants may not be representative of the US population, due to differences in computer access or use, occupation, or other such factors. 
These factors may have an effect on study responses.
Another issue in representation results from the process of jury selection.
Because individuals are not randomly approved for serving on a jury, the jury itself is unlikely to be composed of a representative sample of American citizens.
@abramsonJurySelectionWeeds2018 argue that selection for jury duty does not result in a representative sample due to some courts' reliance solely on voter registration records for contacting eligible jurors, as well as large non response and non deliverable issues that are more prevalent in African American and Hispanic communities when compared to non-Hispanic White communities.

Testimony was presented in a written format, which is unlike the courtroom setting of spoken testimony, where the jurors can see the experts.
However, the use of written testimony allowed for the use of gender-neutral names for the experts (Terry and Adrian), which may be more difficult to achieve in a courtroom or video setting. 
Potential jurors may also develop views regarding the reliability/credibility of the examiner that are dependent on external factors - such as appearance or speech - rather than based on the testimony itself.

Testimony in this case was limited to only include the firearms evidence. 
This led to some confusion on the part of the "inconclusive" and "elimination" scenarios, where there did not appear to be relevant evidence for the prosecution. 
While the goal of this format was to ensure that participants focused on the bullet matching testimony (without the compounding influence of other witnesses or evidence), this would not be representative of courtroom testimony.  

In a courtroom setting, jurors are able to deliberate with each other before reaching a conclusion with regards to the case. 
These deliberations tend to be evidence-driven, as opposed to majority rule [@bornsteinJuryDecisionMaking2011]. 
<!-- [@bornsteinJuryDecisionMaking2011, p. 65].  -->
While the majority decision may be reflected in the final result, some studies suggest that juries tend toward leniency when there is not a definitive majority in a criminal trial [@maccounAsymmetricInfluenceMock1988]. 
The act of deliberation may result in different evaluations for Likert scale questions of reliability/credibility or strength of evidence than individual thought, even if a difference in guilty verdict may not be found after deliberation for this simple case style.  

There were several typos that were not corrected for approximately the first half of the participants. 
For all scenarios, the firearms examiner was referred to as Alex Smith in the questions, whereas the name was Terry Smith throughout the testimony. 
Several participants noted this discrepancy in their feedback on the survey, allowing us to fix the typo before all surveys were completed.
There did not appear to be confusion due to the typo on the part of the participants.
"Convenience" was also misspelled in one question. 
For elimination testimonies, the name "Alex Smith" also occurred in the cross examination.
In the case of non-algorithm inconclusive testimonies, the question: "Can you describe the process of obtaining these test fired bullets?" was missing, but the response: "The test-fired bullets came from a test fire of the gun recovered from the traffic stop." remained unchanged. 
Due to the nature of the online survey, any differences caused by these typos would be confounded with demographics as well as date or time.

### Future Research

One of the most notable features of the histograms presented in this research is the overwhelming proportion of respondents that selected the two highest categories, whether it be in terms of reliability, credibility, or scientificity. 
The concentration of responses in the two highest categories may obscure potential differences in treatments, simply due to the issue of overall trust in the system. 
It may also effect how well ordered logistic regression models fit the data. 
This effect may be diminished through the use of jury instruction, and not referring to the firearms examiner or the algorithm witness as experts (as suggested by @OpinionEvidence).

Efforts are also being made to streamline the testimony into a single document, to prevent confounding typos.
Because the written court testimony may be difficult to follow and may give witnesses an air of impartiality, future studies will include images for relevant actors, and color coded speech bubbles to clarify which side the witness is speaking for.
We plan to develop a tool that can be used in testing courtroom scenarios, with versatile images for a variety of situations. These proposed changes can be seen in Appendix \@ref(study-2-changes).

An additional response to the study that was recorded is participants' note sheets. 
Many participants copied and pasted portions of the testimony into their note sheets for later reference.
We plan to evaluate these note sheets, and develop a method for presenting the written notes alongside the testimony in order to produce a 'heat map' of the text that participants found relevant.
