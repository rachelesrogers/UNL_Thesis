---
# UNL thesis fields
title: "JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE"
author: "Rachel Edie Sparks Rogers"
month: "April"
year: "2023"
location: "Lincoln, Nebraska"
major: "Statistics"
adviser: "Susan Vanderplas"
abstract: |
  Here is my abstract. *(350 word limit)*
acknowledgments: |
  Thank you to all my people!
dedication: |
  Dedicated to...
# End of UNL thesis fields
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    pandoc_args: --top-level-division=chapter
    keep_tex: yes
    latex_engine: xelatex
    template: template.tex
  huskydown::thesis_gitbook: 
    style: style.css
#  huskydown::thesis_word: default
#  huskydown::thesis_epub: default
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: bib/apa.csl
# Download your specific csl file and refer to it in the line above.
lot: true
lof: true
#header-includes:
#- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r setup, include = F}
options(width = 60)
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 4, 
  out.width = "\\linewidth", dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45)
)
```

```{r include_packages, include = FALSE}
# This chunk ensures that the huskydown package is
# installed and loaded. This huskydown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", 
                   repos = "http://cran.rstudio.com")
if(!require(huskydown))
  devtools::install_github(
    "benmarwick/huskydown"
  )
library(huskydown)

library(readr)
#library(ordinal)
library(tidyr)
library(plyr)
library(dplyr)
#library(gofcat)
library(ggplot2)
#library(forcats)
library(ggmosaic)
require(gridExtra)
library(GGally)
library(forcats)
library(ggpcp)
library(ggblend)
library(knitr)
library(patchwork)
library(grid)
library(jpeg)
```

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

<!-- # Introduction {.unnumbered} -->

# Literature Review

## Introduction

There are many forms of forensic evidence that have been generally accepted \authorcol{for use in court cases as} unique and identifying (without proof).
There are even some methods - such as bite mark analysis - that have been used in the courtroom as evidence, but would later be shown to not be scientifically valid [@pcast, 86].
When a \authorcol{defendant's} future hangs in the balance of such evidence, we must know that the type of evidence being presented is something that can be relied upon, and that the jury can weigh it appropriately when reaching a conclusion.
One way to assure that these goals are met is through the use of statistical methods in analysis, which allows for the quantification of comparisons and the establishment of accurate error rate calculations.
However, \authorcol{several studied have indicated that} these statistical methods \authorcol{are} more difficult for jurors to understand.
By testing jury perception in bullet matching, and developing tools for assessing jury perception, we hope to expand what can be learned about jury perception in order to present statistical methods in a way that can be understood and used to make decisions by the wider public.
Bullet matching has a history similar to many areas of forensic pattern evidence, and we have to statistically validate its use before it should be used in the courtroom or treated as reliable evidence.
However, we must also establish that jurors can treat the evidence with appropriate weight.

## Bullet Matching Background

In firearms evidence, there are two pieces of fired evidence that may be evaluated: the cartridge casing and the bullet itself. 
The structure of the bullet is shown in Figure \@ref(fig:structure), provided by @glrx.
Striation marks are created on the bullet as it travels down the barrel due to rifling, defects, and impurities [@hare2017automatic], while the head of the cartridge casing is marked by the breech face and aperture sheer [@chapnick2021].
```{r structure, echo=FALSE, fig.cap="Image of bullet structure. 1 indicates the bullet, 2 indicates the cartridge casing, 3 indicates the powder, 4 indicates the head of the cartridge casing, and 5 indicates the primer.", out.width="50%", fig.align="center"}
include_graphics(path = "images/Cartridge_cross_section.png")
```

The practice of bullet matching is based on the concept that rifling in a gun barrel can produce uniquely identifying marks on bullets fired from the gun, due to random variation [@pcast, 104]. 
\authorcol{Rifling is defined as the spiral ridges in a gun barrel that stabalize the bullet, as shown in Figure} \@ref(fig:rifling) \authorcol{, from} @105mmTank2005.
\authorcol{The raised areas are known as lands, while the depressed areas are known as grooves.}

```{r rifling, echo=FALSE, fig.cap="Cross section of a gun. The rifling is the spiral pattern of lands (raised portions) and grooves (indented portions).", out.width="50%", fig.align="center"}
include_graphics(path = "images/rifling.jpg")
```

\authorcol{When a bullet is fired, it travels down the gun barrel and marks from the rifling are scratched into the bullet's surface.}
\authorcol{This results in indented areas that correspond to the land impressions, as shown in Figure} \@ref(fig:fired) \authorcol{, provided by} @gremi.

```{r fired, echo=FALSE, fig.cap="Fired bullet. Indented portion correspond to the gun lands.", out.width="50%", fig.align="center"}
include_graphics(path = "images/fired_bullet.jpg")
```

\authorcol{The diagram in Figure} \@ref(fig:fireddiagram) from @hare2017automatic \authorcol{demonstrates the structure of these land and groove impressions on a bullet.}
\authorcol{As there are multiple lands on a gun barrel, this results in multiple land impressions on a fired bullet.}

```{r fireddiagram, echo=FALSE, fig.cap="Fired bullet. Indented portion correspond to the gun lands.", out.width="50%", fig.align="center"}
include_graphics(path = "images/bulletdiagram.jpg")
```

\authorcol{Striation marks, or scratches in the bullet surface caused by the rifling, can be seen on these land impressions, as shown in Figure} \@ref(fig:firedland) \authorcol{(provided by} @hare2017automatic). 
\authorcol{Firearms examiners can compare these striation marks across bullets in order to determine if they were fired from the same gun.}

```{r firedland, echo=FALSE, fig.cap="Fired bullet. Indented portion correspond to the gun lands.", out.width="50%", fig.align="center"}
include_graphics(path = "images/bulletland.jpg")
```

\authorcol{Firearms examiners use comparison microscopes, such as} Figure \@ref(fig:microscope), \authorcol{to compare these striation marks between two bullets, shown in the top right of the figure.}
```{r microscope, echo=FALSE, fig.cap="Image of a comparison microscope and aligned bullets", out.width="50%", fig.align="center"}
include_graphics(path = "images/microscope.jpg")
```


An early attempt to distinguish guns based on the rifling can be found in two issues of "The Saturday Evening Post" from 1925, in an article entitled “Fingerprinting Bullets".
<!-- whose title demonstrates an acceptance of fingerprinting by the public that wouldn't be statistically valid until... -->
Stout describes the wrongful conviction and subsequent exoneration of Stielow, who was accused of murder [@SaturdayEveningPost]. 
At the trial where Stielow was found guilty, an expert for the prosecution stated that there were nine abnormalities on Stielow's gun that corresponded with marks on the fired bullet [@SaturdayEveningPost, 7].
It was later found that the bullet found at the crime scene indicated the gun was missing a land – leaving a distinctive pattern – while the alleged gun showed no such evidence; the alleged gun also showed evidence of not being fired in more than 3 years [@SaturdayEveningPost]. 
While this trial did not require 'uniquely identifying' marks to rule out the alleged weapon (only a mismatch in lands), it motivated Charles Waite to attempt to catalog the manufacturing methods of guns by various companies around the country, with the goal of connecting a gun to its manufacturer. 
Waite soon encountered issues due to the multitude of guns produced abroad, as well as “cheap knock-offs” with no clear record, making it difficult to trace a gun or bullet to its manufacturer [@SaturdayEveningPosta].
While Waite's early attempt to match barrel markings to specific gun brands was focused on the creation of a 'database', more recent firearms identification methods have relied on the direct comparison of bullets, such as comparing fired evidence to a suspected gun.

## Issues in Pattern Analysis

### Subjectivity of Comparisons
<!-- Structure of this section may be difficult to follow, and importance of information not always obvious. Review for continuity and structure -->

One of the issues listed in the article still resonates today: disagreement among experts; this can be especially problematic in countries that use the adversarial judicial system, where experts can testify on the side of the prosecution or the defense [@SaturdayEveningPost, 6]. 
\authorcol{This could lead to multiple experts with differing conclusions on the same comparison, and are not guaranteed to be unbiased.}
Because there is no quantitative score describing the strength of a bullet match, it is possible for experts to use different criterion when making a conclusion about the same evidence. 
Disagreement among experts concerning strength of evidence can be found within a single lab, where experts undergo the same training, use the same equipment and operate under the same procedures, as demonstrated by @montani2019. <!-- As well as in studies of bullet comparisons. Find sources -->
@montani2019 discuss the importance of independent verification, and how to present results when experts do not agree.
They suggest that, when experts disagree, a third expert should be consulted, and all conclusions should be documented and presented as evidence.
In the case of bullet comparison studies, researchers ask examiners to make conclusions regarding the comparison of several bullets or cartridge cases, and experts do not always agree.
@nationalresearchcouncilusStrengtheningForensicScience2009 states that subjectivity is involved in determining “sufficient agreement” among firearms [@nationalresearchcouncilusStrengtheningForensicScience2009, 155].
This issues is also reflected in @imwinkelried, who suggest that bullet identification is based more on personal experience than on the scientific method. 
In plain terms, there is not a set standard for what constitutes sufficient agreement between compared evidence.

Procedural issues may also lead to differing expert opinions across laboratories; one system may not allow an exclusion to be documented if the class characteristics match, while another system may not have such a limitation [@baldwin2014, 6]. 
In their study, this led to 45 of 218 examiners labeling all different source comparisons as inconclusive, and 96 examiners labeling none of the comparisons as inconclusive [@baldwin2014, 16].
Without a standardized procedure, experts \authorcol{may} evaluate the evidence similarly when comparing two bullets, but must draw different conclusions based on the procedures of their laboratory. 
This results in inconsistencies in results based on location, which are compounded with inconsistencies that may arise due to the subjectivity of bullet comparisons. 
<!-- Reference the lack of national standardization specifically -->

Aside from the difference in evaluation, subjectivity can lead to bias in pattern recognition. 
The need for an unbiased method of evaluation is aptly demonstrated by the case of the Madrid bombing, in which the FBI incorrectly matched a latent fingerprint to the incorrect individual, despite the use of a verification process [@stacey]. 
The individual incorrectly identified was a Muslim from Oregon who had been on an FBI watch list [@kassinForensicConfirmationBias2013, 42]. 
@kassinForensicConfirmationBias2013 suggest that this incorrect conclusion may relate to confirmation bias, and outline other contributing factors, such as extraneous details, external pressures, and order of presentation. 
While this example is based on fingerprint analysis, the same issues of subjectivity are present in bullet matching.

Research has been conducted to better understand the factors that drive firearms examiners to make their conclusions.
The use of virtual comparisons allowed @chapnick2021 to investigate what examiners considered important when evaluating cartridge cases by including a feature to highlight important aspects of comparisons. 
They found that examiners generally tended to highlight the same features when making match conclusions [@chapnick2021, 8].
However, not all examiners who correctly identified the samples used 'irregularly shaped marks' to distinguish the known matches: the number was around 60%-70% [@chapnick2021, 6].
This is far from a unanimous consensus on important features for determining a match in this type of firearm evidence.
<!-- What about bullet matching? -->

While there are issues in subjective bullet matching, there have been attempts to quantify the method. 
@biasotti1959 suggests that statistical methods can be used in order to determine whether two bullets were fired from the same gun. 
He concludes that evaluation of individual striation marks \authorcol{(or striae)} is not sufficient, but counting the number of consecutively matching lines may provide enough information to distinguish between matching and non-matching bullets, in the case of the .38 Special Smith and Wesson revolvers used in this study [@biasotti1959, 37 – 47].
@biasotti1959 hoped that such methods may lead to a statistical model for evaluating firearms (47). 
The idea of consecutively matching striae (CMS) was addressed again in @bunch2000, but a completely standardized or quantifiable method of examination has yet to be developed. 
According to @AuimatagiHearing, consecutively matching striae is still used today by some examiners (pg 48). However, the use of consecutively matching striae has not been nationally adopted. 
@biasotti1959's idea of using consecutively matching lines is used and extended in the bullet matching algorithm developed by @hare2017automatic. 

### Scientific Validity

In recent years, the scientific validity of many forensic science methods have been called into question, as shown in the @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast reports on feature comparison methods.
General concerns in both reports include foundational evidence for scientific validity, general inability to determine error rates for conclusions, and subjectivity in analysis methods.
Some issues outlined by @pcast are the circular nature of the identification guidelines \authorcol{put forth by the Association of Firearm and Tool Mark Examiners (AFTE)} and the lack of appropriately designed error rate studies [104]. 
AFTE defines sufficient agreement as "...the examiner being convinced that the items are extremely unlikely to have a different origin" [@pcast, 104].
Thus, items are classified as having sufficient agreement if they agree sufficiently enough to conclude that they did not come from different sources. 
This guideline is in itself subjective - there are no benchmarks for determining what one means by "extremely unlikely to have a different origin".
Both @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast suggest a move away from subjective methods.
@pcast states the importance of the development of an objective method for firearms comparisons (113). 

These reports also highlighted the lack of studies that produced accurate error rates due to several issues, such as the reporting of inconclusive decisions as well as simple design issues [@pcast, 104 - 112]. 
For example, @chapnick2021's report does not include inconclusive results as a potential source of error. 
They reported three errors (false positives) in their study, and reported a false negative error rate of 0%, ignoring the wide range of inconclusive decisions.
For known matches of participants from the US and Canada, 38 of 491 comparisons were reported as inconclusive, while 254 of 693 comparisons were reported as inconclusive for know non-matches [@chapnick2021, 6].
The issues of error rate calculation are addressed by @hofmannTreatmentInconclusivesAFTE2021, particularly with regards to whether or not inconclusive decisions are treated as errors in calculations. <!-- (325) --> 
They suggest to not consider inconclusive decisions errors for the calculation of examiner error rates, used in the lab setting; but to consider inconclusive decisions as errors for process error rates, used for determining if the evidence is relevant enough for judging the guilt of the suspect. <!-- (343) -->
This distinction would allow an examiner to report an inconclusive finding without a negative reflection on their work, as inconclusive decisions are sometimes necessary.
But it would also provide jurors with relevant information in terms of evaluating the reliability of firearms evidence.

In another evaluation of inconclusive results, @drorMisUseScientific2020 describe different ways in which inconclusive results are produced.
They differentiate between inconclusive decisions based on the amount of evidence present; an examiner reaching an inconclusive decision when there is not sufficient evidence to reach a conclusion would be correct, whereas an examiner reaching an inconclusive decision when there is sufficient evidence to reach a conclusion would be incorrect. <!-- (334) --> 
This definition relies on a quantifiable amount of evidence necessary to reach a conclusive decision.
By failing to distinguish between correct and incorrect inconclusive decisions in research studies, examiners may opt for the inconclusive choice on hard conclusive comparisons, resulting in error rates that are only valid for easy comparisons when inconclusive decisions are not considered an error[@drorMisUseScientific2020, 336].
@drorMisUseScientific2020 would consider those inconclusive choices to be "incorrect" - there is enough evidence to reach a conclusion.
Similarly, another potential error would be the examiner reaching a conclusive decision when there is not sufficient evidence to reach a conclusion [@drorMisUseScientific2020, 334].
This delineates conclusion and errors into distinct categories, which are not seen in error rate studies.
Both inconclusive and conclusive results for the same analysis cannot be correct – there either is enough evidence to make a conclusion, or there is not enough evidence to make a conclusion; to say otherwise may deflate actual error rates [@drorMisUseScientific2020, 334-335].
@drorMisUseScientific2020 indicate that this inconclusive issue is more prominent in studies than in casework, as inconclusive decisions are more common in error rate studies. <!-- (336) -->
However, this distinction between correct and incorrect decisions are not commonly made in studies due to a lack of a quantifiable method for separating the two categories, leading to inconclusive decisions contributing lowering the error rate in some studies.
<!-- Issue is not knowing numbers to test inconclusives differently - there is a source for this -->

Other issues in the computation of error rates include the use of closed set studies, which result in significantly lower rates of inconclusive decisions and false positives [@pcast, 109]. In closed set studies, the source gun is always present, and examiners are asked to match a set of known bullets to a set of unknown bullets - allowing for them to use a process of elimination to make matches; conversely, open set studies do not include all source guns [@pcast, 108-110].
The Ames Laboratory study is described by @pcast as an appropriate closed set black-box study (111). 
These types of analyses are classified as black-box studies because reported match results are based on an examiner's subjective opinion rather than a list of objective steps [@pcast, 5].
When comparing closed-set studies to non closed-set studies, @pcast found that closed-set studies had a much lower rate of inconclusive decisions as well as false positives (111).

### Scale of Conclusions

This debate regarding scientific validity continues to this day, as @vanderplas2022 shows. 
In this declaration, they discuss the current issues with error rate studies, in which they conclude that, until valid error rate studies have been conducted, they cannot support the use of firearms analysis in the courtroom [@vanderplas2022, 10].
Others have called to scale back conclusions that attest to “individualization” in the courtroom due to the inability to tie a specific piece of evidence to a specific source, to the exclusion of all other sources [@nationalresearchcouncilusStrengtheningForensicScience2009;@imwinkelried]. 
There has been some resistance to scaling back conclusions, however, as demonstrated in a memo sent out by Jim Agar II, an FBI attorney. 
They stated that less conclusive language would not be truthful on the part of the firearm expert and to ask firearms examiners to use this language would result in perjury [@agarmemo], despite the reliance on subjective methods with unclear error rates.

There has also been some push back against the conclusions of @pcast, as shown in @osacresponse (provided by the Firearms and Toolmarks subcommittee.
According to @davisosac2014, the Organization of Scientific Area Committees (OSAC) subcommittees are "generally composed of 70% practitioners, 20% researchers, and 10% research and development technology partners and providers".
@osacresponse argue that a false positive error rate is not necessary for determining foundational validity, and that closed set error rate studies are fine for calculating error rates.
@osacresponse also suggests that @pcast ignores the importance of peer review in studies that instruct firearms examiners to work alone, and that black box studies may not properly reflect casework.
However, in order to accurately represent casework, blind testing (where the examiner does not know they are being tested) would be needed - but due to logistical issues and case load, this may not be feasible [@pcast, 59].
@osacresponse argues that AFTE language is not circular - the basis of a practical impossibility is based on knowledge of best known non-matches conveyed through training. 
Here decisions are based on "knowledge and experience", which does not counteract the subjective nature of this decision making.
Despite these disagreements @osacresponse also promotes the use of more objective methods.

These differing views are represented in @swoffordProbabilisticReportingAlgorithms2022.
@swoffordProbabilisticReportingAlgorithms2022 interviewed 15 individuals with a range of expertise in forensic science and the court system in order to assess their feelings about the use of algorithms and the presentation of probabilistic reporting, as opposed to categorical reporting and traditional analysis methods. 
They found that prosecutors interviewed supported the use of match terms such as “identification” and “individualization” instead of probabilistic terms, but two of the prosecutors also rejected the use of terms that convey “absolute certainty” (7). 
All participants felt that that examiner testimony should accurately reflect scientific limitations, although there was not a consensus on whether this practice was in fact followed in the courtroom - defense attorneys felt that the examiners usually do not uphold this expectation, while lab managers expressed frustration on the part of the court (19).
The scale to which firearm evidence is used in the courtroom is rather contentious, and views may differ based on occupation.

In general, individuals want to guarantee that the information presented in the courtroom is accurate and valid, without causing too much imbalance between the limitations and the categorical conclusion.
However, there isn't agreement on where the balancing point between these two factors are - somewhere between giving too much credit with a match of "absolute certainty", and introducing unfounded reasonable doubt by overstating the limitations.
If accurate numerical information on error rates or an objective method of evaluation that can be described to the court were available, it may be possible to have factual statements of limitations and scope of evidence that both sides of the argument would find satisfactory. 
In an investigation on the effect of the inclusion of error rate on either voice or fingerprint evidence, @garrettErrorRatesLikelihood2020 found that the inclusion of an error rate reduced convictions for fingerprint evidence presented categorically, but had no effect in the case of voice comparison (a less established science) or when the fingerprint comparison was presented as a likelihood ratio.
They state, "This suggests that error rate information is particularly important for types of forensic evidence that people may assume are highly reliable." (1206).
It seems that including error rates for evidence methods seen as reliable may call into question some assumptions participants make about the science, resulting in a closer consideration of the facts presented at trial.

Quantifying both the limitations in terms of error rates and the analysis method with set cutoffs would leave the language used by experts when presenting evidence as a variable that can be manipulated.
While there is much debate over the language used in these conclusions, @garrett2013 found that the specific language used to describe the strength of the match (when a match was present) had little effect on participants’ judgement of guilt. They tested 11 different forms of match language, such as 'individualization', 'match', or 'very likely that the defendant was the source', as well as language that included match conclusions 'bolstered' by certainty or likelihood language [@garrett2013, 489].
@mcquistonsurrett2009 also found that there was not a significant difference in participants' view of the likelihood the defendant committed the crime when the examiner presented the conclusion as a 'match' compared to presenting the conclusion as 'similar in all microscopic characteristics.'
@garrett2013 suggests that this lack of difference may relate to the overall view of firearms evidence as reliable, meaning that even a weak match basically boils down to being a match in the eyes of the participants (507).
They also found some tempering effect of the expert admitting the possibility that their conclusion was made in error (507).
Thus, the use of accurate error rates may moderate the reliability of experts for a more considered weighing of evidence.
In fact, in the Ninth Judicial Circuit Court, there are jury instructions regarding the need for juries to treat firearms and other 'expert' testimony in the same manner as other opinion testimony, in which they must decide how much weight the testimony should receive [@OpinionEvidence]. @OpinionEvidence also suggest to avoid labelling such testimony as expert testimony, since it may cause jurors to give the testimony undue weight.
One way to fix these issues of subjectivity is by developing a more objective method of evaluation.
One objective method introduced to the forensic sciences is bullet matching algorithms, such as the one described in @hare2017automatic.

<!-- Describe how expert testimony works and the laws surrounding it -->

## Quantitative Methods

### Bullet Matching Algorithm

This bullet matching algorithm was developed by @hare2017automatic and uses a random forest in order to determine a match score [@hare2017automatic, 2352]. 
This algorithm can be described as follows:

The algorithm first takes a 3D scan of the two bullet lands, then identifies a cross section, or a stable area of the bullet land that can be used in comparison. 
This cross section is used in order to identify the signatures of the bullets, or a 2D line that captures the marks scratched onto the bullet's surface. 
First the shoulders (the raised area on either side of the scratched pattern) must be removed from the cross section. 
Then a loess smoothing is applied twice in order to remove the curvature of the bullet. 
What is left is the bullet land’s signature (which will show the peaks and valleys produced by the barrel of the gun). 
The two signatures are compared using a variety of attributes, such as consecutively matching striae and the relative height of the peaks/valleys, as shown in Figure \@ref(fig:signaturecompare).

```{r signaturecompare, echo=FALSE, fig.cap="Left image depicts two matching signatures, while the right image depicts two non-matching signatures. These were generated by the author.", fig.align="center", fig.show="hold", out.width="49%"}

include_graphics(path = c("images/Match_Signatures.jpg", "images/K995_NoMatch_Signatures.jpg"))
```

A random forest is used to compare the two signatures and come up with a match score for the lands. 
Lands are then aligned across the bullet for the maximal random forest score, and this number would be considered the match score for the bullet. 
Figure \@ref(fig:gridcompare) shows two grids of land match scores computed in two different bullet comparisons.
```{r gridcompare, echo=FALSE, fig.cap="Left image depicts two matching bullets (indicated by high land-to-land match scores in a diagonal formation), while the right image depicts two non-matching bullets.", fig.align="center", fig.show="hold", out.width="49%"}

include_graphics(path = c("images/F526_Match_SingleGrid.png", "images/K995_NoMatch_SingleGrid.png"))
```
In multiple tests, it was found that the algorithm is able to successfully distinguish between known matches and known non-matches without the use of an inconclusive decision [@vanderplasComparisonThreeSimilarity2020, 10].
If the bullet is not fit for algorithmic comparison, then an 'inconclusive' result may still be reached. 
This may be the case for gun types that have not been tested for the algorithm. 
The use of an algorithm allows for quantification of bullet matching, for cases where the algorithm has been verified.

There are some concerns about implementing algorithms for forensic evidence in the courtroom, due to the issue of explaining complex methods to jurors.
@garrettInterpretableAlgorithmicForensics2023 explain one main issue that may result from "black box" algorithmic methods: the results from the algorithm may not be accurate, and this may be difficult to discern when individuals do not understand the method used.
They instead suggest the use of "glass box" methods, where the algorithm's model is interpretable (people can see how it works, and what information is used to make decisions).
@garrettInterpretableAlgorithmicForensics2023 state that "...interpretability is particularly important in legal settings, where human users of a system... cannot fairly and accurately use what they cannot understand.".
This brings up a particularly salient point in jury trials - jurors should understand the pattern comparison process if they are making decisions based on its results.
In a combination of computerized results and human interpretation, @montani2019 suggest presenting the algorithm as a “second independent expert” whose comparison and conclusion can be compared to and presented alongside the examiner's.
Supplementing the decision of the forensic expert with an algorithm would allow for verification of results in two independent methods.
The three laboratory managers interviewed by @swoffordProbabilisticReportingAlgorithms2022 suggested using algorithms as a supplement to the forensic examiner (6), similar to @montani2019’s suggestion. 
The three prosecutors varied in their feelings of algorithms in the courtroom: one thought that they would add unnecessary complications, while two suggested that the algorithms may be useful for the forensic expert [@swoffordProbabilisticReportingAlgorithms2022, 8]. 
The three defense attorneys, as well as the three judges, also supported the use of algorithms as empirical evidence, while stating concerns of transparency [@swoffordProbabilisticReportingAlgorithms2022, 10-13]. 
Three academic scholars supported the use of algorithms, so long as the algorithms can be understood, have been validated, and do not include factors that may cause systematic biases [@swoffordProbabilisticReportingAlgorithms2022, 16]. 
@swoffordProbabilisticReportingAlgorithms2022's study demonstrates the variety of opinions on algorithm implementation throughout forensic science, although most support the use of algorithms. 

### Quantitative Results

Quantitative methods of examination can lead to quantitative results.
In this case, the quantitative results are in the form of a match score, but in other cases, results may be presented as a likelihood ratio. 
For example, @EvaluatingLikelihoodRatio proposed a likelihood approach for a cartridge case comparison method that uses congruently matching cells. 
This is a change from the match language usually used by forensic examiners, but these likelihood ratios may have some benefit.
@marquis2016 suggest that the use of a likelihood ratio requires examiners to consider what they are including when weighing the strength of evidence, and provides a value that can be consistent across disciplines (4). 
This quantitative approach, even while conducting a subjective evaluation, is meant to prevent examiners from being biased by information provided outside of their direct examination [@bunch2013, 223]. 
The approach also asks examiners to consider both the null and alternative hypotheses, and may make it less likely for either the examiners or jurors to transpose the conditional, which may happen in the case of reporting probabilities [@evett1998;@marquis2016, 3].
<!-- Distinguishing between LRs and Bayes Factors - Danica Ommen paper I do not have access to -->

As an example of transposing the conditional, suppose an examiner were comparing a bullet from the suspect's gun to a bullet recovered from the crime scene.
The examiner may find that there is significant agreement between the two bullets.
They may then conclude that "the probability of seeing such significant agreement between the two bullets *given that another gun fired the bullet at the crime scene* is small".
In this case, the examiner is making a statement regarding the amount of agreement between the two bullets (P(Correspondence|Different Source)).
If the conditional is transposed, the statement would become "the probability that another gun fired the bullet at the crime scene *given the significant agreement between the two bullets* is small" (P(Different Source|Correspondence)).
Here, the conditional part of the statement has been switched, so that the examiner is making a statement regarding the gun being fired at the crime scene instead of a statement with regards to the bullet comparison.
By stating a likelihood ratio, such as "the bullet comparison provides strong support for the proposition that the two bullets came from the same gun rather than the proposition that the two bullets came from different guns", both the null and alternative hypothesis are clearly stated, and the focus of the statement is on the bullet comparison, rather than the gun's presence at the crime scene.

As shown above, the likelihood ratio directly compares the null and alternative hypotheses in a case, which is not accomplished by considering a single probability [@nordgaard2012, 6]. 
Likelihood ratios also allow for the integration of evidence from multiple sources, and individual likelihood ratios may be multiplied together to calculate an overall likelihood ratio.
@meuwly2017a suggested guidelines for validating both score- and feature-based likelihood ratio methods, so this form of result presentation could be more widely accepted.
These methods included validation criteria for all variables considered in a likelihood ratio, where validation criteria can be considered in several ways: a comparison with current “state of the art” methods, detection error trade-off graphs to measure discriminating power, and performing validation on a validation data set.

### Explainability in the Courtroom

One roadblock in the use of quantitative methods and results, such as likelihood ratios, in the forensic sciences is the necessity to explain such methods to jurors so that they can understand the method and results well enough to make informed decisions in court cases.
@afs2009 stated that opinions and conclusions should be expressed in likelihood ratios when outlining guidelines for forensic experts (163). 
Participants in @swoffordProbabilisticReportingAlgorithms2022 expressed concern that probabilistic reporting (as opposed to categorical reporting) would be confusing and easily misinterpreted, when compared to the alternative of categorical reporting (18).
In order to gauge how quantitative methods are perceived in the courtroom, we can consider fingerprint and DNA analysis.
@garrettComparingCategoricalProbabilistic2018 studied the use of FRStat for fingerprint matching in the courtroom.
The FRStat language presented to the study participants is as follows: "The probability of observing this amount of correspondence is approximately [XXX] times greater when the impressions are made by the same source rather than by different sources" [language from @DFSCLPInformation2018].
@garrettComparingCategoricalProbabilistic2018 used ratios from 10 times greater to 100,000 times greater, and found that the likelihood that the subject committed the crime according to the participants did not change significantly.
This demonstrates that potential jurors may have trouble accurately interpreting statistical results, including likelihood ratios.

In DNA research, @koehler2001 found that participants were more likely to believe the subject was the source of the DNA when presented with a probability instead of a frequency.
When asked about how many individuals would match DNA for a given match proportion in a population of 500,000, 60.7\% of participants who received a frequency and 42.1\% of individuals who received a probability answered correctly [@koehler2001, 503].
When asking these individuals to determine the guilt or innocence of a suspect, the percentage of participants who correctly interpreted the DNA results seems relatively low.
Jurors are also prone to find evidence to be weaker when frequencies are presented as whole number (1 out of 100,000) compared to a decimal number (0.1 out of 10,000), even though the frequencies represent the same value [@koehlerThinkingLowProbabilityEvents2004, 544].
These studies show that individuals struggle with interpreting numerical results, and assigning appropriate weight to likelihood ratios.

In addressing these issues of interpretation, attempts have been made to assign a verbal scale to be used alongside a likelihood ratio, such as the European recommendations for reporting forensic science \authorcol{put forth by the European Network of Forensic Science Institutes} [@ENFSIGuidelineEvaluative2016].
A sample of phrasing from their recommended scale has been recreated in Table \@ref(tab:enfsi).
Verbal values range from no support to extremely strong support, and correspond to a numerical output. 
This type of scale would present jurors with both the quantitative result as well as a brief interpretation, so they are not solely relying on their own perception of what the likelihood ratio qualitatively means.
It also offers a numerical scale that may encourage consistency across examiner reports.

--------------------------------------------------------------------------------------------------------------------------------
  Likelihood Ratio                                 Verbal Equivalent
------------------------- ------------------------------------------------------------------------------------------------------
  1                         The forensic findings do not support one proposition over the other                    
  
  2 - 10                    The forensic findings provide weak support for the first proposition relative to the alternative                      
  
  10 - 100                  ...provide moderate support for the first proposition rather than the alternative                    
  
  100 - 1,000               ...provide moderately strong support for the first proposition rather than the alternative                    
  
  1,000 - 10,000            ...provide strong support for the first proposition rather than the alternative
  
  10,000 - 1,000,000        ...provide very strong support for the first proposition rather than the alternative
  
  1,000,000 and above       ...provide extremely strong support for the first proposition rather than the alternative
------------------------- ------------------------------------------------------------------------------------------------------
Table: (\#tab:enfsi) Sample language from @ENFSIGuidelineEvaluative2016 (17)

A similar scale is also suggested by @evett1998, with verbal values of “Limited support”, “Moderate support”, “Strong support”, and “Very strong support” (201). 
These values approximately correspond to the scale above, but consists of fewer categories.
@marquis2016 suggest against providing the full verbal scale to participants, because participants may use other terms on the scale in order to orient the likelihood ratio in comparison to the full scale (7). 
However, this ability to orient relative to other values may assist jurors in accurately judging the strength of evidence presented.

While verbal scales are useful for clarification, individuals may not always interpret them consistently.
For example, @budescu1985a asked participants to rank common probabilistic words (such are "rarely" or "usually") on three separate occasions, finding that individuals typically gave consistent rankings across time points, but there was variation in rankings between individuals.
In an earlier study, @lichtensteinEmpiricalScalingCommon1967 asked individuals to assign numerical probabilities to probabilistic words, finding that eight of the eleven mirror terms (such as “quite likely” and “quite unlikely”) demonstrated asymmetry - positive terms (such as likely) scored lower than their mirrored negative terms (unlikely). For example, "quite likely" and "quite unlikely" had median values of 0.8 and 0.1, respectively [@lichtensteinEmpiricalScalingCommon1967, 564].

@martire2013 studied the use of a verbal scale similar to @ENFSIGuidelineEvaluative2016 in a courtroom setting, in the case of shoe print evidence.
They found a "weak evidence effect", in which findings that provided "weak support" for the proposition that the defendant's shoe left the print resulted in decreased likelihood scores of guilt, compared to likelihood scores collected before the introduction of forensic evidence.
@martire2013 did not find this trend when the evidence was presented numerically as a likelihood ratio; in this case, participants tended to think of the defendant as more guilty after hearing the forensic evidence (as expected).
They did not, however find that the reverse was true: those who received "weak support" for the proposition that the defendant did not leave the shoe print resulted in increased likelihood scores in favor of innocence, as expected.
@martire2013 hypothesize that this demonstrates that the participants are engaging in a "criminal justice perspective" by weighting the evidence toward the defendant's innocence (since the burden of proof rests on the prosecution).
This inconsistency of interpretation between the numerical scale and the verbal equivalent may act as evidence against implementation of a solely verbal scale.

Examiners also show evidence of inconsistency in scale interpretation.
@mattijssen2020 asked examiners to use a verbal scale for degree of similarity and support when digitally comparing cartridge cases.
When using a verbal scale for degree of similarity/support, @mattijssen2020 found that the between-subject and within-subject reliability was moderate to high (11). 
@mattijssen2020 asked 10 examiners who regularly used likelihood ratios to provide both a verbal degree of support along with a likelihood ratio.
They found that the verbal degrees of support were in general an overestimation when compared to the likelihood ratios (8). 
While this is a small non-representative sample, it suggests that verbal scales do not always correspond to quantitative scales across subjects.
@thompsonLayUnderstanding investigated the amount of weight participants gave either DNA or shoeprint evidence when it was presented as a likelihood ratio, verbal equivalent, or random match probability.
While individuals updated their response as expected when the strength of evidence was changed in all cases of the DNA condition, participants did not produce significantly different estimates when shoe print evidence was presented as a likelihood ratio or verbal equivalent, but they did produce different estimates in the case of the random match probability.
These results indicate that participants may weigh evidence differently depending on the presentation method.
@thompsonLayUnderstanding hypothesize that the difference between the DNA results and the shoe print results may stem from the perception of DNA analysis as highly scientific.
By combining the quantitative analysis with a verbal translation, it may be possible to present quantitative results in a manner that is understood consistently by laypeople.

This mix of quantitative language alongside more categorical terms is supported by subjects in @swoffordProbabilisticReportingAlgorithms2022. 
Almost all participants expressed concerns about the interpretation of the probabilistic language, and many recommended a combination of both methods [@swoffordProbabilisticReportingAlgorithms2022]. 
The prosecutors interviewed thought that match language was sufficient by itself, and did not wish to complicate testimony with probabilistic language [@swoffordProbabilisticReportingAlgorithms2022, 8].
@mcquistonsurrett2009 studied the use of match language versus probabilities with both judges and juries.
Both groups assigned higher probabilities to the defendant committing the crime when presented with qualitative evaluation or a single probability for the hair comparison, as compared to frequency methods of reporting.
@mcquistonsurrett2009 found that participants were more sure of the guilt of the defendant when qualitative language was used as opposed to a subjective probability.
While match language may give jurors more confidence, it could have the effect of shifting the consideration of evidence from the jury (who in a quantitative approach would need to determine whether or not they feel a likelihood ratio is large enough to indicate that the subject is the source) to the forensic expert, who can declare a match.

### Demonstrative Evidence

Aside from result language or scales used, another important factor in jury decision making is demonstrative evidence. 
Images can supply helpful context in describing complex forms of analysis, but they may also introduce bias in the form of "truthiness", a\authorcol{n official} term introduced by comedian Stephen Colbert.
He described truthiness as the quality of something *seeming* true [@WordTruthinessColbert2005]. 
In this video, Colbert talks about “thinking with your head vs. knowing with your heart”, where some information may feel true, regardless of the facts. 
@bornsteinJuryDecisionMaking2011 found a "truthiness" effect in the courtroom - jurors tend to remember evidence that align with their previous beliefs (65). 
\authorcol{This concept can apply to images that make people more likely to feel like a statement is true, even when they do not contribute new evidence.}
@kellermannTrialAdvocacyTruthiness2013 suggests that the truthiness or "falsiness" of non-probative visual images should be carefully considered before using images in the courtroom "...both to prevent backfire effects and to capitalize on every possible tactic that can be used to persuade jurors..." (40).
While the concept of truthiness can be used to benefit either side in an adversarial justice system, trial outcomes should be based on factual evidence, rather than feelings that evidence is factual.
In the case of images in trials, it is important to balance the benefit of providing additional information to the jury via images without increasing truthiness. 

The impact of photos on an individual's perception can be rather large, as investigated by @cardwellNonprobativePhotosRapidly2016. 
They asked individuals to “give” or “take” food from an animal, represented by a word. 
Subjects were later asked to identify whether or not they gave food to an animal – either accompanied by an image or not. 
Individuals were more likely to say that they gave food to an animal if it was accompanied by an image [@cardwellNonprobativePhotosRapidly2016, 887]. 
They found that images had an effect for positive associations such as giving food, but not negative associations such as taking food [@cardwellNonprobativePhotosRapidly2016, 883].
In this case, the use of images may make it easier for individuals to visualize a scenario (giving food), and thus makes them more likely to remember the event - whether it happened or not.

In a study of perception, @mccabeSeeingBelievingEffect2008 presented a variety of graphics alongside articles relating to cognitive neuroscience.
The graphics all contained the same information, which was already presented the accompanying article.
They found that participants gave higher ratings of scientific reasoning to articles that included a brain image with activated areas, as opposed to a bar chart, topographical brain graphic, or no graphic.
Participants were also more likely to agree with the conclusion of an article when the brain image was present, compared to when it was absent.
Although the information presented in the articles did not change throughout these conditions, the brain image appears to add more "truthiness" to the articles, causing individuals to find them more scientific than articles with other graphics or no graphics.

In the courtroom, studies have been conducted to evaluate the effect of images of the brain.
@gurleyEffectsNeuroimagingBrain2008 studied the use of brain images when arguing the defendant is not guilty by reason of insanity. 
In a study on introduction to psychology university students, they found that the inclusion of images showing a brain lesion increased the odds of the participants finding the defendant not guilty by reason of insanity.
MRI scans were presented with additional information regarding impulse control for the damaged area, so the effect may not be caused by the images themselves but rather by the additional information.
@schweitzerNeuroimagesEvidenceMens2011 investigated whether images of the brain presented alongside expert testimony with regards to a mental disorder effected the participant's verdict of the defendant's mental state.
In this case, no additional information was provided alongside the neuroimage, allowing the effect of the image itself to be studied. 
They found that the inclusion of neuroimages did not significantly effect the judgement of the participants.
The influence of images on individuals remains unclear, and may be situationally dependent.

## Response Methods

There are many different ways to measure participant responses.
Verbal scales, such as those used in the ENFSI guidelines described in the last section, are often used to record participant responses.
Likert scales can be used to evaluate several factors, such as the reliability, understanding, and scientific quality of the forensics expert, the algorithm, and the testimony in general.
It is therefore important to ensure that participants' views are accurately recorded with Likert type scales, which can vary in the number of categories used.
Several researchers found that the 7-point scale may perform or represent the participants’ true views better than the 5-point scale [@joshiLikertScaleExplored2015;@finstadResponseInterpolationScale2010]. 
Participants often preferred more categories, such as 7, 9, or 10 [@prestonOptimalNumberResponse2000;@komoritaNumberScalePoints1965]. 
However, @prestonOptimalNumberResponse2000 found that reliability decreased with more than 10 categories. 
This indicates that 7, 9, or 10 point scales should be adequate for reliable responses that accurately represent the participant's views.

Other methods have also been used to evaluate participant responses, such as asking participants to give a likelihood ratio or a probability of guilt.
@thompsonJurorsGiveAppropriate2013 asked participants rate the chance of guilt on a scale from: with values generally on a log 10 scale (ex. 1 in 100; 1 in 1,000); a middle value of a fifty-fifty chance; and extreme values of "Certain to be guilty" and "Impossible that he is guilty".
They compared results from participants to Bayesian conditional probabilities based on provided likelihoods of a match and error rates in order to consider how much weight participants were giving DNA evidence.
Their results mainly showed no significant difference between Bayesian and participant estimates (meaning that the estimates were "in the right ballpark"), with some conditions producing estimates either greater than or less than what was expected.

In another study of the relationship between how examiners present evidence (likelihood ratio, verbal equivalent, or random match probability), @thompsonLayUnderstanding applies the same multiple choice scale for selecting the chance that the defendant committed the crime for half of the participants, while the other half of the participants were asked to supply how many more times likely it was that the defendant was guilty as compared to not guilty, or vice versa (based on a response scale found in @martire2013).
The likelihood statement was worded as follows: "Based on the available evidence I believe it is ___ times more likely that Mr. Kelly is guilty than not guilty." (@thompsonLayUnderstanding, 5).
In comparing responses from these two scales, @thompsonLayUnderstanding found that individuals were more likely to give higher estimates on the categorical log scale than when they were asked to provide a likelihood - resulting in the categorical log scale being more consistent with the expected result when using Bayesian methods of calculation.
This indicates an inconsistency in results based on the response method used.


## Conclusion

In order to ensure that the US justice system is just, we must be confident that the evidence presented in the courtroom is scientifically valid, and that the evidence is presented in a way that gives jurors the ability to appropriately judge its strength. 
This includes the use of accurate error rates as well as quantitative responses.
One way to facilitate error rate calculation and quantitative results is through the use of algorithmic or statistical methods.
These methods must, however, be explained in a manner that is understandable to those without a statistical background, while limiting any of the potentially biasing effects of "truthiness".

<!-- Clarify purpose of chapter to the reader, and clean up organization/grammar -->

<!--chapter:end:index.Rmd-->

# Jury Perception of Bullet Matching Algorithms and Demostrative Evidence {#study1}

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache = F}
library(readr)
#library(ordinal)
library(tidyr)
library(plyr)
library(dplyr)
#library(gofcat)
library(ggplot2)
#library(forcats)
library(ggmosaic)
require(gridExtra)
library(GGally)
library(forcats)
library(ggpcp)
library(ggblend)
library(knitr)
library(patchwork)
#library(kableExtra)
library(scales)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# demographics_fingerprints <-
#   read_csv("data/demographic_results.csv")
# demographics2_fingerprints<-
#   read_csv("data/demographic_results2.csv")
# dim(demographics_fingerprints)
# demographics_fingerprints$clean_prints <- as.numeric(as.factor(demographics_fingerprints$fingerprint))
# 
# fingerprint_matching <- demographics_fingerprints %>% select(clean_prints, fingerprint) %>% distinct()
# dim(fingerprint_matching)
# 
# merged_results_fingerprints <- read_csv("data/merged_results_wo_comments_w_allresults.csv")
# dim(merged_results_fingerprints)
# 
# merged_results_fingerprint_join <- left_join(merged_results_fingerprints, fingerprint_matching)
# 
# ## Identifying the print that had two sets of notes ##
# merged_results_fingerprint_join[merged_results_fingerprint_join$fingerprint=="642d497661c84dac328438a2bf0709e4",]$clean_prints
# 
# dim(merged_results_fingerprint_join)
# fingerprint_clean <- subset(demographics_fingerprints, select=-c(fingerprint))
# merged_results_clean <- subset(merged_results_fingerprint_join, select=-c(fingerprint))
# 
# write.csv(fingerprint_clean, "data/demographic_clean.csv")
# write.csv(merged_results_clean, "data/merged_results_clean.csv")

```

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache = F}

# merged_results <-
#   read_csv("../../merged_results_full_wo_comments.csv")
#View(merged_results)

merged_results_allresponses<-read_csv("data/merged_results_clean.csv")
# dim(merged_results_allresponses)

# Removing person who took notes for on the first page that did not correspond to their scenario number
merged_results_allresponses <-merged_results_allresponses %>% subset(clean_prints!=231)

merged_results <- merged_results_allresponses %>% filter(check1 =="9mm" & check2 == "Moderately reliable")

merged_results$conclusion <-
  factor(merged_results$conclusion,
         levels = c("NoMatch", "Inconclusive", "Match"))

demographics <-
  read_csv("data/demographic_clean.csv")

```

## Background

### Firearms Examiners

The foundational belief in bullet comparisons as a form of identifying evidence is that guns can leave unique striation marks on a bullet as an artifact of the rifling process [@pcast, p. 104]. 
Striation marks are left on portions of the bullets known as "lands", due to contact with the rifling as the bullet is fired. 
These marks are compared by examiners across bullets in order to identify if bullets were fired from the same source. 
These examinations are subjective, as they are based on the firearms examiner's experience and judgement [@nationalresearchcouncilusStrengtheningForensicScience2009, p. 153]. 
This can lead to issues of bias in analysis [@kassinForensicConfirmationBias2013]. In 2019, the PCAST report highlighted common issues with traditional bullet matching methods, such as the lack of appropriately designed error rate studies and the circular nature of AFTE's bullet matching guidelines [@pcast, p. 104-112]. 
Issues in error rate studies have been discussed in several articles [@hofmannTreatmentInconclusivesAFTE2021;@drorMisUseScientific2020]. 
PCAST emphasizes the importance of the development of an objective method for firearms comparisons [@pcast, p. 113]. 
The use of an automatic, objective method would contribute to quantifiable presentation of evidence and would lower the effort required to identify the method's error rate. 

### Bullet Matching Algorithm

Following PCAST's publication, many methods of statistical matching have been developed and evaluated, such as @hare2017automatic, @vanderplasComparisonThreeSimilarity2020, @songDevelopmentBallisticsIdentification2012, and @vorburgerTopographyMeasurementsApplications2015. 
In this study, the bullet matching algorithm used was developed by @hare2017automatic. 
The algorithm can briefly be described as follows:

1. A 3D scan is taken of each bullet land (containing striation marks to be compared), a stable cross-section is extracted, and shoulders (without relevant striation marks) are removed, as in \@ref(fig:shoulder), an image from @hare2017automatic.

```{r shoulder, echo=FALSE, fig.cap="The land is shown in the center. Shoulders are shown outside of the vertical lines."}
include_graphics(path = "images/shoulder.jpg")
```

2. A smoothing function is applied twice in order to extract the signature, a pattern of high and low points on the bullet's surface corresponding to the striation marks. 
The signature can then be compared to land signatures from other bullets, as in \@ref(fig:signature).

```{r signature, echo=FALSE, fig.cap="Two aligned signatures for a known match. Image generated from Houston dataset by authors."}
include_graphics(path = "images/F526_Match_Signatures.png")
```

3. Traits, such as consecutively matching striae and depth of grooves, are then used in a random forest to produce a match score (ranging from 0 to 1) for the lands. 
The random forest consists of decision trees that consider a combination of variables and responses in order to predict if two signatures were created by the same gun. 
\authorcol{The decision trees in a random forest each consider a subset of variables and observations to decide whether or not the lands match.}
The decisions of these trees are combined to produce the match score. 
Lands are then aligned across bullets in order to compute an overall match score for the bullets, as in \@ref(fig:grid).

```{r grid, fig.cap="Diagonal correspondence (in orange) among lands indicate a match, as shown here. Image generated from Houston dataset by authors."}
include_graphics(path = "images/Test_Fire_F526.jpeg")
```

This algorithm was trained on Hamby's Consecutively Rifled Ruger Barrel Study data sets 252 and 173 [@vanderplasComparisonThreeSimilarity2020], and the final random forest was able to correctly predict all matches and non-matches [@hare2017automatic, p. 2350]. 
Three sets were then used to verify the algorithm: Hamby set 44, Phoenix PD, and Houston FSC [@vanderplasComparisonThreeSimilarity2020, p. 5]. 
The algorithm performed well for undamaged bullets, and was able to completely distinguish between matches and non-matches when a cutoff value was individually chosen for each data set [@vanderplasComparisonThreeSimilarity2020, p. 10].

### Explainable Machine Learning - Previous Research

Jurors' ability to interpret statistical methods and language is \authorcol{in doubt}; in a study conducted by @koehler2001 with regards to the probability of a random match in DNA evidence, they found that jurors had different interpretations when the probability was presented as 1 out of 1,000 versus 0.1 out of 100, where those presented with a decimal number were more likely to view the probability of a random match as smaller. 
In another study, @garrettComparingCategoricalProbabilistic2018 asked jurors to evaluate evidence that used the following FRStat language, typically used in fingerprint analysis: "The probability of observing this amount of correspondence is approximately [XXX] times greater when the impressions are made by the same source rather than by different sources" [language from @DFSCLPInformation2018].
When asked for the likelihood the defendant committed the crime when presented with the above FRStat language, participants did not provide significantly different likelihoods for values ranging from 10 times greater to 100,000 times greater. 
This may demonstrate a lack of understanding when jurors are presented with numerical results for statistical evidence. 
@ENFSIGuidelineEvaluative2016 proposes a verbal scale to supplement likelihood ratios, which may alleviate some of the burden of statistical interpretation from potential jurors. 
This scale ranged from weak support, corresponding to a likelihood ratio between 2 and 10, to very strong support, corresponding to a likelihood ratio greater than 10,000 [@ENFSIGuidelineEvaluative2016, p. 64]. 
When interviewing judges, lawyers, forensic scientists, and forensic researchers, @swoffordProbabilisticReportingAlgorithms2022 found that many expressed concern regarding the interpretation of probabilistic language, and several suggested a combination of both match and probabilistic language. 
@hare2017automatic's algorithm differs from previous presentation methods in that its output is a match score, as opposed to a likelihood ratio. 
In this study, potential jurors may encounter both the algorithm's match score alongside the categorical match language of the examiner.

### Demonstrative Evidence

Images are often used to assist individuals in understanding non-image information. 
However, images may have an effect of making a scenario more believable, as demonstrated in a study by @cardwellNonprobativePhotosRapidly2016 which involved "giving" or "taking" food from animals with or without images. 
Images are also mentioned as a factor that can influence how truthful individuals view statements in the courtroom - even if no new information is presented through the presence of the image [@kellermannTrialAdvocacyTruthiness2013].
Therefore, the use of images in a courtroom should be studied for an effect with regards to how subjects perceive the evidence presented - namely, if there is a difference in how reliable or credible they feel the experts are, based on the presence or absence of images.
\authorcol{Because the images are intended to aid only in interpretation, we would hope for an increase in understanding across all other conditions, while reliability and credibilty stay the same.}
\authorcol{However, based on previous research, it seems probable that the presence of images would increase the perceived reliability and credibility of the evidence and expert, respectively.}

## Methods

### Study Format

\authorcol{First,} participants are presented with a short scenario based on @garrettMockJurorsEvaluation2020: a bullet is the only evidence recovered from an attempted convenience store robbery, and is tested against a gun found in Richard Cole's car in a routine traffic stop. 
\authorcol{Participants are informed that the store clerk was unable to identify the robber because they were wearing a ski mask, and that the testimony presented represented all relevant information.}
\authorcol{They were also advised that they would be unable to re-read testimony, and a notepad was provided for their convenience.}
Participants are then asked to read a transcript of mock court testimony, and rate their impression of the evidence presented, as well as their impression of the experts. 
This document included expert testimony, cross examination, and questions from the jury conveyed through the judge regarding error rates. 
\authorcol{The expert testified to their qualificiations, as well as the process of bullet matching.}
\authorcol{They then described comparing the fired evidence to a test fire from the defendant's gun, and the results of the comparison.}
\authorcol{When the algorithm was included, the firearms examiner also described the algorithm's match score resulting from this comparison.}
\authorcol{Cross examination included questions regarding the ability to uniquely identify the source of the bullet, as well as the subjectivity of the comparison.}

\authorcol{When the algorithm was included, an algorithm expert then testified.}
\authorcol{They also listed their qualifications and involvement in the development of the algorithm.}
\authorcol{The expert would then describe the process for obtaining a match score (similar to the algorithm description in the previous section).}
\authorcol{They also spoke to the publication history of the algorithm, the code availability, and the algorithm's applicability to the type of firearm considered in the case.}
\authorcol{Cross examination included questions on the newness of the algorithm, as well as subjective aspects of calibration, and limitations with respect to the types of bullets that it can evaluate.}
The testimony was based on actual court testimony provided by forensics experts lawyers, and judges, shown in Appendix \@ref(testimony-transcripts).

\authorcol{After reading the transcript, participants were directed to respond to some questions regarding the testimony.}
\authorcol{They were first given information about their responsibility as jurors to choose whether or not to convict, and the 'beyond reasonable doubt' threshold was established before asking participants for their conviction decision.}
\authorcol{Participants were also asked to estimate the probability that the defendant committed the crime, and the probability that the gun was involved in the crime.}
\authorcol{These questions were followed by several Likert scales on the strength of evidence, the credibility of the examiners, reliability and scientificity of the evidence, and understanding of the procedures.}
Two attention check questions were asked, in order to ensure that participants were reading both the testimony and the subsequent questions. 
The first attention check asked participants to identify the caliber of bullet recovered from the crime scene, while the second attention check asked participants to select a specific value from a Likert scale.  

The study includes three \authorcol{independent} variables: \authorcol{presence or absence} of the algorithm, \authorcol{presence or absence} of demonstrative evidence (images), and conclusion (match, exclusion, or inconclusive). 
In the case of the algorithm, two testimonies were presented: that of the firearms expert (Terry Smith), and that of the algorithm expert (Adrian Jones). 
The firearms expert presented the algorithm results for the case alongside their own analysis, and suggests that the algorithm's results supports their conclusion. 
By presenting the algorithm results with the interpretation suggested by the firearms expert, we hoped that potential jurors could use the firearms expert's explanation and conclusion to guide their understanding of the algorithm's results. 
The algorithm expert then describes in greater detail the algorithm's process, and its validity for this particular case. 
When demonstrative evidence is present, images of rifling [@105mmTank2005;@gremi], a bullet comparison, and algorithm images (such as those shown above) were included alongside the testimony. 
In terms of the conclusion: a "match" indicated agreement in individual and class characteristics; an "exclusion" indicated an agreement of class characteristics, but disagreement in individual characteristics; and an "inconclusive" indicated an agreement in class characteristics, but not enough agreement in individual characteristics to state that there was a match.  

The number of survey questions that respondents received depended on the scenario; participants who received the algorithm were asked more questions than those who did not receive the algorithm. 
For example, participants who did not receive the algorithm were asked about the reliability of the forensics examiner's bullet comparison and the reliability of the field of firearms as a whole. 
Those who received the algorithm were asked about the reliability of the forensics examiner's bullet comparison, the reliability of the algorithm's comparison, the overall reliability in the case (which includes both the algorithm comparison and the forensics expert's comparison), as well as the reliability of the field of firearms as a whole. 
This same format was used when asking participants about credibility and scientificity.

### Prolific

Participants were recruited using Prolific, an online survey-taking website. 
\authorcol{From the Prolific website, participants were directed to a link containing our survey, created using RShiny.}
We selected options to recruit a representative sample of individuals located in the United States, and asked that participants self-screen for jury eligibility before completing the survey. 
Jury eligibility was defined as US citizens over the age of majority in their state who had not been convicted of a felony, were not active law enforcement, military, emergency response, or a judge, and who did not have a disability that would prevent them from serving on a jury. 
They were also required to have normal or corrected to normal vision, due to the images used in the study. 
Participants were compensated with \$8.40 for completing the study, for an hourly compensation rate of about \$27.79 (median completion time of approximately 18 minutes).  
Individuals who did not include their Prolific identification number and an individual whose notes indicated that they had progressed far enough into the survey to get a separate scenario before restarting were excluded from analysis.

```{r}
#| completiontime,
#| fig.cap= "Completion Time by Condition for all unique fingerprints",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

fingerprint_freq<-table(merged_results_allresponses$clean_prints)

freq_df <- data.frame(fingerprint_freq)
single_fingerprint <- freq_df %>%
  filter(Freq == 1)
scenarios <- merged_results_allresponses %>% dplyr::select(scenario_number, conclusion, clean_prints, 
                                         algorithm, picture, time) %>%
  filter(clean_prints %in% single_fingerprint$Var1) %>% rename(finish_time = time)
beginning_wend <- right_join(demographics, scenarios)
beginning_wend$total_time <- NA
beginning_wend$total_time <- beginning_wend$finish_time - beginning_wend$time
beginning_wend$total_time_minutes <- beginning_wend$total_time/60
dim_g_75 <- beginning_wend %>% filter(total_time_minutes > 75) %>% dim()
beginning_wend$truncated_minutes <- beginning_wend$total_time_minutes
beginning_wend[beginning_wend$total_time_minutes > 75,]$truncated_minutes <- Inf

# median(beginning_wend[beginning_wend$algorithm=="Yes",]$total_time_minutes, na.rm=TRUE)
# median(beginning_wend[beginning_wend$algorithm=="No",]$total_time_minutes, na.rm=TRUE)

png(filename="images/completiontime.png",type = "cairo-png")
ggplot(beginning_wend, aes(x = truncated_minutes, fill=algorithm)) +
    geom_density(alpha=0.75, color=NA) |> partition(vars(algorithm)) |> blend("multiply")+
  facet_grid(conclusion~picture, labeller =  purrr::partial(label_both, sep = ":\n"))+
  ggtitle("Histogram of Completion Time") +
  xlab("Completion Time in Minutes")+
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()
hide<-dev.off()


include_graphics(path = "images/completiontime.png")


```
Figure \@ref(fig:completiontime) depicts completion times (from completion of the demographics information to the end of the survey) by conditions for unique fingerprints (n=`r dim(single_fingerprint)[1]`), excluding `r dim_g_75[1]` observations greater than 75 minutes. In general, it appears that those who received the algorithm condition took slightly more time on average than those who did not (median values of `r  round(median(beginning_wend[beginning_wend$algorithm=="Yes",]$total_time_minutes, na.rm=TRUE), digits=3)` minutes and `r round(median(beginning_wend[beginning_wend$algorithm=="No",]$total_time_minutes, na.rm=TRUE), digits=3)` minutes, respectively). This is unsurprising, given the increased length of the algorithm condition.


## Results

\authorcol{Due to scale compression, no statistical analysis was performed on this data.}

### Participants

```{r}
#| demographics,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

# dim(demographics)
 # table(demographics$gender)
 # sum(demographics$gender=="Female")
# median(demographics$age, na.rm=TRUE)

# dim(merged_results_allresponses)
# dim(merged_results)

ggplot(subset(demographics, !is.na(age) & age > 17), aes(x = gender, y=age)) +
    geom_violin(position = position_dodge(1),
              draw_quantiles=c(.25, .5, .75),
              color = "black",
              alpha = 0.5) +
  geom_jitter(width=0.15)+
  ggtitle("Age by Gender") +
  theme_bw()

```
Of the `r dim(demographics)[1]` participants with Prolific ID's who started the survey, there were `r sum(demographics$gender=="Female")` female participants, `r sum(demographics$gender=="Male")` male participants, and `r sum(demographics$gender=="Other/non-binary")` non-binary participants.
\authorcol{These participants are a representative sample (in terms of age, sex and ethnicity, based on census data) of Prolific participants who are located in the United States.}
The median age was `r median(demographics$age, na.rm=TRUE)`. 
Age and gender are shown in Figure \@ref(fig:demographics).
An age of 0.2 was excluded due to implausibility and lack of study completion.
`r dim(merged_results_allresponses)[1]` participants eligible for analysis completed the survey, and `r dim(merged_results)[1]` participants correctly answered both attention check questions.
\authorcol{In two instances, participants reported being removed from the survey website prior to survey completion, which may have happened in other cases where the participants did not complete the survey.}
The division of correct answers on the attention check questions are shown in Table \@ref(tab:attentioncheck).
These `r dim(merged_results)[1]` participants were considered for the following analysis.

```{r}
#| attentioncheck,
#| echo= FALSE,
#| message= FALSE,
#| eval= TRUE
merged_results_allresponses$gun_caliber<-NA
merged_results_allresponses$gun_caliber[merged_results_allresponses$check1=="9mm"] <- "Caliber Correct"
merged_results_allresponses$gun_caliber[merged_results_allresponses$check1!="9mm"] <- "Caliber Incorrect"

merged_results_allresponses$question_reading<-NA
merged_results_allresponses$question_reading[merged_results_allresponses$check2=="Moderately reliable"] <- "Reading Correct"
merged_results_allresponses$question_reading[merged_results_allresponses$check2!="Moderately reliable"] <- "Reading Incorrect"

dem_table<-table(merged_results_allresponses$gun_caliber, merged_results_allresponses$question_reading)
kable(dem_table, caption="Attention Check Information")
# ggplot(merged_results_allresponses) +
#   ggmosaic::geom_mosaic(aes(x = product(gun_caliber), fill=question_reading))+
#   ggtitle("Correct Answers") +
#     scale_fill_manual(values = c("grey", "black"))+
#   theme_bw()

```
### Overview

Participants were asked a variety of questions in order to assess their thoughts and feelings on the case and the evidence presented. 
Two questions related to probability: respondents were asked to provide a value for the probability that the gun was at the crime scene, and to provide a value for the probability that Richard Cole (the defendant) committed the crime. 
Another question asked participants if they would convict Cole, based on the evidence. 
Questions of credibility, reliability, and scientificity were assessed using a 7-point Likert scale (eg. "Extremely unscientific" to "Extremely scientific"). 
Understanding was ranked on a 5-point Likert scale ("I understood nothing" to "I understood everything"). 
Participants were also asked about uniqueness of firearm toolmarks, as a simple yes/no question. 
Strength of evidence was ranked on a 9-point Likert scale ("Not at all strong" to "Extremely strong"). The perceived frequency with which firearms examiners made mistakes was assessed on a 7-point Likert scale ("Never" to "Always").  

### Probability

There is a difference in the perceived probability that Cole committed the crime as well as the perceived probability that the gun was present at the crime scene based on the examiner's decision, as shown in Figure \@ref(fig:probalgorithm). 
The match condition resulted in high probabilities, while the non-match condition resulted in low probabilities, indicating that participants reacted to the examiner's testimony. 
A wider range of probability values were observed for the inconclusive decision, without the high peaks that were present for conclusive decisions. 
Definite conclusions of match or not a match resulted in higher peaks at more extreme values for the probability that the gun was at the crime scene, compared with the probability that Cole committed the crime. 
This may indicate that some individuals are distinguishing between evidence that the weapon was used and evidence against Cole. 
The inconclusive decision resulted in a more spread out distribution that is practically the same for both the probability that the gun was at the crime scene and the probability that Cole committed the crime, with participants generally selecting values below 50%.  

```{r}
#| probcompare,
#| fig.cap= "Comparison of Probabilities Selected for Cole and the Gun",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE
merged_prob <-
  dplyr::select(merged_results, c("conclusion", "probability", "gunprob"))

merged_problong <-
  merged_prob %>% gather(subject, probability,-conclusion)



ggplot(merged_problong, aes(x = probability, fill = subject)) +
  geom_density(alpha = 0.5, color=NA) +
  facet_grid(. ~ conclusion, labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  scale_fill_manual(values = c("orange", "purple"),
                    labels = c("The Gun", "Cole"))+
  theme_bw()

```

For both Cole and the gun, values are slightly more concentrated toward the lower end of the scale when the algorithm is absent for the non-match condition, and the inconclusive decision produced similar densities across both algorithm conditions. 
In the case of the gun's involvement, values are more concentrated toward the higher end of the scale when the algorithm is present for the match condition. 
There is no real difference between match distributions for the probability that Cole committed the crime. 
The vertical lines indicate the match score presented by the algorithm (0.034 or 3.4% for the non-match condition, 0.34 or 34% for the inconclusive condition, and 0.989 or 98.9% for the match condition). 
These are displayed in order to visually assess whether the subjects are anchoring to the given match score when assessing the probability of the gun being present at the crime scene. 
Because the match score is on a scale of 0 to 1, this value could be misinterpreted as a probability for the gun being used in the crime. 
It does not appear that the participants are anchoring to this value, however, as the distributions do not correspond more to the line when the algorithm is present.

```{r}
#| probalgorithm,
#| fig.cap= "Probability the gun was used in the crime, or that Cole committed the crime. Black lines indicate bullet match scores for the algorithm.",
#| fig.width= 10,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

merged_probcombine<- merged_results %>% 
  select(algorithm, gunprob, conclusion, probability) %>%
  pivot_longer(cols=c(gunprob, probability), names_to = "Type", values_to="Probability")

merged_probcombine$Type <- factor(merged_probcombine$Type, levels = c("probability", "gunprob"), 
                  labels = c("Cole", "Gun"))

png(filename="images/probalgorithm.png",type = "cairo-png")
ggplot(merged_probcombine, aes(x = Probability, fill = algorithm)) +
  geom_density(alpha = 0.75, color=NA) |> partition(vars(algorithm)) |> blend("multiply") +
  geom_vline(data = filter(merged_probcombine, conclusion == "NoMatch"),
             aes(xintercept = 3.4)) +
  geom_vline(data = filter(merged_probcombine, conclusion == "Inconclusive"),
             aes(xintercept = 34)) +
  geom_vline(data = filter(merged_probcombine, conclusion == "Match"),
             aes(xintercept = 98.9)) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  facet_grid(Type ~ conclusion, labeller = label_both) +
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()
hide<-dev.off()

include_graphics(path = "images/probalgorithm.png")

```

```{r}
#| Coleprobalgorithm,
#| fig.cap= "Probability Cole committed the crime. Black lines indicate values produced by the algorithm",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(merged_results, aes(x = probability, fill = algorithm)) +
  geom_density(alpha = 0.5, color=NA) +
  geom_vline(data = filter(merged_results, conclusion == "NoMatch"),
             aes(xintercept = 3.4)) +
  geom_vline(data = filter(merged_results, conclusion == "Inconclusive"),
             aes(xintercept = 34)) +
  geom_vline(data = filter(merged_results, conclusion == "Match"),
             aes(xintercept = 98.9)) +
  ggtitle("Probability Cole Committed the Crime") +
  facet_grid(. ~ conclusion, labeller =  purrr::partial(label_both, sep = ":\n")) +
  scale_fill_manual(values = c("grey20", "tomato"))+
  theme_bw()

table(merged_results$guilty, merged_results$conclusion)
sum(merged_results$guilty=="Yes" & merged_results$conclusion=="NoMatch")
sum(merged_results$conclusion=="NoMatch")
```

#### Probability and Guilt

After reading the testimony, the participants were given the following question: 
"The State has the burden of proving beyond a reasonable doubt that the defendant is the person who committed the alleged crime. 
If you are not convinced beyond a reasonable doubt that the defendant is the person who committed the alleged crime, you must find the defendant not guilty. 
Would you convict this defendant, based on the evidence that you have heard?" 
`r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="NoMatch")` out of `r sum(merged_results$conclusion=="NoMatch")` individuals in the non-match category, `r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Inconclusive")` out of `r sum(merged_results$conclusion=="Inconclusive")` individuals in the inconclusive category, and `r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Match")` out of `r sum(merged_results$conclusion=="Match")` individuals in the match category chose to convict, despite the bullet matching being the only evidence against Cole in the crime. 
As \@ref(fig:probguilt) illustrates, across all categories individuals who chose to convict generally assigned a higher probability to Cole committing the crime than those who did not choose to convict. 
The same general trend of higher probability values for those who chose to convict and lower probability values for those who chose not to convict is also seen for non-match and inconclusive conditions when discussing the probability that the gun was used in the crime. 
However, in the case of the match condition, those who did not convict gave a generally higher probability that the gun was used in the crime than the probability that Cole committed the crime, resulting in comparable values (albeit with more variability) to those who chose to convict.

```{r}
#| probguilt,
#| fig.cap= "Probabilities based on whether the participants thought the defendant was guilty",
#| fig.width= 10,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

probplot <-
  ggplot(merged_results, aes(x = conclusion, y = probability, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  theme_bw()

gunplot <-
  ggplot(merged_results, aes(x = conclusion, y = gunprob, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Gun was used in the Crime") +
  ylab("probability")+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  theme_bw()

grid.arrange(probplot, gunplot, ncol = 2)

```


### Credibility

Through all conditions, the level of credibility remained approximately the same for both the firearms examiner and the algorithm expert. 
As Figure \@ref(fig:cred) demonstrates, "Extremely credible" was by far the most selected category for the firearms examiner, with some people selecting "Moderately credible", while the other categories quickly drop off in responses. 
This trend was also reflected in the data for the algorithm expert, and can be seen in most histograms resulting from this study (leading to a question of scale compression). 
The lack of difference based on examiner decision, image, or algorithm (in the case of the firearms examiner) provides a hopeful indication that the credibility of expert witnesses is not necessarily swayed by the facts of the case or the presence of images, when their written testimony remains the same.

```{r}
#| cred,
#| fig.cap= "Histogram of Firearms Examiner Credibility",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| warning= FALSE,
#| message= FALSE,
#| echo= FALSE


merged_results$firetestcred = factor(
  merged_results$firetestcred,
  levels = c(
    "Extremely noncredible",
    "Moderately noncredible",
    "Weakly noncredible",
    "Neither credible nor noncredible",
    "Weakly credible",
    "Moderately credible",
    "Extremely credible"
  )
)
algorithm_results <- merged_results %>% filter(algorithm == "Yes")
algorithm_results$algtestcred = factor(
  algorithm_results$algtestcred,
  levels = c(
    "Extremely noncredible",
    "Moderately noncredible",
    "Weakly noncredible",
    "Neither credible nor noncredible",
    "Weakly credible",
    "Moderately credible",
    "Extremely credible"
  )
)

levels(merged_results$firetestcred) <-
  gsub(" ", "\n", levels(merged_results$firetestcred))

examcred <-
  ggplot(subset(merged_results,!is.na(firetestcred)),
         aes(x = firetestcred, fill = conclusion)) +
  geom_bar(
    mapping = aes(y =after_stat(prop), group = conclusion),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #geom_histogram(stat="count", position="dodge")+
  facet_grid(algorithm ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle("How credible did you find the testimony of Terry Smith\n(the firearm examiner)?") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())
  

levels(algorithm_results$algtestcred) <-
  gsub(" ", "\n", levels(algorithm_results$algtestcred))

algcred <-
  ggplot(subset(algorithm_results,!is.na(algtestcred)),
         aes(x = algtestcred, fill = conclusion)) +
  geom_bar(mapping = aes(y = after_stat(prop), group = conclusion),
           position = "dodge", color="black") +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture ~ ., labeller = label_both) +
  ggtitle("Algorithm Expert Testimony") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

#grid.arrange(examcred, algcred, ncol=2)
examcred
```

### Reliability


In terms of reliability, participants appeared to find the case evidence less reliable when an inconclusive decision was given than they did when a conclusive decision was reached, as shown in Figure \@ref(fig:caserel). 
Note that this question was only answered by participants who received the algorithm condition and is meant to encompass both the examiner's comparison and the algorithm comparison. 
As with the credibility condition, the majority of individuals selected the two highest conditions, "Moderately reliable" and "Extremely reliable", across all categories of conclusions. 
In the case of an inconclusive decision, the highest proportion of participants selected "Moderately reliable", while for the conclusive decisions, the highest proportion of participants selected "Extremely reliable".

```{r}
#| caserel,
#| fig.cap= "Histogram of overall case reliability",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

df.overrel <- ddply(
  algorithm_results,
  .(conclusion),
  summarise,
  prop = prop.table(table(overrel)),
  overrel = names(table(overrel))
)

df.overrel$overrel = factor(
  df.overrel$overrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

#Reorder factors (match/inconclusive/nomatch)
#gsub on axis (replace spaces with new lines)
levels(df.overrel$overrel) <-
  gsub(" ", "\n", levels(df.overrel$overrel))
ggplot(df.overrel, aes(overrel, prop, fill = conclusion)) +
  geom_bar(stat = "identity", position = position_dodge(preserve = "single"), color="black") +
  ggtitle("How reliable do you think the firearm evidence in this case is?") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```

Individuals were also asked to rate the general reliability of firearms evidence as a field (Figure \@ref(fig:genrel)). 
As with case reliability, those who received an inconclusive condition were more likely to select "Moderately reliable" than they were to select "Extremely reliable". 
This trend is also shown in the non-match condition when the algorithm is present. 
However, it is not reflected in the match condition. 
When the algorithm is absent, both the match and the non-match conditions appear to produce approximately equal proportions for the two highest categories of reliability. 
As with previous responses, all other categories are sparsely populated.

```{r}
#| genrel,
#| fig.cap= "Histogram of perceived firearm reliability as a field",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$firerel = factor(
  merged_results$firerel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(merged_results$firerel) <-
  gsub(" ", "\n", levels(merged_results$firerel))

ggplot(subset(merged_results,!is.na(firerel)),
       aes(x = firerel, fill = conclusion)) +
  geom_bar(
    mapping = aes(y = after_stat(prop), group = conclusion),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(algorithm ~ ., labeller = label_both) +
  ggtitle("In general, how reliable do you think firearm evidence is?") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())
```


In all conditions, participants were asked to rate the reliability of the examiner's subjective opinion of the firearm evidence (Figure \@ref(fig:examrel)).
Participants from both the algorithm and non-algorithm groups gave similar reliability ratings in the non-match condition: most chose "Moderately reliable" or "Extremely reliable", with more choosing "Extremely reliable". 
For inconclusive and match conditions, there is a difference in proportions of which category is selected based on the presence or the absence of the algorithm. 
When the algorithm is absent, the trend is fairly similar to that in the non-match condition: between the two highest categories, the majority chose "Extremely reliable". 
However, in the case that the algorithm was present, this trend was flipped: more participants chose "Moderately reliable" over "Extremely reliable". 

```{r}
#| examrel,
#| fig.cap= "Histogram of perceived firearm exam reliability",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$examrel = factor(
  merged_results$examrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(merged_results$examrel) <-
  gsub(" ", "\n", levels(merged_results$examrel))

ggplot(subset(merged_results,!is.na(examrel)),
       aes(x = examrel, fill = algorithm)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = algorithm),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle(
    "How reliable do you think the firearms examiner's subjective opinion \n of the bullet comparison is, in this case?"
  ) +
  scale_fill_manual(values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

## Cannot center title: faceting variable error
#  theme(plot.title=element_text(hjust=0.5))+
```

Individuals who received the algorithm were also asked to rate algorithm reliability, responses are shown in Figure \@ref(fig:algrel). 
The responses were in many ways similar to those given in the case of general reliability: those who received an inconclusive decision were more likely to give a lower reliability rating, and the two highest categories were by far the most selected. 
Both also demonstrated a higher selection of "Weakly reliable" when individuals were presented with an inconclusive decision.

```{r}
#| algrel,
#| fig.cap= "Histogram of perceived algorithm reliability",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

df.algrel <- ddply(
  algorithm_results,
  .(conclusion),
  summarise,
  prop = prop.table(table(algrel)),
  algrel = names(table(algrel))
)

df.algrel$algrel = factor(
  df.algrel$algrel,
  levels = c(
    "Extremely unreliable",
    "Moderately unreliable",
    "Weakly unreliable",
    "Neither reliable nor unreliable",
    "Weakly reliable",
    "Moderately reliable",
    "Extremely reliable"
  )
)

levels(df.algrel$algrel) <-
  gsub(" ", "\n", levels(df.algrel$algrel))

ggplot(df.algrel, aes(algrel, prop, fill = conclusion)) +
  geom_bar(stat = "identity", position = position_dodge(preserve = "single"), color="black") +
  ggtitle("How reliable do you think the firearm algorithm evidence is,\n in this case?") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

In most cases, individuals gave lower reliability ratings when an inconclusive decision was reached. 
They also tended to select the two highest reliability categories, "Moderately reliable" and "Extremely reliable", regardless of other conditions.
The presence or absence of images did not have a noticeable effect on reliability ratings. 
The presence of the algorithm is related to a slight reduction in reliability ratings for the firearms examiner's personal bullet comparison, in the case of an inconclusive or match decision.

### Scientificity

Participants were also asked about how scientific they felt the process was, in a similar four-question format to reliability. 
These results bore some similarity in responses to reliability across questions. 
As previously stated, images did not appear to have a large effect and the two highest categories were by far the most selected. 
When asked about their rating of how scientific the evidence was in the case overall, those receiving the inconclusive condition were less likely to select "Extremely scientific" compared to their counterparts given conclusive conditions.  

In terms of firearms evidence as a field, results are shown in Figure \@ref(fig:gensci). 
Here, as before, those who received the inconclusive decision were more likely to select "Moderately scientific" over "Extremely scientific" for both cases of the algorithm. 
A higher proportion of individuals selected "Extremely scientific" for conclusive decisions when the algorithm was present compared to when the algorithm was absent.

```{r}
#| gensci,
#| fig.cap= "Histogram of perceived firearm scientificity as a field",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$firesci = factor(
  merged_results$firesci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$firesci) <-
  gsub(" ", "\n", levels(merged_results$firesci))

ggplot(subset(merged_results,!is.na(firesci)),
       aes(x = firesci, fill = algorithm)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = algorithm),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle("In general, how scientific do you think the firearm evidence is?") +
  scale_fill_manual(values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

Regarding how scientific individuals found the examiner's comparison, the algorithm only seemed to have an effect when individuals were presented with an inconclusive decision (\@ref(fig:examsci)). 
When the algorithm was not present, people were more likely to select "Extremely scientific", resulting in a proportion on par with conclusive results. 
When the algorithm was present, however, "Moderately scientific" became the most popular choice for those who received an inconclusive decision, which reflects the general trend of the majority of participants selecting the second highest category for inconclusive results. 
Results for conclusive categories are fairly similar across algorithm conditions, with a close split between "Moderately scientific" and "Extremely scientific".

```{r}
#| examsci,
#| fig.cap= "Histogram of perceived scientificity of the bullet comparison of the firearm examiner",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$examsci = factor(
  merged_results$examsci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(merged_results$examsci) <-
  gsub(" ", "\n", levels(merged_results$examsci))

ggplot(subset(merged_results,!is.na(examsci)),
       aes(x = examsci, fill = algorithm)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = algorithm),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle(
    "How scientific do you think the firearms examiner\'s subjective opinion \n of the bullet comparison evidence is, in this case?"
  ) +
  scale_fill_manual(values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

Participants gave the algorithm a high rating in scientificity across all categories of conclusions, as shown in Figure \@ref(fig:algsci). 
Here, all decisions had the highest proportion of respondents select "Extremely scientific". 
The proportion selecting "Moderately scientific" was noticeably lower than the proportion selecting "Extremely scientific".
The use of demonstrative evidence did not have a visible effect.

```{r}
#| algsci,
#| fig.cap= "Histogram of perceived algorithm scientificity in this case",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algorithm_results$algsci = factor(
  algorithm_results$algsci,
  levels = c(
    "Extremely unscientific",
    "Moderately unscientific",
    "Weakly unscientific",
    "Neither scientific nor unscientific",
    "Weakly scientific",
    "Moderately scientific",
    "Extremely scientific"
  )
)

levels(algorithm_results$algsci) <-
  gsub(" ", "\n", levels(algorithm_results$algsci))

ggplot(subset(algorithm_results,!is.na(algsci)),
       aes(x = algsci, fill = conclusion)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = conclusion),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture ~ ., labeller = label_both) +
  ggtitle("How scientific do you think the firearm algorithm evidence is, \n in this case?") +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

In summary, the algorithm is related an an increase of perceived scientificity for the field of firearm evidence as a whole when individuals were presented with a conclusive decision, and "Extremely scientific" was the most selected category when evaluating the scientificity of the algorithm evidence regardless of conclusion. 
As for how individuals rated the scientificity of the algorithm expert's comparison, the algorithm only appeared to influence results for those who received the inconclusive condition. 
Individuals who received the algorithm were more likely to select "Moderately scientific", while those who did not receive the algorithm were more likeley to select "Extremely scientific". 

### Understanding

Individuals were asked to rate their understanding of both the algorithm and the examiner's personal bullet comparison on a 5-point Likert scale. Most responses ranged from 3 ("I understood about half of the method") to 5 ("I understood everything"), leading to less scale compression than was seen in scales relating to credibility, reliability, and scientificity.  

For the firearms examiner's personal comparison, few individuals selected that they understood less than half the method (Figure \@ref(fig:expunder)). 
Those who did not receive the algorithm were more likely to select "I understood everything" compared to those who did receive the algorithm, across all conclusions. 
There was not a discernible difference in responses based on the presence or absence of images.

```{r}
#| expunder,
#| fig.cap= "Histogram of understanding for the explanation of the firearms examiner",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$expunder = factor(
  merged_results$expunder,
  levels = c(
    "1 <br/> I understood nothing",
    "2.0",
    "3 <br/> I understood about half of the method",
    "4.0",
    "5 <br/> I understood everything"
  )
)

levels(merged_results$expunder) <-
  gsub("<br/>", "", levels(merged_results$expunder))

levels(merged_results$expunder) <-
  gsub("  ", " ", levels(merged_results$expunder))

levels(merged_results$expunder) <-
  gsub(" ", "\n", levels(merged_results$expunder))

ggplot(subset(merged_results,!is.na(expunder)),
       aes(x = expunder, fill = algorithm)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = algorithm),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion ~ ., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle(
    "Based on this testimony, how would you rate your understanding of \n the method described for the examiner's personal bullet comparison?"
  ) +
  scale_fill_manual(values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))


```

The results for the particpants' understanding of the algorithm description differs based on the presence or absence of images (Figure \@ref(fig:algunder)).
When images are absent, participants selected values from "I understood about half the method" to "I understood everything" with a fairly uniform frequency, with a few participants selecting values in the lower categories. 
When images were present, however, category selection appeared to relate to conclusion. 
Those receiving the match condition were more likely to select 4 than other categories, meaning that they felt they understood more than half of the method but didn't understand everything. 
Those receiving the non-match condition were more likely to select that they understood half of the method compared to other categories. 
Those with an inconclusive condition had responses fairly evenly distributed across the top three categories, similar to when images were absent.
As in the case without images, few individuals indicated that they understood less than half of the method.
It is unclear what may have caused these differences in ratings of understanding.

```{r}
#| algunder,
#| fig.cap= "Histogram of understanding for the algorithm explanation",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algorithm_results$algunder = factor(
  algorithm_results$algunder,
  levels = c(
    "1 <br/> I understood nothing",
    "2.0",
    "3 <br/> I understood about half of the method",
    "4.0",
    "5 <br/> I understood everything"
  )
)

levels(algorithm_results$algunder) <-
  gsub("<br/>", "", levels(algorithm_results$algunder))

levels(algorithm_results$algunder) <-
  gsub("  ", " ", levels(algorithm_results$algunder))

levels(algorithm_results$algunder) <-
  gsub(" ", "\n", levels(algorithm_results$algunder))

ggplot(subset(algorithm_results,!is.na(algunder)),
       aes(x = algunder, fill = conclusion)) +
  geom_bar(
    mapping = aes(y = ..prop.., group = conclusion),
    position = position_dodge(preserve = "single"), color="black"
  ) +
  #  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture ~ ., labeller = label_both) +
  ggtitle(
    "Based on this testimony, how would you rate your understanding of \n the method described for the bullet matching algorithm?"
  ) +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))

```
\authorcol{To investigate this potential relationship further, Table} \@ref(tab:undertb)
\authorcol{was created. Because the counts are so small for the individual cells, the resemblence to a relationship based on conclusion is possibly due to random variation.}
```{r}
#| undertb,
#| echo= FALSE,
#| message= FALSE,
#| eval= TRUE
kable(table(subset(algorithm_results,!is.na(algunder) & picture=="Yes")$algunder,
      subset(algorithm_results,!is.na(algunder) & picture=="Yes")$conclusion),
      caption = "Understanding Frequency")
# %>% 
#   column_spec(1, width = "10em")
```

### Uniqueness

Individuals were asked whether or not they thought that guns left unique markings on discharged bullets and casings after reviewing the testimony. 
Of the `r dim(merged_results)[1]` responses, only `r sum(merged_results$unique=="No")` individuals indicated that they did not think guns left unique markings. 
These respondents were split across conditions. 
Thus, respondents in this study overwhelmingly believe in the uniqueness of markings on discharged bullets. 
<!-- Would responses be different if they were asked to evaluate uniqueness before reading the testimony? -->
```{r, eval=F, echo=F}

table(merged_results$unique)
sum(merged_results$unique=="No")
```

### Strength

```{r}
#| strength,
#| fig.cap= "Histogram of perceived strength of evidence against the defendant",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE
# How strong is the evidence that the defendant's gun was used to fire the shot in
             #       the convenience store, in your opinion?

merged_results$strength=factor(merged_results$strength, levels =c("1 <br/> Not at all strong","2.0", "3.0",
                                                                  "4.0","5 <br/> Moderately strong",
                                                                  "6.0", "7.0", "8.0", "9 <br/> Extremely strong"))

levels(merged_results$strength) <-
  gsub("<br/>", "", levels(merged_results$strength))

levels(merged_results$strength) <-
  gsub("  ", " ", levels(merged_results$strength))

levels(merged_results$strength) <-
  gsub(" ", "\n", levels(merged_results$strength))

ggplot(subset(merged_results, !is.na(strength)), aes(x=strength, fill = algorithm))+
  geom_bar(mapping = aes(y = ..prop.., group = algorithm), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion~., labeller =  purrr::partial(label_both, sep = ":\n")) +
  ggtitle(
    "How strong would you say the case against the defendant is?"
  ) +
  scale_fill_manual(values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nNot\nat all\nstrong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))


```

Individuals were also asked to rate the strength of evidence, both against Richard Cole and against the gun. 

When asked about the case against the defendant, shown in Figure \@ref(fig:strength), there was a small difference in terms of the algorithm. 
When the examiner reached a non-match conclusion, individuals who also received the algorithm were more likely to select the lowest category ("not at all strong") compared to those who did not receive the algorithm. 
Alternatively, in the case of an inconclusive decision, individuals who did not receive the algorithm were more likely to select "not at all strong" compared to those who did receive the algorithm.
For the match condition, responses were more widely distributed.

```{r}
#| gunstrength,
#| fig.cap= "Histogram of perceived strength of evidence against the gun",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$gunstrength=factor(merged_results$gunstrength, levels =c("1 <br/> Not at all strong","2.0", "3.0",
                                                                  "4.0","5 <br/> Moderately strong",
                                                                  "6.0", "7.0", "8.0", "9 <br/> Extremely strong"))

levels(merged_results$gunstrength) <-
  gsub("<br/>", "", levels(merged_results$gunstrength))

levels(merged_results$gunstrength) <-
  gsub("  ", " ", levels(merged_results$gunstrength))

levels(merged_results$gunstrength) <-
  gsub(" ", "\n", levels(merged_results$gunstrength))

ggplot(subset(merged_results, !is.na(gunstrength)), aes(x=gunstrength, fill = algorithm))+
  geom_bar(mapping = aes(y = ..prop.., group = algorithm), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(conclusion~., labeller = purrr::partial(label_both, sep = ":\n")) +
  ggtitle(
    "How strong is the evidence that the defendant's gun
    \nwas used to fire the shot in the convenience store, in your opinion?"
  ) +
  scale_fill_manual("Algorithm", values = c("grey20", "tomato")) +
  theme_bw()+
  theme(axis.title.x = element_blank(),
        legend.position = c(1, 1), legend.justification = c(1, 1), 
        legend.direction = "horizontal", legend.background = element_rect(fill = "transparent"))+ 
  scale_x_discrete(labels=c('1\nNot at\nall strong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))


```

When asked about the strength of evidence against the defendant's gun, there was no real difference between those who received the algorithm and those who did not (\@ref(fig:gunstrength)).
The match condition resulted in a more concentrated distribution at higher strength values than were seen for the strength of evidence against Cole.

```{r}
#| probstrength,
#| fig.cap= "Probabilities based on perceived strength of evidence.",
#| fig.width= 10,
#| fig.height= 4.5,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

strengthdat <- merged_results %>% 
  select(strength, probability, conclusion, gunstrength, algorithm, picture) %>%
  pivot_longer(cols=c("strength", "gunstrength"), names_to="strength", values_to="scale") %>%
  mutate(strength = stringr::str_replace_all(strength, c("gunstrength" = "Gun", "^strength$" = "Defendant")))

levels(strengthdat$scale) <-
  gsub(" ", "\n", levels(strengthdat$scale))

 ggplot(strengthdat, aes(x = scale, y = probability, color = strength, fill = strength)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    alpha = 0.5,
    size = 0.5
  ) +
  geom_violin(position = position_dodge(1),
              draw_quantiles=c(.25, .5, .75),
              color = "black",
              alpha = 0.5,
              outlier.shape = NA) +
  ggtitle("Probability __ was Involved in/Committed the Crime") +
  scale_color_manual("Strength of\nEvidence Against", values = c("orange", "purple"))+
  scale_fill_manual("Strength of\nEvidence Against", values = c("orange", "purple"))+
   theme_bw()+
  theme(axis.title.x = element_blank(), 
        legend.position = c(0, 1), legend.justification = c(0, 1), 
        legend.direction = "horizontal", legend.background = element_rect(fill = "transparent")) + 
  scale_x_discrete(labels=c('1\nNot at\nall strong', '2', '3', '4', '5\nModerately\nstrong', '6', '7', '8', '9\nExtremely\nstrong'))
```

In comparing how individuals scored strength of evidence and the probability that they assigned to Cole committing the crime and the gun being present at the crime scene, the results were largely consistent (\@ref(fig:probstrength)). 
In general, the probability assigned increases as the assigned strength of evidence increases, which is to be expected. 

### Mistakes

Individuals were asked how often firearms examiners make mistakes when determining whether bullets were fired through the same gun, with a scale from "Never" to "Usually". 
"Rarely" was by far the most selected category, as shown in Figure \@ref(fig:mistakes). 
There does not appear to be a strong relationship between how often individuals felt firearms examiners made mistakes and factors such as conclusion, images, or the algorithm.

```{r}
#| mistakes,
#| fig.cap= "Histogram of perceived frequency of mistakes made by firearms examiners",
#| fig.width= 7,
#| fig.height= 3,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

merged_results$mistakes=factor(merged_results$mistakes, levels =c("Never","Rarely", "Occasionally",
                                                                  "Sometimes", "Frequently", "Usually", "Always"))


ggplot(subset(merged_results, !is.na(mistakes)), aes(x=mistakes, fill = conclusion))+
  geom_bar(mapping = aes(y = ..prop.., group = conclusion), position=position_dodge(preserve = "single"), color="black") +
#  geom_histogram(stat="count", position="dodge")+
  facet_grid(picture~., labeller = label_both) +
  ggtitle(
    "How often do firearms examiners make mistakes when determining whether \n bullets were fired through the same gun?"
  ) +
  scale_fill_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```


### Comparing Algorithm Values to Examiner Values 

Participants who received the algorithm condition were asked to evaluate their feelings on the reliability, credibility, scientificity, and their understanding of both the algorithm method as well as the traditional bullet analysis method, as discussed in previous sections. 
Results for both methods were compared on an individual level as well as on a group level. 
The group level results are represented in histograms, while the individual level results are represented in parallel coordinates plots. 
Note that these results only reflect the feelings of individuals who received the algorithm condition.

#### Group Level Results

```{r}
#| histcred,
#| fig.cap= "Histogram of perceived credibility of experts",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algtest_table<-table(merged_results$algtestcred, merged_results$conclusion)
alg_df<-data.frame(algtest_table, question="algorithm")
colnames(alg_df)[2] <- "conclusion"
firetest_table<-table(merged_results[merged_results$algorithm=="Yes",]$firetestcred,merged_results[merged_results$algorithm=="Yes",]$conclusion)
fire_df<- data.frame(firetest_table, question="firearm")
colnames(fire_df)[2] <- "conclusion"
test_df<-rbind(alg_df, fire_df)

levels(test_df$Var1) <- gsub(" ", "\n", levels(test_df$Var1))

test_df$Var1=factor(test_df$Var1, levels = c("Extremely\nnoncredible","Moderately\nnoncredible","Weakly\nnoncredible",
                                                         "Neither\ncredible\nnor\nnoncredible", "Weakly\ncredible", "Moderately\ncredible",
                                                         "Extremely\ncredible"))

ggplot(test_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How credible did you find the testimony of the ___ expert?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  purrr::partial(label_both, sep = ":\n")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

In terms of credibility, as shown in Figure \@ref(fig:histcred), the algorithm and the expert scored fairly similarly, with the algorithm expert resulting in slightly more individuals selecting "Extremely credible" than the firearms expert. 
As mentioned before, most participants limited their selection to the two highest categories - "Moderately credible" and "Extremely credible".

```{r}
#| histrel,
#| fig.cap= "Histogram of perceived reliability of evidence",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algrel_table<-table(merged_results$algrel, merged_results$conclusion)
algrel_df<-data.frame(algrel_table, question="algorithm")
colnames(algrel_df)[2] <- "conclusion"
examrel_table<-table(merged_results[merged_results$algorithm=="Yes",]$examrel, merged_results[merged_results$algorithm=="Yes",]$conclusion)
examrel_df<- data.frame(examrel_table, question="examiner")
colnames(examrel_df)[2] <- "conclusion"

rel_df<-rbind(algrel_df, examrel_df)



levels(rel_df$Var1) <- gsub(" ", "\n", levels(rel_df$Var1))

rel_df$Var1=factor(rel_df$Var1, levels = c("Extremely\nunreliable","Moderately\nunreliable","Weakly\nunreliable",
                                            "Neither\nreliable\nnor\nunreliable", "Weakly\nreliable", "Moderately\nreliable",
                                            "Extremely\nreliable"))

ggplot(rel_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How reliable do you think the ____ evidence is, in this case?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  purrr::partial(label_both, sep = ":\n")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```

For reliability, shown in Figure \@ref(fig:histrel), a similar number of individuals selected "Extremely reliable" for both the algorithm evidence and the examiner's comparison, with a larger proportion of individuals selecting "Extremely reliable" in the match condition when the algorithm is present. 
There appears to be, however, a difference in the categories of "Moderately reliable" and "Weakly reliable". 
Individuals were more likely to select that the algorithm was "Weakly reliable", while they were less likely to select that the algorithm was "Moderately reliable", compared to the examiner.

```{r}
#| histsci,
#| fig.cap= "Histogram of perceived scientificity of evidence",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algsci_table<-table(merged_results$algsci, merged_results$conclusion)
algsci_df<-data.frame(algsci_table, question="algorithm")
colnames(algsci_df)[2] <- "conclusion"
examsci_table<-table(merged_results[merged_results$algorithm=="Yes",]$examsci,merged_results[merged_results$algorithm=="Yes",]$conclusion)
examsci_df<- data.frame(examsci_table, question="examiner")
colnames(examsci_df)[2] <- "conclusion"

sci_df<-rbind(algsci_df, examsci_df)

levels(sci_df$Var1) <- gsub(" ", "\n", levels(sci_df$Var1))

sci_df$Var1=factor(sci_df$Var1, levels = c("Extremely\nunscientific","Moderately\nunscientific","Weakly\nunscientific",
                                         "Neither\nscientific\nnor\nunscientific", "Weakly\nscientific", "Moderately\nscientific",
                                          "Extremely\nscientific"))

ggplot(sci_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("How scientific do you think the ____ evidence is, in this case?")+
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  purrr::partial(label_both, sep = ":\n")) +
  theme_bw()+
  theme(axis.title.x = element_blank())


```

Individuals generally saw the algorithm as more scientific than the expert, as shown in Figure \@ref(fig:histsci). 
They were more likely to select "Extremely scientific" in the case of the algorithm when compared to the expert. 
Accordingly, they were less likely to select "Moderately scientific" in the case of the algorithm when compared to the examiner. 
No real difference can be seen in the lower values on the scale, due to few individuals selecting these categories.

```{r}
#| histunder,
#| fig.cap= "Histogram of participants\' understanding",
#| fig.width= 7,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algunder_table<-table(merged_results$algunder, merged_results$conclusion)
algunder_df<-data.frame(algunder_table, question="algorithm")
colnames(algunder_df)[2] <- "conclusion"
expunder_table<-table(merged_results[merged_results$algorithm=="Yes",]$expunder,merged_results[merged_results$algorithm=="Yes",]$conclusion)
expunder_df<- data.frame(expunder_table, question="examiner")
colnames(expunder_df)[2] <- "conclusion"

under_df<-rbind(algunder_df, expunder_df)

levels(under_df$Var1) <-
  gsub("<br/>", "", levels(under_df$Var1))

levels(under_df$Var1) <-
  gsub("  ", " ", levels(under_df$Var1))


levels(under_df$Var1) <- gsub(" ", "\n", levels(under_df$Var1))

ggplot(under_df, aes(Var1, Freq, fill=question))+
  geom_bar(stat="identity", position=position_dodge(preserve = "single"), color="black")+
  ggtitle("Based on this testimony, how would you rate your understanding of the method
                  described for the ______?") +
  scale_fill_manual(values = c("gray", "black")) +
  facet_grid(conclusion~., labeller =  purrr::partial(label_both, sep = ":\n")) +
  theme_bw()+
  theme(axis.title.x = element_blank())+ 
  scale_x_discrete(labels=c('1\nI\nunderstood\nnothing', '2', '3', '4', '5\nI\nunderstood\neverything'))


```
Individuals rated their understanding of the examiner's bullet comparison higher than their understanding of the algorithm method, generally speaking (\@ref(fig:histunder)). 
Participants were more likely to select a 4 or 5 with regards to their understanding of the examiner's comparison, while they were more likely to select a 3 or 4 with regards to their understanding of the algorithm method.
While these questions gauge how well participants believe they understand the given concepts, @dunningFlawedSelfAssessmentImplications2004 indicate that individuals are not unbiased judges of their own understanding.


#### Individual Level Results

```{r}
#| coordcred,
#| fig.cap= "Plots of understanding and perceived expert credibility",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

algtestcred_reorder<- algorithm_results %>% mutate(algtestcred=fct_relevel(algtestcred, "Extremely noncredible","Moderately noncredible",
                                                                           "Weakly noncredible",
                                                         "Neither credible nor noncredible", 
                                                         "Weakly credible", "Moderately credible",
                                                         "Extremely credible"))

algtestcred_reorder<- algorithm_results %>% mutate(firetestcred=fct_relevel(firetestcred, "Extremely noncredible","Moderately noncredible",
                                                                           "Weakly noncredible",
                                                         "Neither credible nor noncredible", 
                                                         "Weakly credible", "Moderately credible",
                                                         "Extremely credible"))


cred_data<-data.frame(algtestcred=algtestcred_reorder$algtestcred, firetestcred=algtestcred_reorder$firetestcred)
testcred_tab<-table(cred_data)
testcred_df<-as.data.frame(testcred_tab)



credplot<-ggparcoord(data=subset(testcred_df, Freq != 0), columns=c("algtestcred","firetestcred"), order=c(1,2,3,4,5,6,7), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("How credible did you find the \n testimony of the ___ expert?")+
  facet_grid(drop=FALSE)



under_data<-data.frame(examiner=algorithm_results$expunder, algorithm=algorithm_results$algunder)
under_tab<-table(under_data)
under_df<-as.data.frame(under_tab)

under_df<-rbind(under_df, data.frame("examiner" = "2.0", "algorithm"=NA, "Freq"=NA))

levels(under_df$algorithm) <-
  gsub("<br/>", "", levels(under_df$algorithm))

levels(under_df$algorithm) <-
  gsub("  ", " ", levels(under_df$algorithm))

levels(under_df$algorithm) <- gsub(" ", "\n", levels(under_df$algorithm))


levels(under_df$examiner) <-
  gsub("<br/>", "", levels(under_df$examiner))

levels(under_df$examiner) <-
  gsub("  ", " ", levels(under_df$examiner))

levels(under_df$examiner) <- gsub(" ", "\n", levels(under_df$examiner))

under_df$examiner<- factor(under_df$examiner, 
                           levels = c("1\nI\nunderstood\nnothing","2.0","3\nI\nunderstood\nabout\nhalf\nof\nthe\nmethod", "4.0", "5\nI\nunderstood\neverything"))
under_df$algorithm<- factor(under_df$algorithm, levels=
                           c("1\nI\nunderstood\nnothing","2.0","3\nI\nunderstood\nabout\nhalf\nof\nthe\nmethod", "4.0", "5\nI\nunderstood\neverything"))



underplot<- ggparcoord(data=subset(under_df, Freq != 0), columns=c("algorithm","examiner"), order=c(1,2,3,4,5,6,7), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("Based on this testimony, how would \n you rate your understanding of the \n method described for the ______?")+
  facet_grid(drop=FALSE)


grid.arrange(credplot, underplot, ncol = 2)


```
Figure \@ref(fig:coordcred) depicts parallel coordinate plots for participants' ratings of the credibility of the experts, as well as their understanding of the methods described in the testimony. 
Higher values indicate higher scores in understanding and credibility. These plots map the frequency that individuals selected the shown combination of values for credibility or understanding. 
In the case of credibility, we can see that most individuals selected the highest category of credibility for both the algorithm and the firearms expert. 
In the case of understanding, individuals tended to select the two highest categories the most. 
It can also be seen that there is a trend of individuals selecting a category lower for the algorithm compared to what they selected for the examiner by the thicker downward line between categories 4 and 5, as well as between categories 4 and 3. 
This corresponds to Figure \@ref(fig:histunder) in that individuals tend to give higher understanding ratings to the examiner's bullet comparison method than they did for the algorithm.

```{r}
#| coordscirel,
#| fig.cap= "Plots of perceived scientificity and reliability of methods",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

sci_reorder<- algorithm_results %>% mutate(algsci=fct_relevel(algsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))

sci_reorder<- algorithm_results %>% mutate(examsci=fct_relevel(examsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))

sci_data<-data.frame(examiner=sci_reorder$examsci, algorithm=sci_reorder$algsci)
sci_tab<-table(sci_data)
sci_df<-as.data.frame(sci_tab)

levels(sci_df$algorithm) <- gsub(" ", "\n", levels(sci_df$algorithm))


sciplot<- ggparcoord(data=subset(sci_df, Freq != 0), columns=c("algorithm","examiner"), order=c(1,2,3,4,5,6,7), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("How scientific do you think the ____ \n evidence is, in this case?")+
  facet_grid(drop=FALSE)




algorithm_results<- rbind.fill(algorithm_results, data.frame("examrel"="Extremely unreliable"))

rel_reorder<- algorithm_results %>% mutate(algrel=fct_relevel(algrel, "Extremely unreliable","Moderately unreliable","Weakly unreliable", "Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

rel_reorder<- algorithm_results %>% mutate(examrel=fct_relevel(examrel, "Extremely unreliable", "Moderately unreliable","Weakly unreliable","Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

rel_data<-data.frame(examiner=rel_reorder$examrel, algorithm=rel_reorder$algrel)
rel_tab<-table(rel_data)
rel_df<-as.data.frame(rel_tab)
rel_df$algorithm <- factor(rel_df$algorithm, levels=
                            c( "Extremely unreliable", "Moderately unreliable","Weakly unreliable","Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

relplot<- ggparcoord(data=subset(rel_df, Freq != 0), columns=c("algorithm","examiner"), order=c(1,2,3,4,5,6,7), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("How reliable do you think the ____ \n evidence is, in this case?")+
  facet_grid(drop=FALSE)


grid.arrange(sciplot, relplot, ncol = 2)


```

Figure \@ref(fig:coordscirel) demonstrates the trends for selection for how scientific and reliable individuals felt the evidence was. 
In terms of how scientific individuals felt the evidence was, it appears that most participants selected the highest category for both the algorithm and the examiner. 
Some participants selected the second highest category for the examiner, and the highest category for the algorithm.
Others selected the second highest category in both cases.
In terms of reliability, participants tended to choose one of the two highest categories for both the algorithm and the examiner. 
Some participants switched between the two highest categories for the algorithm or the examiner, in similar numbers.  

```{r}
#| allpcp,
#| echo= FALSE,
#| warning= FALSE,
#| message= FALSE,
#| fig.width= 8,
#| fig.height= 9,
#| fig.cap = "Parallel Coordinate Plot for reliability, credibility, scientificity, and understanding"

algtestcred_reorder <- algorithm_results %>% mutate(conclusion = fct_relevel(conclusion, "NoMatch", "Inconclusive", "Match"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(algrel=fct_relevel(algrel, "Extremely unreliable","Moderately unreliable","Weakly unreliable", "Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(examrel=fct_relevel(examrel, "Extremely unreliable", "Moderately unreliable","Weakly unreliable","Neither reliable nor unreliable", "Weakly reliable", "Moderately reliable",
                                            "Extremely reliable"))

algtestcred_reorder <- algtestcred_reorder %>% mutate(examsci=fct_relevel(examsci, "Extremely unscientific","Moderately unscientific","Weakly unscientific",
                                           "Neither scientific nor unscientific", "Weakly scientific", "Moderately scientific",
                                           "Extremely scientific"))


cred_data2 <- data.frame(
  algtestcred = algtestcred_reorder$algtestcred, 
  firetestcred = algtestcred_reorder$firetestcred, 
  conclusion = algtestcred_reorder$conclusion,
  examrel = algtestcred_reorder$examrel,
  algrel = algtestcred_reorder$algrel,
  algunder = algtestcred_reorder$algunder,
  expunder = algtestcred_reorder$expunder,
  algsci = algtestcred_reorder$algsci,
  examsci = algtestcred_reorder$examsci
)

testcred_tab2 <- table(cred_data2)
testcred_df2 <- as.data.frame(testcred_tab2)

testcred_df2 <- testcred_df2 %>%
  purrr::map(.f = function(x) rep(x, testcred_df2$Freq)) %>%
  as.data.frame() %>%
  select(-Freq)


testcred_df2 %>%
  pcp_select(algunder, algsci, algtestcred, algrel, examrel, firetestcred, examsci, expunder) %>%
  pcp_scale() %>%
  pcp_arrange() %>%
  ggplot(aes_pcp()) +
  scale_y_continuous(labels=c("low scores", "", "", "", "high scores"),
                     breaks=c(0,0.25, .5,.75, 1)) +
  geom_pcp(aes(color=conclusion), alpha = 0.75) +
  scale_colour_manual(values = c("#FF8E00", "grey20", "#037AC7")) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  geom_pcp_boxes(fill = "transparent", color = "black", linewidth = 1)+
  theme_bw()


```

Figure \@ref(fig:allpcp) shows individual responses across all of the considered questions. 
There were some individuals who selected the highest category for responses across the board, while others selected the highest response for all except their understanding of the algorithm. 
In fact, algorithmic understanding seems to have the least consistent responses, based on the differences in the lines between algorithmic understanding and algorithmic scientificity. 
These coordinate plots indicate that individuals tended to choose the same categories for the algorithm and the examiner across variables, with most variation between categories resulting from individuals moving up or down a single category. 
Relatively few individuals changed their response by more than one category, when comparing between the algorithm and the expert. 
These results generally reflect the trends shown in the histograms in the previous section.

## Discussion

### Summary of Results

As can be seen in most graphics, scale compression was an issue with this study. 
The vast majority of participants selected the two highest categories for questions rating how credible, reliable, and scientific the expert/method was. 
This corresponds with @garrett2013's study of fingerprint match language, where they did not find a significant difference in the participants' feelings of guilt based on various match language.
@garrett2013 hypothesized that this may relate to strong feelings reliability for fingerprint evidence, resulting in individuals viewing any match as automatically reliable (without the need for stronger language).
\authorcol{In future studies, we will be evaluating different methods of response, aside from Likert scales, to study other formats that may yield less compressed results.}
People generally seemed to have less faith in inconclusive decisions. 
They understood the examiner's comparison better than the algorithm comparison, which was expected given the statistical complexity of the algorithm procedure. 
In general, participants also found the algorithm to be more scientific than the examiner, when presented with both methods. 
The presence of images did not have a discernible effect in terms of credibility, reliability, or scientificity. 
Participants' views of the credibility of the experts and the frequency with which firearms examiners make mistakes did not depend on the algorithm, images, or conclusion.  

### Limitations

Some of the major limitations for this project relate to the format and distribution of the survey material: the pool of participants, the format of the testimony, the limited testimony, and the inability to deliberate. 
These limitations are also mentioned by @garrettMockJurorsEvaluation2020.

Participants were limited to those who participate in online survey-taking websites. 
These participants may not be representative of the US population, due to differences in computer access or use, occupation, or other such factors. 
These factors may have an effect on study responses.
\authorcol{Another issue in representation results from the process of jury selection.}
\authorcol{Because individuals are not randomly approved for serving on a jury, the jury itself is unlikely to be composed of a representative sample of American citizens.}
@abramsonJurySelectionWeeds2018 \authorcol{argue that selection for jury duty does not result in a representative sample due to some courts' reliance solely on voter registration records for contacting eligible jurors, as well as large non response and non deliverable issues that are more prevalent in African American and Hispanic communities when compared to non-Hispanic White communities.}

Testimony was presented in a written format, which is unlike the courtroom setting of spoken testimony, where the jurors can see the experts.
However, the use of written testimony allowed for the use of gender-neutral names for the experts (Terry and Adrian), which may be more difficult to achieve in a courtroom or video setting. 
Potential jurors may also develop views regarding the reliability/credibility of the examiner that are dependent on external factors - such as appearance or speech - rather than based on the testimony itself.

Testimony in this case was limited to only include the firearms evidence. 
This led to some confusion on the part of the "inconclusive" and "not a match" scenarios, where there did not appear to be relevant evidence for the prosecution. 
While the goal of this format was to ensure that participants focused on the bullet matching testimony (without the compounding influence of other witnesses or evidence), this would not be representative of courtroom testimony.  

In a courtroom setting, jurors are able to deliberate with each other before reaching a conclusion with regards to the case. 
These deliberations tend to be evidence-driven, as opposed to majority rule [@bornsteinJuryDecisionMaking2011, p. 65]. 
While the majority decision may be reflected in the final result, some studies suggest that juries tend toward leniency when there is not a definitive majority in a criminal trial [@maccounAsymmetricInfluenceMock1988]. 
The act of deliberation may result in different evaluations for Likert scale questions of reliability/credibility or strength of evidence than individual thought, even if a difference in guilty verdict rates are not found after deliberation for this simple case style.  

There were several typos that were not corrected for approximately the first half of the participants. 
For all scenarios, the firearms examiner was referred to as Alex Smith in the questions, whereas the name was Terry Smith throughout the testimony. 
\authorcol{Several participants noted this discrepancy in their feedback on the survey, allowing us to fix the typo before all surveys were completed.}
\authorcol{There did not appear to be confusion due to the typo on the part of the participants.}
"Convenience" was also misspelled in one question. 
For exclusion testimonies, the name "Alex Smith" also occurred in the cross examination.
In the case of non-algorithm inconclusive testimonies, the question: "Can you describe the process of obtaining these test fired bullets?" was missing, but the response: "The test-fired bullets came from a test fire of the gun recovered from the traffic stop." remained unchanged. 
Due to the nature of the online survey, any differences caused by these typos would be confounded with demographics as well as date or time.

### Future Research

One of the most notable features of the histograms presented in this research is the overwhelming proportion of respondents that selected the two highest categories, whether it be in terms of reliability, credibility, or scientificity. 
The concentration of responses in the two highest categories may obscure potential differences in treatments, simply due to the issue of overall trust in the system. 
It may also effect how well ordered logistic regression models fit the data. 
This effect may be diminished through the use of jury instruction, and not referring to the firearms examiner or the algorithm witness as experts (as suggested by @OpinionEvidence).

Efforts are also being made to streamline the testimony into a single document, to prevent confounding typos.
Because the written court testimony may be difficult to follow and may give witnesses an air of impartiality, future studies will include images for relevant actors, and color coded speech bubbles to clarify which side the witness is speaking for.
We plan to develop a tool that can be used in testing courtroom scenarios, with versatile images for a variety of situations. These proposed changes can be seen in Appendix \@ref(study-2-changes).

An additional response to the study that was recorded is participants' note sheets. 
Many participants copied and pasted portions of the testimony into their note sheets for later reference.
We plan to evaluate these note sheets, and develop a method for presenting the written notes alongside the testimony in order to produce a 'heat map' of the text that participants found relevant.











<!--chapter:end:01-chap1.Rmd-->

# Text analysis with Transcripts {#textcolor}

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(knitr)
library(tidyverse)
library(stringdist)
library(dplyr)
#library(kableExtra)
library(ggplot2)
library(readtext)
require(quanteda)
require(quanteda.textstats)
require(quanteda.corpora)
library(purrr)
library(ggrepel)
library(stringi)
#library(PTXQC)
library(DiagrammeR)
library(fuzzyjoin)

```

## Introduction

When conducting studies that rely on participants reading a longer document, researchers may be interested in determining what portions of the document participants find worth noting.
This could provide useful information about areas of interest by creating a type of heat map for the document.
When the study document consists of multiple pages and notes are recorded sequentially, the problem then becomes twofold: first the participants' notes must be cleaned so that the recorded text corresponds to the current page, then a method must be developed to match the frequency of the participants' notes to the study document.

In order to clean the notes, two different methods are considered and then combined: the First n Character method and the Longest Common Substring method.
The First n Character method relies on the concept of edit distance for matching and removing previous notes from sequential study documents.
This method was developed specifically for this document comparison.
The Longest Common Substring method, on the other hand, searches for common text between two sequential pages to be removed.
After the notes are cleaned, sequences of 5 words (collocations) are used to find areas of the testimony that participants focus on.
For indirect matches, such as typos, weights are applied to these fuzzy matches to contribute to the total count. 

This method was applied to notes taken from a recent study.
Dr. Vanderplas and I conducted a study on jury perception, in which participants were asked to read over a testimony transcript and answer some questions related to the scenarios. 
Participants were supplied with a notepad in order to take notes throughout the testimony. 
We are interested in comparing this notepad to the given testimony, in order to determine which portions of the testimony the participants found to be worth recording the most.
Portions of the testimony that bear the most similarity to the collective notes will be more highlighted than testimony that appears less frequently in the collective notes.
The method outlined above resulted in scenario testimonies that indicate where participants copied notes.

## Background

For data cleaning, a previous page's notes are matched to the next page's notes in order to remove duplicate text.
This can be thought of as a form of pattern matching.
There are plenty of examples of pattern matching in text analysis - such as @baeza-yates and @landau1988. 
These papers rely on a set pattern to be found in the reference text, with a defined number of differences allowed between the pattern and the text. 
According th @baeza-yates, many algorithmic methods have been developed to solve this problem, with allowances for mismatches between the reference text and the pattern.
A similar allowance for differences is present in @landau1988, where differences are defined as either insertions, deletions, or substitutions.
This definition of differences is the same definition that is used in computing Levenshtein distance, also known as edit distance (@levenshtein).
Levenshtein considered the issue of insertions, deletions, and substitutions with binary code.
This concept has later been extended to include more extensive strings, as demonstrated by @konstantinidis2005.
The edit distance can be used to determine the extent of matching text when comparing two sequential note sheets, in order to identify if a portion of notes should be removed.

One thing that differentiates this problem from other text analysis methods is that there is no set in stone pattern - while the analysis is based on the previous page of notes, individuals do not always keep the previous page of notes the same - some delete parts, and some add new information in the middle. 
Thus, the goal is to not only find and remove only exact matches of the entire previous note sheet, but to also remove previous notes - whether they be shorter or discontinuous.
While evaluating edit distance is useful when finding direct matches, the Longest Common Substring (LCS) method can be used to find pieces of notes that match between pages, so that this repeated text can be removed.

@landau1988's discussion of pattern matching with k differences includes both a dynamic programming approach, as well as the construction of suffix trees.
These methods correspond to those proposed for finding the Longest Common Substring between two sequences.
This problem has been used to compare biological sequences (@crochemore2017).
An early solution for finding the longest common substring involves the construction of a suffix tree (@charalampopoulos2021).
The suffix tree method is described by @gusfield1997; in short, each unique suffix of a string would contribute a new branch to the suffix tree, where leafs consist of the string's terminal character: \$ is used avoid recurrence with previous characters in the string.
While I have located a package on Github that utilizes the suffix tree for LCS (@OmegahatRlibstreeSuffix), I have been unable to successfully download the package.
Another solution implemented by @ptxqc in a CRAN R package uses dynamic programming, which involves the construction of a matrix that records the length of a matching string at character $i$ for string 1 (in the $i$th row), and character $j$ for string 2 (in the $j$th column).
Because the dynamic programming method has an R implementation, it is utilized for this analysis.
The comparison between suffix tree and dynamic programming methods will be evaluated in a future analysis.

Once the participants' notes are cleaned, they must be compared to the study transcript.
In the realm of non-fixed pattern matching, there are plagiarism recognition methods (such as TurnItIn).
In these cases, a portion of a student's text is cross referenced with published or online references, in order to determine if the text was copied.
Because this type of correspondence is on a larger level than individual words, collocation analysis is used.
While collocations traditionally refer to words that occur more frequently together - such as "white house"(@DefinitionCOLLOCATION2023), tools for collocation analysis helpfully provide the frequency of occurrence for strings of a specified length (@collocationpage). 
In a reference to algorithmic detection of plagiarism in computer programs, @parker1989 describe an algorithm that uses the character differences in order to compare how similar programs are.
Other algorithms they discuss use code-specific features such as number of operators, lines of comments, and number of various loop statements.
Because we are only concerned with the content of the notes, there is little information to be gained from formatting, as there is in plagiarism detection in programs.
However, character differences can effectively be used to indicate similarity between participants' notes and the study testimony.
This is useful in situation where individuals do not copy the notes directly.
In these cases, fuzzy matching can be used to find the testimony collocation that is the most similar to the participants' written notes.
This approximate string matching allows for matching strings that do not correspond directly (@gusfield1997), which would allow for matching up participants' inexact notes with the closest testimony collocation.

## Methods

### Data Cleaning

Table \@ref(tab:noteexample) is an example of what sequential notes may look like, taken from the second and third pages of a study participant's notes. 
Because participants are provided with the same continuous notepad throughout the study, the database records each page as a "screenshot" of what is written on their notes when they advance to the next page - this provides cumulative notes.

```{r noteexample, echo=FALSE, message=FALSE, warning=FALSE}
comments <- read.csv("data/clean_notes.csv")
# length(unique(subset(comments, notes != "")$clean_prints))
# val_prints <- sample(unique(subset(comments, notes != "")$clean_prints), size=30, replace=FALSE)
# 
# val_prints <- c(val_prints, 503, 359, 39, 368, 547)
# 
# val_dataset <- comments %>% subset(clean_prints %in% val_prints)
# 
# write.csv(val_dataset, "validation_dataset.csv")
# dim(val_dataset)

test_comments <-comments[1:4,]

kable(test_comments %>% filter(page_count==2 | page_count==3) %>% dplyr::select(page_count, notes), caption="Participant Note Example") 
# %>% column_spec(2, width = "30em")
```

In order to appropriately clean the notes, the duplicated text ("Richard Cole - has been charted with willfully discharging a firearm in a place of business. This crime is a felony.") needs to be removed from the third page, to leave only notes that correspond to the third page's testimony.

#### First n Character (FNC) Method

In scenarios such as the table above, a straightforward method for note cleaning would be to measure the length of the previous page's notes (n), and compare the previous notes to the first n characters in the current pages notes via edit distance. 
The edit distance is the number of characters that would need to be changed in order to transform the first string into the second string.
This includes insertions, deletions, and substitutions(in the case of the "adist" function from the R Core Team).
For example, "Hat" and "Hot" would have an edit distance of 1, because the strings match if the "a" is turned into an "o".
Similarly, "Over there" and "there" would have an edit distance of 5, since 5 characters are deleted (the letters in "Over" plus an additional space).
If the edit distance is small (indicating a correspondence to the previous page's notes), the first n characters can be removed.
If we consider the table is above, it is clear to see how this method can be applied.
The page 2 notes perfectly line up with the first two lines in the page 3 notes, resulting in an edit distance of 0 when comparing the first n characters of the third page.
Thus, by removing this beginning section, we would be left with the notes that are unique to page 3 - namely, the last two lines.

This method works well when participants take notes sequentially.
In some instances, however, participants will delete portions of their previous notes, add new notes either before or in the middle of their old notes, or duplicate their old notes.
When participants delete a portion of their notes or add new notes before/in the middle of their old notes, the edit distance between the old and new notes could be large.
The first n character method would not be able to appropriately clean these notes.

#### Longest Common Substring (LCS) Method

Another method for matching text between two different sets of notes is the Longest Common Substring (LCS).
This method searches two strings for the longest sequence of sequential characters that they have in common, which is then returned. 
For example, in Figure \@ref(fig:lcs), the LCS between the two strings is "the cat enjoys napping".
In the process of note cleaning, this would be the string that is removed from the second page of notes, resulting in "When it is quiet" as the cleaned (non-repeated) notes.

```{r}
#| lcs,
#| fig.cap= "Longest Common Substring Diagram",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/svg_graph.png")
```

The PTXQC package (@ptxqc) implements LCS using dynamic programming, as opposed to the suffix tree method (which has not been implemented in CRAN).
Their dynamic programming process is described below.
This method creates an empty matrix, where the number of rows correspond to the length of the first string, and the number of columns correspond to the length of the second string.
Thus, the first cell would correspond to the first character of both strings, and so on.
If the characters match and it is the first row or column, then the cell gets a value of 1.
If the characters match and it is not the first row or column, the the cell receives the value of the (i-1,j-1) cell plus one.
The first string's index for the substring of the longest length is saved.
This process is shown in Table \@ref(tab:dylcs), with two strings: "cabbacced" and "bbacccade".
The diagonal of numbers in red indicate the longest substring, beginning with 'b' and ending with 'c'
Thus, "bbacc" is identified as the LCS.

_ | c | a | b | b | a | c | c | e | d
--|---|---|---|---|---|---|---|---|---
b | 0 | 0 | \textcolor{red}{1} | 1 | 0 | 0 | 0 | 0 | 0
b | 0 | 0 | 1 | \textcolor{red}{2} | 0 | 0 | 0 | 0 | 0
a | 0 | 1 | 0 | 0 | \textcolor{red}{3} | 0 | 0 | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | \textcolor{red}{4} | 1 | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | 1 |\textcolor{red}{5} | 0 | 0
c | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 0 | 0
a | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0
d | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1
e | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0

: Dynamic Programming for LCS (\#tab:dylcs)

The Longest Common Substring method is able to handle the issues found in the First n Character method, which is demonstrated in Table \@ref(tab:issues).
If notes are deleted, the LCS method can identify the substring from the previous notes, despite the large edit distance that would prevent removal in the First n Character method.
If new notes are inserted in the middle of previous notes, the First n Character method would compare the Page 1's notes with the red text of the Page 2 notes shown in the second row, resulting in an edit distance that would prevent removal.
With the LCS method, the first substring "The cat ran" could be identified and removed, then in a second iteration, the second substring of "up the tree" can also be removed, leaving only the new text.
In the case of duplication, the First n Character method would be able to identify the first iteration of text that matches Page 1 and remove it; but it would not be able to identify the second occurrence of the text.
As before, the LCS method would be able to identify both instances through two iterations, and remove all Page 1 text.

Issue | Page 1 | Page 2 | LCS | Edit Distance
--------|--------|---------|---------|---------
Deletion | The cat ran up the tree | The cat ran | The cat ran | 12
Insertion | The cat ran up the tree | \textcolor{red}{The cat ran, chased by a} dog, up the tree | (The cat ran)(up the tree) | 11
Duplication | The cat ran up the tree | \textcolor{red}{The cat ran up the tree} The cat ran up the tree | (The cat ran up the tree)(The cat ran up the tree) | 0

: LCS Comparison for FNC Issues (\#tab:issues)

While the LCS method is able to solve the issues related to the First N Character method, it has some issues of its own.
If the testimony itself contains duplicate text, such as the text related to swearing in witnesses, the LCS method would remove the new text that corresponds to the later testimony.
The other issue with the LCS method is the time required when it is applied to the entire testimony.
In the validation study described below, the LCS method took approximately 30 times as long as the FNC method.

#### Hybrid Method

In order to compare the two methods described above, I cleaned a subset of the survey response notes to have a "correct" baseline with which to calculate error rates.
This dataset consists of 561 pages of 'notes' (including blanks) with 35 unique participants. 
30 participants were randomly selected from the larger dataset, while 5 participants were selected based on demonstrated issues with data cleaning. 
The 5 selected participants included an individual who deleted almost all of the beginning notes; two individuals who pasted new notes in the middle of old notes; an individual who duplicated notes; and an individual who included the algorithm expert's trial confirmation as well as the forensic scientist's - duplicate text that should be included as the new notes.

For the FNC method, the length of the previous notes is compared to the beginning of the current page's notes. 
If the edit distance is below a certain threshold, these notes are removed. 
In this case, I changed the threshold with values of 0 character difference, 5 character difference, and 15 character difference (set as less than 1, less than 6, and less than 16).
In the case of LCS, a threshold must be picked for the minimum matching substring length to be considered 'previous notes' and removed.
If the threshold is too small, the LCS method can identify words like "the" as matching previous testimony to be removed.
If the threshold is too large, the LCS method may be unable to remove substrings when new text is inserted in the middle of the old text.
Here, I tried several different character lengths: 40 characters, half the previous note length, a third of the previous note length, and a fourth of the previous note length plus 5 (to compensate for shorter strings).

I also investigated the use of a hybrid method in the hope that the FNC method could be used as a quick way to clean the simplest cases of notes, while the LCS method could be used to clean up the notes that are less clear-cut.
In order to determine which observations would need the second cleaning method, I created Figure \@ref(fig:errorplot).

```{r}
#| errorplot,
#| fig.cap= "Error Comparison",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/cleaningerror.jpg")
```


Here, the yellow circles indicate the FNC method with a 16 character threshold, while the black circles represent the LCS method with a threshold of 1/3 the previous character length.
Larger circles indicate a larger amount of error when compared to the hand-cleaned notes.
The y axis shows the edit distance computed between the current and previous notes for the FNC method, while the x axis indicates the length of the clean notes.
The horizontal line is the 16 character threshold for the FNC method.
As can be seen, most observations are located in the bottom left of the graph, and have relatively small circles.
These are notes where the FNC method was effectively applied.
In the observations above the threshold, most have a larger error when the FNC method is applied compared to the LCS method, with the exception of one observation in the top left.
In that case, the error appeared to be equal for both methods.
There is also a larger FNC method error for the observation on the bottom right of the graph.
This was a case of duplicate text, resulting in abnormally long notes.
This graph appears to show that and edit distance above the threshold of 16 for the FNC method and the length of the cleaned notes can be used as criterion for determining when the LCS method should be applied.
I chose to set a threshold of 4 standard deviations above the mean note length for cleaned notes of the current page to constitute "unusually long" notes to apply the LCS method to.
I chose 1/3 of the previous notes as the LCS threshold because the mean edit distance to the correct notes was comparable to the 1/4 + 5 character threshold, but it is more conservative for larger notes.

Methods | Error | SD | Time (Minutes)
--------|--------|---------|---------
FNC 1 Character | 498.965 | 2769.826 | 2.4 
FNC 6 Character | 201.353 | 1890.009 | 2.5 
FNC 16 Character | 36.912 | 411.252 | 2.5 
LCS 40 Character | 6.337 | 35.497 | 75.8 
LCS 1/2 Previous Notes | 7.288 | 104.692 | 85.7 
LCS 1/3 Previous Notes | 3.064 | 26.769 | 76.3 
LCS 1/4 Previous Notes + 5 | 2.823 | 26.137 | 73.6 
Hybrid (FNC 16 and LCS 1/3) | 2.135 | 24.625 | 3.5 

: Comparison of Note Cleaning Methods (\#tab:cleancompare)

The results of this study are shown in Table \@ref(tab:cleancompare).
Notably, the mean edit distance to the correct notes (or the error) is large for the FNC methods, with a high level of variation.
However, the computation time is approximately 2.5 minutes.
In the case of the LCS method, the error is much lower, but the computation time is above an hour.
The hybrid method appears to combine the benefits of both of these methods. 
The error rate is on par with that of the LCS methods, while the time is only slightly above that of the FNC methods.
The hybrid method was therefore implemented for the larger database.
The flowchart in Figure \@ref(fig:flowchart) shows how this hybrid method can be applied.

```{r}
#| flowchart,
#| fig.cap= "Hybrid Flowchart",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/flowchart.jpg")
```

### Note Analysis

#### Collocations

In order to analyze which areas of the text individuals focus on, we must first find a way to compare their notes to the written testimony.
One way to do this comparison is through collocation analysis, which uses the frequency of strings of sequentially-appearing words (n-grams) in order to identify words that tend to 'stick together'.
For example, in the case of the phrase "The cat ran up the tree", there would be three collocations of length 4: "The cat ran up", "cat ran up the", and "ran up the tree".
By matching the collocations that appear in the testimony to a frequency of the collocation from the given notepad, we would be able to get frequency values for all n-grams that occur in the testimony.

In setting the size of the collocation, there are two important factors to keep in mind: the frequency with which a phrase appears in the testimony itself, and the number of words in a row that individuals may directly copy from the testimony.
If the number of words considered in a collocation is too short, we may run into an issue of phrases occurring multiple times throughout the testimony, without having a way to distinguish which sections are of importance.
For example, if we used a collocation of length two, a common phrase in the testimony would be "Richard Cole".
Because this phrase occurs so frequently in the testimony, it does not by itself tell us if there are particular portions of the testimony that individuals find important, even if we add weights to calculate the relative frequency.
On the other hand, if we use a collocation of length 10, it is unlikely that a large amount of individuals would copy the exact 10-word phrase, which would result in fewer observations.
In considering these two issues, we decided on a collocation size of 5.
This would be long enough to avoid many common phrases such as "the bullet matching algorithm", but short enough that individuals may copy the phrase down directly.
The implementation of collocations used the 'quanteda' package, with guidance from a tutorial by Schweinberger.

N-grams are identified in the testimony, and assigned ordered word numbers (so the first n-gram of length 5 will have words 1-5, the second will have words 2-6, and so on). 
These are then bound with collocations from the notes based on the sequence of 5 words, in order to assign a frequency to each n-gram. 
The average is then taken per word out of the possible words that the n-gram could have appeared in (for example, the first word can only appear in a single n-gram, so its average will be equal to the frequency of the first collocation). 
When multiple 5-grams appear in the same testimony page, the note frequency is divided by the number of appearances in the testimony in order to account for multiple occurrences.
The table below shows the highest frequencies for the second page of the testimony: 

"In this case the defendant Richard Cole has been charged with willfully discharging a firearm in a place of business. This crime is a felony. 
Mr. Cole has pleaded not guilty to the charge. 
You will now hear a summary of the case.
This summary was prepared by an objective court clerk.
It describes all of the evidence that was presented at trial."

```{r}
#| colcount,
#| fig.cap= "Collocation Count for Page 2",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/collocationcount.jpg")
```

The highlighted collocations in Figure \@ref(fig:colcount) show all of the collocations for the word "willfully" from the testimony above.
The frequencies for 'willfully' can be averaged: $\frac{121+93+91+76+75}{5}=91.2$.
This number will correspond to the shading of the word "willfully" in the highlighted testimony of Figure \@ref(fig:highlights).

Note that the words have a gradient.
This is to ensure a smooth transition between words in order to give an overall "highlight" effect, because we are interested in the important phrases in the testimony as opposed to the importance of individual words. 

```{r}
#| highlights,
#| fig.cap= "Collocation Count for Page 2",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

include_graphics(path = "images/collocationanalysis.jpg")
```

In order to create this effect, we first implemented a graph of the testimony in 'ggplot2'(@ggplot) in order to assign color values to the frequencies.
Then we used HTML gradient boxes, where the left color is that of the previous word, while the right color is based on the frequency of the current word.
This results in a smooth transition between words.


#### Fuzzy Matching

In order to perform fuzzy matching on the indirect collocations from participants' notes, I used 'stringdist_join' from the 'fuzzyjoin' R package (@fuzzy). 
This function conducts a join with the best matching observations from the second dataset. 
For example, consider asking young children to spell their birth month.
This may result in a dataset like Table \@ref(tab:months).

Name | Month
--------|--------
Billy | Mach
Jimmy | Apil 
Frances | Mae
Gary | Febary
Steve | Octobr
Bobby | Agust
Tammy | Febrey
Greg | Agast
Benny | Jun

: Dataset of misspelled months (\#tab:months)

We may then want to match the children's written months to their correctly spelled counterparts. 
While 'stringdist_join' will give values for all matches between the two datasets, we can use group_by and slice_min to find the month that is closest to the misspelled value.

```{r fuzzymonth, echo=FALSE,message=FALSE, warning=FALSE}

months <- data.frame(month=c("January","February","March","April","May","June","July","August","September","October",
                             "November","December"))
classroom <- data.frame(month=c("Mach","Apil","Mae","Febary","Octobr","Agust","Febrey","Agast","Jun"), 
                    name=c("Billy","Jimmy","Frances","Gary","Steve","Bobby","Tammy","Greg","Benny"))

fuzzy_matches <-stringdist_join(months, classroom, 
                                  by='month', #match based on team
                                  mode='right', #use right join
                                  method = "lv", #use levenshtein distance metric
                                  max_dist=99, 
                                  distance_col='dist')%>%
    group_by(name) %>%
    slice_min(order_by=dist, n=1)

kable(fuzzy_matches, caption="Fuzzy Match by Month") # %>%
#  kable_styling(position = "center")

```

In Table \@ref(tab:fuzzymonth) above, "month.x" is the correctly spelled month, "month.y" is the children's spellings, and "dist" is the edit distance between the two words. 
This same concept is applied to the notepad dataset.
In this case, the transcript of the testimony is considered the "correct" version, and the participants' collocations that do not directly match are considered the misspellings.
This method finds the closest transcript collocation to the written collocation, with a maximum edit distance of 99 for collocations to still be considered a match.

Because the intent of the participant is more clear with simple typos (such as a single letter change) than with larger differences (such as the changing of multiple words), the count for these fuzzily matched observations are weighted based on the edit distance.
Specifically, a weight of $\frac{1}{dist+0.25}$ is used, where $dist$ represents the edit distance (between 1 and 99).
An edit distance of 0 would receive a weight of 1, since fuzzy matching need not be applied.
Another factor in the calculation is the number of matches per 'misspelled' collocation.
To return to the month example, if an individual were to spell a month "Jur", the edit distance to both "June" and "July" is 2.
It is then unclear which month the person was born in, but they most likely meant June or July.
Similarly, some collocation notes had the same edit distance for multiple transcript collocations.
In these cases, the frequency of the 'misspelled' collocation notes are split evenly between the closest transcript collocations.
This results in a total fuzzy frequency per transcript collocation of:

$$
\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i}
$$
where $x_i$ is the count for note collocation $i$, $d_i$ is the distance between the transcript collocation and the 'misspelled' collocation $i$, and $c_i$ is the number of transcript collocation matches for note collocation $i$.
This quantity is then added to the count of direct collocation matches calculated in the previous section.


#### Standardization

The testimony analyzed comes from a 2x2x3 factorial design (presence or absence of images, presence or absence of the algorithm, and the conclusion: match, inconclusive, or not a match).
Due to this design, the testimony diverges and merges in several places. 
For clarity, when the testimony corresponds, the frequency for all scenarios are considered together.
However, when the testimony diverges, each scenario is considered separately.
In order to consistently evaluate the frequencies throughout the testimonies, the final frequency count was standardized by the number of scenarios included.
This would result in collocations with a final frequency weight of:

$$
(z+\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i})\frac{1}{ks}
$$

In this case, $z$ is the count of direct matches; $\sum_{i=1}^n\frac{x_i}{(d_i+0.25)c_i}$ is the weighted fuzzy match count as defined above; $k$ is the number of times the collocation re-occurs in the transcript page itself; and $s$ is the number of scenarios included in the frequency count.

## Results

### The Data

Our study consisted of 559 uniquely recorded participants.
Of these 559 participants, 430 recorded something on the notepad.
This has resulted in 10,329 pages of notes with at least something recorded.
For individuals who clicked through the survey multiple times but submitted a single result, the notes correspond to the closest time less than their submission time for observation per page number.
Note pages that corresponded to the questions (and not the testimony) were removed; this was above page 10 in conditions without the algorithm and page 19 in conditions with the algorithm.

### Data Cleaning

On the complete dataset, the hybrid method took a total of 19.33 minutes. 
This again demonstrates the time benefit of the hybrid method over the Longest Common Substring method - it took less time to clean the complete dataset with the hybrid method than it took to evaluate the reduced dataset with the LCS method. 
The data cleaning resulted in a total of 2371 note pages with some text recorded.
This is much smaller than the uncleaned total of 10,329 note pages, demonstrating a significant reduction in the note size (as is expected when removing the previous page's notes).

```{r}
#| notecount,
#| fig.cap= "Number of Notes Taken Per Page",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE
#| 
include_graphics(path = "images/freq_barchart.jpg")
```

Figure \@ref(fig:notecount) consists of the number of participants who took notes based on the page number.
In both the algorithm and the non-algorithm scenario, more individuals took notes on the first two pages of the testimony compared to the rest of the testimony.
However, the plot shows that individuals took notes throughout the testimony.

### Final Output
This final note analysis, which includes fuzzy matching, is attached as "Collocation_Comment_Analysis.html".
In visually comparing the highlighted transcripts for fuzzy matches to those without fuzzy matching, the results appear to be fairly similar. 
While the fuzzy matching did increase the frequency number, the overall trends remained the same for the dataset.
This can be seen in Table \@ref(tab:nonfuzzycount).
The ranking of the collocations remains fairly consistent, aside from differences that can be seen in collocations that had the same non-fuzzy count.

collocation | nonfuzzy count | fuzzy count
------------|----------------|------------
discharging a firearm in a | 134 | 151.19
firearm in a place of | 133 | 139.43 
a firearm in a place | 130 | 137.01 
in a place of business | 129 | 136.70 
willfully discharging a firearm in | 121 | 136.33 
with willfully discharging a firearm | 93 | 107.52 
charged with willfully discharging a | 91 | 101.56 
been charged with willfully discharging | 76 | 83.24 
has been charged with willfully | 75 | 78.75 
cole has been charged with | 74 | 76.12 
a place of business this | 74 | 88.34 

: Table of Collocation Frequencies (\#tab:nonfuzzycount)

The standardized frequency values remain fairly consistent throughout the testimony.
Some areas that individuals focused on were the crime and its extent (in terms of injury), the qualifications/process of the firearms examiner, and the results from the analysis.
While this type of analysis may allow us to determine the areas that participants copied into their notes, it does not indicate why the participants found this testimony worth copying.
This may either indicate areas that participants saw as interesting or confusing, or a mixture of both.
The functions for data cleaning and collocation plotting are included in "Comment_Functions.R", and the script for creating the final note analysis is "Collocation Comment Analysis.qmd".

## Discussion

### Conclusion

Overall, this process appears to effectively clean and display notes with the desired highlight format.
For sequential notes, such as this case, the First N Character method manages to clean the notes in most cases.
In the cases where the FNC method is ineffective (when the edit distance is above the threshold or the clean notes are unusually long), the Longest Common Substring method can be applied.
This process was demonstrated in a dataset of 35 participants, and showed a significant note reduction on the full dataset.

After the notes are cleaned, collocations can be used to determine what phrases participants decided to copy into their notes.
The length of the collocation was chosen to be long enough to avoid commonly repeated phrases, but short enough that participants may directly copy the phrase into their notes.
Notes that were not directly copied were included with weights proportional to edit distance through the use of fuzzy matching.
The final output qualitatively indicates which portions of testimony participants find relevant to copy.

### Future Research

In the future, I also want to find an implementation of suffix trees to more effectively implement the LCS method.
Once these data cleaning methods have been streamlined, I plan to create R packages for both the cleaning of sequential notes and for the visualization of transcript notes.
Another application of this type of analysis may be used in the evaluation of student note taking.

## Notes
- Participants of the first study were allowed to take notes while reading through the transcripts, which are saved by page number
- We would like to use this to highlight which parts of the testimonies the participants found to be important

- First attempt:
  - ggplot: allows for both a gradient and transparency to be added to words based on their frequency
  - graphing words of the transcript resulted in uneven spacing
  - Relative frequency was computed based on the number of times the word appeared on the page
- Second attempt:
  - Using CSS instead of ggplot: can set gradient through color outputted by ggplot
  - Words can be equally spaced
  <!-- - Currently assigning a new color to each word - this is not efficient -->
  <!-- - Now in CSS for tab format -->
  - Labels added with gradient scale
  - Change from relative frequency of individual words to average collocation frequency per word (with collocations of length 5)
     -Include fuzzy matching for indirect matches (typos/skipped words)
       

<!--chapter:end:02-chap2.Rmd-->

# Jury Perception Revisited: This Time with Caricatures {#study2}

## Background


### Study 1 and Scale Compression

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales in this scenario, as well as the use of a transcript testimony format.

Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a micro study in order to compare various response types, in order to determine substitute questions that may be more sensitive to scenario differences.
Additional changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner and algorithm developer as they would any other witness.
Unfortunately, the jury instructions were not present in the initial microstudy.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well.
The transcript for the second study can be found in Appendix \@ref(study-2-changes).

Some participants left confused comments with regards to who the witnesses were testifying for.
The transcript format may lend an air of impartiality to the witnesses, when they are in fact testifying for a specific side in the case.
Also, the format of the testimony transcripts does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by a participant.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, one master file is used with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text.
Unique identification is used throughout the database files instead of relying on unique fingerprints, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study, as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a study in order to determine how these figures are perceived.

## Micro Study on Response Type

To investigate a which response type may be most informative for this study and to troubleshoot the use of figures and speech bubble format, we conducted a smaller micro study using various response types.

### Methods

#### Study Format

Participants were presented with the same scenario described in Study 1.
In this case, however, the only factor that was changed was the conclusion of the firearms examiner: either a match or not a match.
In all conditions, the algorithm was absent and there were no images.
The testimony largely followed that of Study 1, aside from differences in the format and additional testimony described above.
In this case, it was also explicitly state that the testimony did not reflect all evidence presented in the case, because in the non match condition there may not be enough evidence to bring the case to trial.

At the end of the testimony, the participants were asked a variety of questions regarding the strength of evidence in the case.
Questions of probability, strength of evidence, and decision to convict were also asked in Study 1.
They were asked to evaluate the probability that the defendant committed the crime, both with a visible probability scale (allowing the participants to select the exact probability value) and with a non-visible scale (only the numbers of 0 and 100 were available on the extremes of the scale).
The strength of evidence was a Likert scale with values from "Not at all strong" to "Extremely strong" with nine points.
In terms of conviction, participants were reminded of the decision criterion of "beyond a reasonable doubt" when making a decision in a criminal trial, and asked if they would choose to convict.

New questions for this micro study asked for individuals to give their opinion of the guilt of the defendant, as well as assessing the chances that the defendant committed the crime, how much they would be willing to bet that the defendant was either innocent or guilty, and their opinion of the defendant.
In addition to a question regarding whether or not the participant would choose to convict, we also asked the participants to give their personal opinion on the guilt of the defendant.
This provides a second threshold for assessing the strength of evidence, aside from the "beyond a reasonable doubt" standard.
Two different questions were asked with regards to the chances that the defendant committed the crime. 
One was in a multiple choice format, with extreme values of "Impossible to be guilty" and "Certain to be guilty", and intermediate values ranging from "About 1 chance in 10,000" to "About 9,999 chances in 10,000" with denominator values changing by a decimal place for each choice, and a middle value of "1 chance in 2".
This format was taken from @thompsonLayUnderstanding's study of DNA, which consisted of larger scaled values in the denominator (up to 1 in 10 million).
The second question allowed participants to select both the numerator and denominator: "About ___ chance(s) in ___".
The format of the numeric chance of guilt question depends on the participant's opinion of guilt.
If the participant thought that the defendant was guilty, they were asked to provide the chance that the defendant was innocent.
If the participant thought that the defendant was innocent, they were asked to provide the chance that the defendant was guilty.
In both cases, the numerator had to be less than or equal to the denominator.
Similarly, if individuals thought that the defendant was guilty, they were asked how much they were willing to be that the defendant was guilty, if hypothetically researchers provided them with \$50 and they would double their money if they were correct, and vice versa.
A final new question was open ended, and asked participants to provide their opinion of the defendant.

Individuals were first asked to provide their opinion of the defendant, their conviction decision, and their personal opinion of the guilt of the defendant.
All other questions were randomized in a way that guaranteed the two probability questions and the two chance questions were not asked directly following each other.


#### Prolific

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(readr)
microstudy <- read_csv("data/microstudy_cleanish_results.csv")

microstudy_clean <- microstudy %>% dplyr::filter(check=="9mm")

```

Participants were recruited in a similar manner as Study 1 in terms of the representative sample and self-screened jury requirements through Prolific.
In the first part of the study, some individuals encountered technical issues.
Some of these technical issues, relating to the recording of a unique id or "fingerprint" were resolved, as some browsers have moved to prevent fingerprint recording.
However, some individuals continued having technical issues directly after the informed consent (before reading any testimony), and we have not yet been able to resolve this problem.
These technical issues appeared to happen to more individuals in the match condition compared to the non match condition, with a total of `r sum(microstudy$conclusion.x=="Match")` individuals in the match condition and `r sum(microstudy$conclusion.x=="NoMatch")` participants in the non-match condition, for a total of `r dim(microstudy)[1]` participants.
Participants were paid \$4.00 with a median completion time of 14 minutes and 47 seconds according to Prolific, for an average reward of \$16.23 per hour.
Figure \@ref(fig:completiontime2) shows the time spent after the completion of the first demographics page to the completion of the final results page.
Because it does not include the informed consent and the time to complete the first demographics page, its time estimates appear to be less than those found via Prolific.


```{r}
#| completiontime2,
#| fig.cap= "Completion Time by Condition",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$completion_time <- microstudy$actual_time3-microstudy$actual_time1

ggplot(microstudy, aes(x = completion_time, fill=conclusion.x)) +
    geom_density(alpha=0.75, color=NA) +
  ggtitle("Histogram of Completion Time") +
  xlab("Completion Time in Minutes")+
  scale_fill_manual(name="Conclusion", values = c("#FF8E00", "#037AC7"))+
  theme_bw()


```

### Results

#### Participants

Of the `r dim(microstudy)[1]` participants, `r sum(microstudy$gender=="Male")` participants identified as male, `r sum(microstudy$gender=="Female")` participants identified as female, and `r sum(microstudy$gender=="Other/non-binary")` participants identified as other or non-binary. 
The categories of "Male" and "Female" are currently more associated with sex than with gender, and this categorization will be relabeled in future studies.
The median age category was 46 - 55.
Age and gender are shown in Figure \@ref(fig:demographics2).
Individuals were asked a single attention check question with regards to the caliber of gun used in the attempted robbery.
`r dim(microstudy_clean)[1]` participants passed the attention check, meaning that only `r sum(microstudy$check!="9mm")` participants failed the attention check, and will not be included in the analysis.


```{r}
#| demographics2,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$age = factor(
  microstudy$age,
  levels = c(
    "18 - 25",
    "26 - 35",
    "36 - 45",
    "46 - 55",
    "56 - 65",
    "Over 65"
  )
)

  ggplot(microstudy,
         aes(x = age, fill = gender)) +
  geom_bar(mapping = aes(y = after_stat(count), group = gender),
           position = position_dodge(preserve = "single"), color="black") +
  #  geom_histogram(stat="count", position="dodge")+
  ggtitle("Algorithm Expert Testimony") +
  scale_fill_manual(values = c("#E69F00", "#009E73", "#F0E442")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```
#### Questions from Study 1

`r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition chose to convict, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition chose to convict.
This is a slightly lower proportion of individuals choosing to convict than that seen in the Match condition in the original study, although there is no algorithm present in this case.

Figure \@ref(fig:strength2) shows participant responses to the strength of evidence in this study.
As can be seen, those in the non-match category tended to choose the smallest value for the strength of evidence ("Not at all strong"), while in the case of the match condition, individuals tended to distribute their views of the strength of evidence more evenly.
This graph resembles the strength of evidence results from the initial study.
While not as dramatic as the scale compression for questions of reliability, credibility, and scientificity, the non-match results for strength of evidence do appear to show a scale limitation in the way that individuals tended to select the lowest category.

```{r}
#| strength2,
#| fig.cap= "Microstudy Strength of Evidence",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$strength = factor(
  microstudy_clean$strength,
  levels = c(
    "1 <br/> Not at all strong",
    "2",
    "3",
    "4",
    "5 <br/> Moderately strong",
    "6",
    "7",
    "8",
    "9 <br/> Extremely strong"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=strength, fill=conclusion.x), position="dodge") +
  ggtitle("What is the Strength of Evidence against the Defendant?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition")+
  ylab("Count")+
  xlab("Strength")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

Figure \@ref(fig:prob2) demonstrates the participants' selected probabilities that the defendant committed the crime.
As in the case of the strength of evidence, the graph resembles the probability graph found in Study 1, where there is a higher peak of extreme values for the non-match conditions than for the match condition.
There does not appear to be much difference in the density curves based on the visibility of the probability scale.

```{r}
#| prob2,
#| fig.cap= "Probability Cole Committed Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

colors <-  c("Hidden"="red", "Visible"="grey")

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=prob_hide, fill="Hidden")) +
  geom_density(alpha=0.75, aes(x=prob_vis, fill="Visible")) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = colors, name="Probability")+
  ylab("Density")+
  xlab("Probability")+
  facet_grid(.~conclusion.x)+
  theme_bw()


```

`r round(sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition thought the defendant was guilty, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition thought the defendant was guilty.
Figure \@ref(fig:opinionguilt) shows the comparison between the participant's decision to convict and their personal opinion.
Approximately half of the participant in the match condition who chose not to convict thought the defendant was in fact guilty.

```{r}
#| opinionguilt,
#| fig.cap= "Comparison of opinions of guilt and choice to convict",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=guilty, fill=opinion_guilt), position="dodge") +
  ggtitle("") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Convict?")+
  facet_grid(.~conclusion.x)+
  theme_bw()

```

Figure \@ref(fig:betting) indicates how much participants said they would be willing to bet that the defendant was either guilty or innocent, if provided with \$50.
If the participant indicated that they thought the defendant was innocent, they were asked how much they would be willing to bet that the defendant was innocent, and vice versa.
Two individuals did not answer the question, while three individuals selected values larger than 50: two in the non-match condition who thought Cole was innocent (\$58 and \$100), and one in the match condition who thought Cole was guilty (\$100).
This figure indicates that most people who thought Cole was innocent in the non-match condition were willing to bet the full amount.
It also appears that people tended to select values around multiples of 5, and those in the match condition selected less extreme values (for both those who thought the defendant was guilty and those who thought the defendant was innocent) than what is seen in those with the non-match condition who thought the defendant was innocent.
In this way, the betting response is similar both to the strength of evidence response and the probability of committing the crime response.

```{r}
#| betting,
#| fig.cap= "If the researchers provided 50 dollars, how much would you be willing to bet?",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$bet<- NA

microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$bet<- 
  microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$guilt_bet

microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$bet<- microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$innocent_bet

ggplot(microstudy_clean) +
  geom_histogram(aes(x=bet, fill=opinion_guilt), binwidth=5, color="black",
           position = position_dodge(preserve = "single")) +
  ggtitle("How much would you bet that the Defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Bet Amount")+
  xlim(c(0,50))+
  facet_grid(conclusion.x~.)+
  theme_bw()


```

The remaining questions relate to the chance that the defendant committed the crime.
One question, shown in Figure \@ref(fig:fixedlike), allowed participants to select the chance that the defendant committed the crime from a multiple choice scale.
Based on the scale used, this question remained the same regardless of the participant's opinion on the guilt of the defendant.
This scale does not provide a linear distance between intervals, but instead changes by multiples of 10 in the denominator (ex. one category is 1 in 10, and the next category is 1 in 100).
The exception is the endpoints, which are "Impossible to be guilty" and "Certain to be guilty", as well as the midpoint of 1 chance in 2.
In this case, the non-match scale is not encountering the ceiling or floor effect that has been seen in previous scales, as participants did not overwhelmingly select the lowest value.
Instead, participants are distributed mainly throughout the lower half of the scale, while those in the match condition tended to be a little closer to the center.
Thus, the multiple choice chance scale appears to have less scale compression than seen previously in the other response types.

```{r}
#| fixedlike,
#| fig.cap= "Multiple Choice Chance",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$fixed_like = factor(
  microstudy_clean$fixed_like,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=fixed_like, fill=conclusion.x), position="dodge") +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition")+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

A final question asks individuals to provide a numerical chance that the defendant is either innocent or guilty, depending on their expressed opinion.
If the participant thought that the defendant was innocent, they were asked to supply the chance that the defendant was guilty, and vice versa.
Their responses were limited so that the numerator was smaller than the denominator, resulting in a range of 0 to 1.
These results are shown in Figure \@ref(fig:freelike).
As seen in previous graphs, in the case of the non-match condition, those who thought the defendant was innocent gave small chances that the defendant had in fact committed the crime.
Note that this graph consists of the density in each group.
In the case of the match condition, those who thought the defendant was guilty tended to give lower chances that the defendant had in fact committed the crime, although it is not as extreme as the non-match condition case.

```{r}
#| freelike,
#| fig.cap= "Free Response Chance",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$likelihood<- NA

microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                !is.na(microstudy_clean$innocent_free_denom),]$likelihood<-
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_num/
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) & 
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_denom

microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                !is.na(microstudy_clean$guilt_free_denom),]$likelihood<-
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_num/
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) & 
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_denom

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=likelihood, fill=opinion_guilt), position="dodge") +
  ggtitle("What is the chance that the defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Chance")+
  facet_grid(.~conclusion.x)+
  #scale_x_continuous(trans='log10')+
  theme_bw()


```

Based on the results shown in Figure \@ref(fig:fixedlike), it seems that the results from this free chance scale may benefit from a transformation of the scale.
The log scale transformation is shown in \@ref(fig:freelike10).
This scale transformation shifted the observations from the far left side of the graph to the right side of the graph.
The distribution of the scale is more spread out than that shown in Figure \@ref(fig:freelike), and may lend itself better to analysis.

```{r}
#| freelike10,
#| fig.cap= "Free Response Chance, Log 10 Scale",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=likelihood, fill=opinion_guilt), position="dodge") +
  ggtitle("What is the chance that the defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Chance")+
  facet_grid(.~conclusion.x)+
  scale_x_continuous(trans='log10')+
  theme_bw()


```

#### Scale Comparison

##### Chance Comparison

Consistency across response types is another important aspect of this study.
If different question types result in responses that are inconsistent, it would be difficult to tell which questions truly capture the attitudes of the participants, in order to most accurately answer the research questions.
As mentioned in the previous literature review, there have been studies to suggest that individuals may struggle with the interpretation of chance scales.
Questions of how much a participant is willing to bet may also depend on their personal feel for risk, and betting hypothetical money may have different results than betting real money, or betting money for someone else.
\authorcol{There is a source for making decisions for others somewhere, and it may have had something to do with betting or something to do with plea deals. TBD}
Because individuals may be influenced by which scale they see first, the order of the questions were randomized and recorded, so that the orders can be compared.

Figure \@ref(fig:likecomp1) shows the comparison of the multiple choice chance responses to the numeric chance responses, in the case that the participants thought the defendant was innocent.
Because these individuals thought that the defendant was innocent, there are few responses higher than a "fifty-fifty chance".
The red dots represent the actual value represented by the multiple choice chance (for example, the red dot for "About 1 chance in 10" is located at the y-value of 0.10).
This allows for comparison on the consistency between the multiple choice and the numeric scales.
For values of "1 chance in 2" and "About 1 chance in 10", the responses seem fairly consistent - however, it is difficult to tell if these values are consistent for smaller values, because they are seen as small on the linear scale.

```{r}
#| likecomp1,
#| fig.cap= "Innocent Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


set_values <- data.frame(fixed_like=c(    "Impossible that he is guilty",
                                          "About 1 chance in 10,000",
                                          "About 1 chance in 1,000",
                                          "About 1 chance in 100",
                                          "About 1 chance in 10",
                                          "1 chance in 2 (fifty-fifty chance)",
                                          "About 9 chances in 10",
                                          "About 99 chances in 100",
                                          "About 999 chances in 1,000",
                                          "About 9,999 chances in 10,000",
                                          "Certain to be guilty"),
                         value=c(0,1/10000,1/1000,1/100,1/10,0.5,9/10,99/100,999/1000,9999/10000,1))
clean_results_merged<- dplyr::left_join(microstudy_clean, set_values)
clean_results_merged$fixed_like = factor(
  microstudy_clean$fixed_like,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)
ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Chance for those who thought Cole was innocent") +
  geom_jitter(aes(y=(guilt_free_num/guilt_free_denom)),
    # position = position_jitterdodge(
    #   jitter.width = 0.2,
    #   #jitter.height = 0.4,
    #   dodge.width = 1
    # ),
    size = 1
  ) +
  geom_boxplot(aes(y=(guilt_free_num/guilt_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Chance")+
  xlab("Closed Response Chance")+
#  scale_y_continuous(trans='log10')+
  scale_x_discrete(labels = wrap_format(10))

```

This transformation is shown in Figure \@ref(fig:likecomp1scale).
From this transformed scale, there still appears to be consistency between when participants choose their own chance numerically and when they were offered a multiple choice.
There does, however, appear to be a difference in those who selected "Impossible that he is guilty" from the multiple choice scale.
Many of the numerical responses appear to be more consistent to "About 1 chance in 10,000" than the value of 0 that would indicate impossibility.

```{r}
#| likecomp1scale,
#| fig.cap= "Innocent Chance Comparison Log 10 Scale",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Chance for those who thought Cole was innocent") +
  geom_jitter(aes(y=(guilt_free_num/guilt_free_denom)),
    # position = position_jitterdodge(
    #   jitter.width = 0.2,
    #   #jitter.height = 0.4,
    #   dodge.width = 1
    # ),
    size = 1
  ) +
  geom_boxplot(aes(y=(guilt_free_num/guilt_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Chance")+
  xlab("Closed Response Chance")+
  scale_y_continuous(trans='log10')+
  scale_x_discrete(labels = wrap_format(10))

```

This same procedure can be repeated for those who thought the defendant was guilty, as shown in Figure \@ref(fig:likecomp2).
In this case, because individuals who thought the defendant was guilty were asked to supply the chance that the defendant did not commit the crime, their numerical chance was changed to the chance that the defendant did commit the crime by subtracting their provided chance estimate from 1.
In a reverse of the chance for those who thought Cole was innocent, in this case most responses are at or above "1 chance in 2".
Even without a transformation, it can be seen that there is a large spread of those who selected "Certain to be guilty", and that those who selected "fifty-fifty chance" tended to select higher chances of guilt when they were free to choose their own response, while those who selected "About 9 chances in 10" tended to align well between the numerical and multiple choice options.

The reduced consistency seen in the guilty chance may related to the change in the question wording, where the multiple choice question considers the chance of guilt, while their numeric chance considered the chance of innocence.

```{r}
#| likecomp2,
#| fig.cap= "Guilty Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Chance for those who thought Cole was guilty") +
  geom_jitter(aes(y=(1-innocent_free_num/innocent_free_denom)),
             # position = position_jitterdodge(
             #   jitter.width = 0.2,
             #   #jitter.height = 0.4,
             #   dodge.width = 1
             # ),
             size = 1
  ) +
  geom_boxplot(aes(y=(1-innocent_free_num/innocent_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Chance")+
  xlab("Closed Response Chance")+
  scale_x_discrete(labels = wrap_format(10))

```

```{r}
#| likecomp2scale,
#| fig.cap= "Guilty Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Chance for those who thought Cole was guilty") +
  geom_jitter(aes(y=(1-innocent_free_num/innocent_free_denom)),
             # position = position_jitterdodge(
             #   jitter.width = 0.2,
             #   #jitter.height = 0.4,
             #   dodge.width = 1
             # ),
             size = 1
  ) +
  geom_boxplot(aes(y=(1-innocent_free_num/innocent_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
    scale_y_continuous(trans='sqrt')+
  ylab("Open Response Chance")+
  xlab("Closed Response Chance")+
  scale_x_discrete(labels = wrap_format(10))

```

Similar to the figures above, a comparison graph can be made to compare the multiple choice chance values to the probability scales. 
This scale comparison is shown in Figure \@ref(fig:likeprob).
In the case of the probabilities, individuals were only able to select integers between 0 and 100, meaning that this scale would not translate to the more extreme values of the multiple choice chance scale (outside of the values of "About 1 in 100" and "About 99 in 100").
For values that can map from the probability scale to the chance scale, the values appear to be more spread out than those in the numerical chance scale, and those who selected "About 1 chance in 10" on the multiple choice scale tended to select larger chances.

```{r}
#| likeprob,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Probability vs. Chance That Cole is Guilty") +
  geom_jitter(aes(y=(prob_vis/100)),
             # position = position_jitterdodge(
             #   jitter.width = 0.2,
             #   #jitter.height = 0.4,
             #   dodge.width = 1
             # ),
             size = 1
  ) +
  geom_boxplot(aes(y=(prob_vis/100)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Visible Probability")+
  xlab("Closed Response Chance")+
  scale_x_discrete(labels = wrap_format(10))

```

```{r}
#| likeprobcon,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$likelihood_rev<- NA

microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                !is.na(microstudy_clean$innocent_free_denom),]$likelihood_rev<-1-(
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_num/
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) & 
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_denom)

microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                !is.na(microstudy_clean$guilt_free_denom),]$likelihood_rev<-
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_num/
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) & 
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_denom


ggplot(microstudy_clean, aes(x=likelihood_rev, color=opinion_guilt, y=prob_vis)) +
  geom_jitter(alpha=0.5) +
  ggtitle("Probability vs Chance") +
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Visible Probability")+
  xlab("Chance of Committing Crime")+
  facet_grid(.~conclusion.x)+
  theme_bw()

microstudy_clean$innocent_likelihood<- NA

microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                !is.na(microstudy_clean$innocent_free_denom),]$innocent_likelihood<-(
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_num/
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) & 
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_denom)

microstudy_clean$guilt_likelihood<- NA

microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                !is.na(microstudy_clean$guilt_free_denom),]$guilt_likelihood<-
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_num/
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) & 
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_denom

```

Figure \@ref(fig:likeprobcon) shows a comparison of the perceived probability that the defendant committed the crime compared to the perceived chance that the defendant is guilty.
In the case of the match condition, it can be seen that many individuals assigned a high probability that the defendant committed the crime which corresponded to a high chance of guilt, as shown by the cluster of observations in the top right corner of the graph.
Similarly, in the non-match condition, many individuals gave a low probability that the individual committed the crime and a low chance that the defendant was guilty.
The correlation between the probability and the chances of guilt is `r round(cor(microstudy_clean[!is.na(microstudy_clean$guilt_likelihood),]$prob_vis, microstudy_clean[!is.na(microstudy_clean$guilt_likelihood),]$guilt_likelihood),2)`,
while the correlation between the probability and the chances of innocence is `r round(cor(microstudy_clean[!is.na(microstudy_clean$innocent_likelihood),]$prob_vis, microstudy_clean[!is.na(microstudy_clean$innocent_likelihood),]$innocent_likelihood),2)`.
While there is a fairly strong correlation in the case of chances of guilt and probability, the correlation between the probability and the chances of innocence is weak.
This weak correspondence can be seen in the "Guilty" opinion observations in the graph above
When the chances are combined into a single variable, values can be seen across the entire probability scale.
The correlation between the probability and the combined chance responses is `r round(cor(microstudy_clean[!is.na(microstudy_clean$likelihood_rev),]$prob_vis, microstudy_clean[!is.na(microstudy_clean$likelihood_rev),]$likelihood_rev),2)`, which appears to be fairly strong.

Figure \@ref(fig:likeconvict) demonstrates the relationship between likelihood, probability, and the decision to convict.
In the non-match case, few individuals chose to convict.
In the match condition, those who chose to convict selected a probability above 50%, and tended to select values in the top right corner, which appears to be consistent with their conviction choice.
The clustering of values at around 1 on the chance scale may be due to compression in higher numbers, much like what was seen on the multiple choice question.


```{r}
#| likeconvict,
#| fig.cap= "Probability and Conviction Decision",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x=likelihood_rev, color=guilty, y=prob_vis)) +
  geom_jitter(alpha=0.85) +
  ggtitle("Probability vs Chance") +
  scale_color_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Chance of Committing Crime")+
  facet_grid(.~conclusion.x)+
  theme_bw()

```


Figure \@ref(fig:coordstrcat) shows the relationship between individuals' ratings of the strength of evidence, and the categorical selection of the chance that the defendant committed the crime.
The left graph is for the non-match condition, while the right graph is for the match condition.
Both strength of evidence and the chances of committing the crime have weaker/less of a chance of committing the crime on the bottom, and stronger/more chance of committing the crime at the top.
In the case of the match condition, individuals in the strength condition selected various values for the chance that the defendant committed the crime, indicating that participants do not interpret the chances of committing the crime as the same amount of strength of evidence uniformly.
In the case of the match condition, most individuals chose the smallest level for the strength of evidence.
While most participants in the lowest category of the strength of evidence marked "About 1 chance in 10,000" that the defendant committed the crime, many participants also selected values between "Impossible that he is guilty" and "About 1 chance in 100".
Indeed, the second most popular chance category for those who chose the weakest strength of evidence is "About 1 chance in 100", which is also the most popular chance category for those who chose the second weakest strength of evidence.
Based on these graphs, there does not appear to be consensus on how the strength of evidence translates to the chance that the defendant committed the crime.
Because of the variety of responses in the chance that the defendant committed the crime, it appears that it may be the better method for recording responses.

```{r}
#| coordstrcat,
#| fig.cap= "Plots of perceived strength of evidence and categorical likelihood",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE


strlike_tab<-table(microstudy_clean$strength, microstudy_clean$fixed_like, microstudy_clean$conclusion.x)
strlike_df<-as.data.frame(strlike_tab)
colnames(strlike_df) <- c("Strength", "Chances", "Conclusion", "Freq")


ggparcoord(data=subset(strlike_df, Freq != 0), columns=c("Strength","Chances"), order=c(1,2,3,4,5,6,7,8,9), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("Strength of Evidence Vs Chances of Committing the Crime")+
  facet_grid(.~Conclusion, drop=FALSE, labeller=label_both)

```

In their study, @thompsonJurorsGiveAppropriate2013 compared the given chances that the defendant committed a crime to the decision to convict, and found a threshold of about 9 chances in 10, where participants who selected 9 chances in 10 or a higher chance amount were more likely to select a guilty verdict, while those below 9 chances in 10 were more likely to select a not guilty verdict.
Figure \@ref(fig:convictlike) depicts the results for our study.
Here, participants cross the threshold of being more likely to convict when they select "About 99 chances in 100" in the match condition, or "About 999 chances in 1,000" in the non-match condition.
This threshold is one category larger than what was seen by @thompsonJurorsGiveAppropriate2013.


```{r}
#| convictlike,
#| fig.cap= "Multiple Choice Chance and Conviction Choice",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=fixed_like, fill=guilty), position=position_dodge(preserve = "single")) +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))


```
Figure \@ref(fig:opinionlike) demonstrates the relationship between the chance scale and the participant's opinion of the guilt of the defendant.
In this case, individuals thought the defendant committed the crime if they were above the "fifty-fifty chance" value for the match condition, showing a lower threshold for their opinion of guilt than seen in Figure \@ref(fig:convictlike), when they were asked for their conviction choice.
Individuals who selected the "fifty-fifty chance" appear to be fairly evenly split on their opinion of the guilt of the defendant in the match condition.

```{r}
#| opinionlike,
#| fig.cap= "Multiple Choice Chance and Opinion of Guilt",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=fixed_like, fill=opinion_guilt), position=position_dodge(preserve = "single")) +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Guilty?")+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))


```

##### Probability Comparison

Figure \@ref(fig:probcomp) shows the correspondence between the visible and hidden probabilities.
The values appear to be fairly 1:1.
In the case of the match condition, there appears to be a congregation of values at 90% when the scale is visible.
This is also reflected in the non-match condition, where there is a group of observations at 10%.
Otherwise, the observations appear to correspond fairly well.
The correlation between the probabilities is `r round(cor(microstudy_clean$prob_hide, microstudy_clean$prob_vis),2)`, indicating a strong relationship.

```{r}
#| probcomp,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x=prob_hide, color=opinion_guilt, y=prob_vis)) +
  geom_jitter(alpha=0.5) +
  ggtitle("Visible and Hidden Probability Comparison") +
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Guilty","Innocent"))+
  ylab("Visible Probability")+
  xlab("Hidden Probability")+
  scale_y_continuous(breaks = seq(0, 100, by = 10))+
  facet_grid(.~conclusion.x)+
  coord_fixed(ratio=1)+
  geom_abline(intercept = 0, slope = 1)+
  theme_bw()

```

Figure \@ref(fig:probbet) compares the amount that individuals were willing to bet to their assigned probability that the defendant had committed the crime.
In this case, there is not a straightforward correspondence.
The correlation is `r round(cor(microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$prob_vis, microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$guilt_bet),2)` in the case that the participants were asked to bet on if the defendant committed the crime, and `r round(cor(microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$prob_vis, microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$innocent_bet),2)` when participants were asked to bet on if the defendant did not commit the crime (where the betting procedure is as explained earlier).
This correlation is lower than seen between probability and the chance of committing the crime when both of the chances were combined.

```{r}
#| probbet,
#| fig.cap= "Betting and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x=bet, color=opinion_guilt, y=prob_vis)) +
  geom_jitter(alpha=0.75) +
  ggtitle("Betting and Probability Comparison") +
  scale_color_manual(values = c("#004D40","#D81B60"), name="Bet Defendant is...", labels=c("Innocent","Guilty"))+
  ylab("Visible Probability")+
  xlab("Amount (Dollars)")+
  scale_y_continuous(breaks = seq(0, 100, by = 10))+
  facet_grid(.~conclusion.x)+
  xlim(c(0,50))+
  theme_bw()

```

@mattijssen2020 compared likelihood and strength of evidence ratings from 10 examiners who regularly used likelihood scales.
While this sample size is rather small, they found that examiners tended to give higher verbal degrees of support than they should have, based on their likelihood ratio.
A similar effect can be investigated in the potential jurors of this study, when asked to rate the strength of evidence in the case as compared to their perceived probability that the defendant committed the crime.

Figure \@ref(fig:strengthprob) shows the relationship between the strength of evidence compared to the probability that the defendant committed the crime.
In the match condition, there is a generally increasing correspondence between the perceived strength of evidence and the participants' predicted probability that the defendant committed the crime.
This corresponsence is less clear in the non-match condition.

```{r}
#| strengthprob,
#| fig.cap= "Strength of Evidence and Numerical Chance of Committing the Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean, aes(x = strength, y = prob_vis, fill = opinion_guilt)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Strength of Evidence") +
  facet_grid(conclusion.x~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion: Guilty?")+
  ylab("Visible Probability")+
  xlab("Strength of Evidence")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

#### Demographic Comparison

Demographic information, such as income, race, gender, and education level, was collected on the participants.
This demographic information can be compared with the participants' responses.

##### Income
Figure \@ref(fig:probincome) explores a potential relationship between income and believed probability that the defendant committed the crime.
While there are generally low probabilities across all income levels for the non-match condition, in the match condition there appears to be higher probabilities on the higher and lower ends of the income scale, with lower assigned probabilities for those in the middle of the income scale for those who did not choose to convict.

```{r}
#| probincome,
#| fig.cap= "Income and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$income = factor(
  microstudy_clean$income,
  levels = c(
    "Less than $10,000",
    "$10,000 - $19,999",
    "$20,000 - $29,999",
    "$30,000 - $39,999",
    "$40,000 - $49,999",
    "$50,000 - $59,999",
    "$60,000 - $69,999",
    "$70,000 - $79,999",
    "$80,000 - $89,999",
    "$90,000 - $99,999",
    "$100,000 - $149,999",
    "More than $150,000"
  )
)

ggplot(microstudy_clean, aes(x = income, y = prob_vis, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Income") +
  facet_grid(conclusion.x~.)+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Income")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Bases on Figure \@ref(fig:convictsincome), some individuals in the match condition chose to convict across all income levels.
There does not appear to be a clear trend between conviction choice and income.
While two income amounts (\$30,000 - \$39,999 and \$100,000 - \$149,999) consisted of individuals who chose to convict more than they chose not to convict, this trend is not demonstrated in any other income categories.
In the case of the non-match condition, the individuals who did choose to convict came from different income brackets.

```{r}
#| convictsincome,
#| fig.cap= "Income and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_bar(aes(x=income, fill=guilty), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Income")+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

One may expect income to make a difference in how much individuals choose to bet on their opinion in the case, either in the amount of risk an individual is willing to take or in their desire to keep hypothetical money.
This does not appear to be the case, however, based on Figure \@ref(fig:incomebet).
As discussed earlier, many participants in the non-match condition were willing to bet the full amount that the defendant was innocent.
This is reflected across most income categories as well.
In the case of the match condition, we saw earlier that participants were not as extreme in their bets as is seen in the non-match condition.
This trend is also reflected in the boxplots, with a fairly similar distribution of both the innocent and guilty bets across all income categories.

```{r}
#| incomebet,
#| fig.cap= "Income and Betting Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x = income, y = bet, fill = opinion_guilt)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Amount Bet Compared with Income") +
  facet_grid(conclusion.x~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Bet Defendant is...", labels=c("Guilty","Innocent"))+
  ylab("Visible Probability")+
  xlab("Income")+
  scale_x_discrete(labels = wrap_format(10))+
  ylim(c(0,50))+
  theme_bw()

```

##### Education

Figure \@ref(fig:probeduc) shows the education level along with the participants' probability that the defendant committed the crime.
In this case, there does not appear to be a relationship between education level and probability, as the boxplots for probability are similarly distributed across education levels.

```{r}
#| probeduc,
#| fig.cap= "Education Level and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$educ = factor(
  microstudy_clean$educ,
  levels = c(
    "Less than high school",
    "High school graduate",
    "Some college",
    "2 year degree",
    "4 year degree",
    "Professional degree",
    "Doctorate"
  )
)

ggplot(microstudy_clean, aes(x = educ, y = prob_vis, fill = guilty)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Education") +
  facet_grid(conclusion.x~.)+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Education")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Figure \@ref(fig:convictseduc) shows the relationship between the decision to convict and education level.
As before, there doesn't appear to be much of a relationship between education level and conviction decision.
In the match condition, individuals chose to convict across all educational categories.

```{r}
#| convictseduc,
#| fig.cap= "Education and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_bar(aes(x=educ, fill=guilty), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Education")+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Figure \@ref(fig:educbet) shows the relationship between the betting and the education level.
There does not appear to be a trend between the education level and the amount that participants were willing to bet.
Based on these graphs, it does not appear that there is a relationship between education level and responses.

```{r}
#| educbet,
#| fig.cap= "Education and Betting Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x = educ, y = bet, fill = opinion_guilt)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Amount Bet Compared with Education") +
  facet_grid(conclusion.x~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Bet Defendant is...", labels=c("Guilty","Innocent"))+
  ylab("Visible Probability")+
  xlab("Education")+
  scale_x_discrete(labels = wrap_format(10))+
  ylim(c(0,50))+
  theme_bw()

```

#### Order of the Questions

The order of the questions was assigned through the use of a random sampler, which was re-sampled if the questions of the chance or guilt or the questions regarding probability were placed side by side.
This is because it is possible that the participants would be influenced by the values they chose on the first scale when choosing a value for the second scale.

```{r}
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$first_chance <- NA
microstudy_clean$first_probability<- NA

microstudy_clean$freechance_position <- unlist(gregexpr("2", microstudy_clean$order))
microstudy_clean$fixedchance_position <- unlist(gregexpr("3", microstudy_clean$order))
microstudy_clean$probhide_position <- unlist(gregexpr("5", microstudy_clean$order))
microstudy_clean$probvis_position <- unlist(gregexpr("6", microstudy_clean$order))

microstudy_clean[microstudy_clean$freechance_position < microstudy_clean$fixedchance_position,]$first_chance <- "Numeric"

microstudy_clean[microstudy_clean$freechance_position > microstudy_clean$fixedchance_position,]$first_chance <- "Multiple Choice"

microstudy_clean[microstudy_clean$probhide_position < microstudy_clean$probvis_position,]$first_probability <- "Hidden"

microstudy_clean[microstudy_clean$probhide_position > microstudy_clean$probvis_position,]$first_probability <- "Visible"

```

Figure \@ref(fig:chanceorder) shows the number of participants who received each condition.
In the case of the non-match condition, the counts for each question category appears to be rather even.
However, in the case of the match condition, there is a large discrepancy in which probability question individuals received first.
There are `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Visible")` participants in the match condition who saw the visible probability question first, and `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Hidden")` participants in the match condition who saw the hidden probability question first.
This provides a difference of `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Visible") - sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Hidden")` participants.


```{r}
#| chanceorder,
#| fig.cap= "Order of Chance and Probability Questions",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

chanceplot <- ggplot(microstudy_clean) +
  geom_bar(aes(x=first_chance), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  xlab("First Chance Question")+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

probplot <- ggplot(microstudy_clean) +
  geom_bar(aes(x=first_probability), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  xlab("First Probability Question")+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

chanceplot + probplot


```

```{r}
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_bar(aes(x=first_chance), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  xlab("First Chance Question")+
  facet_grid(conclusion.x~first_probability)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()


```

```{r}
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$first_question <- substring(microstudy_clean$order, 1, 1)
microstudy_clean$second_question <- substring(microstudy_clean$order, 2, 2)
microstudy_clean$third_question <- substring(microstudy_clean$order, 3, 3)
microstudy_clean$fourth_question <- substring(microstudy_clean$order, 4, 4)
microstudy_clean$fifth_question <- substring(microstudy_clean$order, 5, 5)
microstudy_clean$sixth_question <- substring(microstudy_clean$order, 6, 6)
microstudy_clean$seventh_question <- substring(microstudy_clean$order, 7, 7)

first <- ggplot(microstudy_clean) +
  geom_bar(aes(x=first_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

second <- ggplot(microstudy_clean) +
  geom_bar(aes(x=second_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

third <- ggplot(microstudy_clean) +
  geom_bar(aes(x=third_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

fourth <- ggplot(microstudy_clean) +
  geom_bar(aes(x=fourth_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

fifth <- ggplot(microstudy_clean) +
  geom_bar(aes(x=fifth_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

sixth <- ggplot(microstudy_clean) +
  geom_bar(aes(x=sixth_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

seventh <- ggplot(microstudy_clean) +
  geom_bar(aes(x=seventh_question), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  ylab("Count")+
  facet_grid(.~conclusion.x)+
  theme_bw()

first/second/third

fourth/fifth/sixth/seventh

chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$first_question))
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$second_question))
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$third_question))
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$fourth_question))
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$fifth_question))
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$sixth_question))

chisq.test(table(microstudy_clean$first_chance, microstudy_clean$conclusion.x))
chisq.test(table(microstudy_clean$first_probability, microstudy_clean$conclusion.x))

#Significant difference in conclusion
chisq.test(table(microstudy_clean$conclusion.x))

#borderline significant
chisq.test(table(microstudy_clean$seventh_question, microstudy_clean$conclusion.x))

#Significant when only looking at the match condition - not true for other numbers
# The betting question (4) and strength question (7) are less represented
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$seventh_question))

#borderline significant when only looking at the match condition
chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$first_probability))
```

#### Gun Comfort

Participants were asked to rate how comfortable they are with guns.
Figure \@ref(fig:convictscomfort) shows the relationship between choice to convict and comfort with guns.
There does not appear to be a big trend in conviction rate.

```{r}
#| convictscomfort,
#| fig.cap= "Gun Comfort and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$guncomfort = factor(
  microstudy_clean$guncomfort,
  levels = c(
    "Extremely Uncomfortable",
    "Moderately Uncomfortable",
    "Slightly Uncomfortable",
    "Neither Comfortable nor Uncomfortable",
    "Slightly Comfortable",
    "Moderately Comfortable",
    "Extremely Comfortable"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=guncomfort, fill=guilty), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Gun Comfort")+
  facet_grid(conclusion.x~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

## Notes


- Reliability: Questions regarding the consistency of the comparison
  - How often to you think the firearms examiner makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - How often to you think the algorithm makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner
     - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison
   - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results?
       - [blank] out of [blank] runs would agree with the results of the bullet comparison
   <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual -->
   - What if we used a blank slider for credibility/scientificity?
   - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to
     - Did you find the algorithm or the bullet comparison to be more scientific?
     - Compared to the case description, how would you rate the scientificity of the bullet comparison
   - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun?
  
- Clarify if the examiner used the algorithm before or after their initial comparison.
 <!-- - If the algorithm is used first, this would potentially be confounding -->

<!--chapter:end:03-chap3.Rmd-->

# Conclusion - Initial Study Revisited {#finalstudy}

## Introduction

The initial goal of this research was to investigate how a bullet matching algorithm or demonstrative evidence may affect juror perception of the courtroom evidence and procedure.
In the course of this investigation, we encountered two issues: confusion based on the transcript-based testimony format and Likert scale compression.
In order to address these issues, we redesigned the study format to include cartoons and color-coded speech bubbles for clarity, and conducted a survey investigating response type.
We also designed a method for analyzing participant notes that were taken while reading through the testimony, to create a 'heatmap' of areas of text that participants found to be important.

These diverging avenues of investigation have again merged for a final study.
The new cartoon study format seemed to alleviate the confusion that came from the initial transcript format, the response type study indicates that phrasing questions in terms of chance (i.e. "1 chance in 10 that...") reduced/eliminated(?) the scale compression seen in Likert scales, and the text analysis provides useful information about what participants found worth copying into their notes.
In consolidating the information learned from these sub-studies, we returned to the initial study with significant changes designed to assist in elucidating the relationship between the inclusion of an algorithm in courtroom testimony and juror perception.

## Methods

The same initial study scenario is used, as explained in Chapter \@ref(study1): Richard Cole is accused of attempted robbery, and the only evidence presented to study participants is the bullet comparison between the bullet recovered from the scene and Cole's gun.
Independent variables include the examiner's conclusion (match/not a match/inconclusive), the use of demonstrative evidence, such as images (yes/no), and the use of the bullet matching algorithm, with additional testimony from an algorithm expert (yes/no).
The study was reformatted to resemble the format of the response type study - including cartoons, speech bubbles, color-coding, and jury instructions.
This also involved streamlining the testimony transcript to come from a single document, in order to reduce the chance of confounding typos.
Questions with regard to reliability, scientificity, and credibility were re-worded from Likert scale format to chance format.

### Study Format

As in Chapter \@ref(study1), the trial scenario was based on @garrettMockJurorsEvaluation2020, where a bullet is recovered from an attempted robbery of a convenience store and compared to a gun found in the defendant's vehicle in a routine traffic stop.
While the initial study specified that this bullet comparison was the only evidence linking the defendant (Richard Cole) to the crime scene, in this follow up study we instead specified that the transcript provides "select evidence" presented at trial.
Because the examiner may reach an inconclusive or non-match decision, the bullet evidence may not carry enough weight to justify a trial if no other evidence is present.
However, to stay consistent with the initial study, the scenario description included the fact that the store clerk was unable to make an identification because the robber was wearing a ski mask.
As before, participants were asked to use the study notepad to record relevant information, as they would be unable to re-read the testimony.
Participants were then provided with mock court testimony complete with cartoon figures and speech bubbles, followed by questions regarding their impression of the witnesses and the evidence presented.
While the initial study referred to the firearms examiner and algorithm developer as expert witnesses, this language was removed to coincide with recommendations from @OpinionEvidence.
This transcript included witness testimony, cross examination, jury questions in reference to error rates, and jury instructions on witness testimony.
The transcript also included additional cross examination on the subjectivity of the firearm examiner's bullet comparison.
Additional testimony was also included for the swearing in of the witnesses, in order to more clearly indicate that they were testifying for the prosecution (the initial study started after the witness had been called and sworn in).
The witness then testified to their qualifications and the bullet matching process before describing their comparison of the fired evidence to the test fire from the defendant's gun and the subsequent results.
In the scenarios that included the algorithm, the firearm examiner would describe the algorithm's match score for the comparison, and an interpretation of how the match score corresponds to their personal bullet comparison.
The examiner conclusions are described as follows: the match condition includes correspondence in class and individual characteristics; the inconclusive condition includes correspondence in class characteristics, but not enough correspondence in individual characteristics to declare a match; and the exclusion condition included correspondence in class characteristics, but disagreement in individual characteristics.
Demonstrative evidence included images of rifling [@105mmTank2005;@gremi], a bullet comparison, and algorithm images (shown in Chapter \@ref(study1). 
Cross examination consists of questions regarding the ability to uniquely identify the source of the bullet, and an increased number of questions regarding the subjectivity of the bullet comparison.

An algorithm developer also testified if the algorithm was used in the testimony.
The testimony would begin with their qualifications and relationship with the algortihm, before the developer describes the process of obtaining a match score.
As in the initial study, this algorithm description matches that of @hare2017automatic.
They then mention the algorithm's publication history, the open source nature of the code, and if the algorithm can be applied to the type of gun used in this case.
In cross examination, they were asked about the relative newness of the algorithm and subjective calibration aspects, as well as limitations in terms of the types of bullets that the algorithm can evaluate.
The testimony from the initial study is found in Appendix \@ref(testimony-transcripts), while additional modifications are described in Appendix \@ref(study-2-changes)

Participants are then asked to consider the testimony and respond to a set of questions.
They are asked if they would choose to convict, based on the 'beyond a reasonable doubt' threshold.
Unlike the initial study, participants are also asked if they personally believe that the defendant is guilty.
As demonstrated in Chapter \@ref(study2), these two questions can act in conjunction to judge how participants perceive the strength of evidence in the case - those who personally believe the defendant was guilty but choose not to convict are placing the evidential strength somewhere between enough evidence to sway their personal belief, but not enough evidence to say the defendant is guilty 'beyond a reasonable doubt'.
Because of the scale compression found in Likert scales in the initial study, the questions regarding the strength of evidence, the credibility of the examiners, the reliability and scientificity of the evidence, and the understanding of the procedures were changed.
Instead, questions were formed in terms of chance scales, as described in Chapter \@ref(study2). 
<!-- **[TO BE DEVELOPED]**. -->
Chapter \@ref(study2) also demonstrates some of the limitations of asking participants to give a probability that the defendant committed the crime - in the match condition, responses are clustered at the end of the scale. 
<!-- **[LOOK INTO A WAY TO CHANGE THE SCALING ON THE SLIDER?]** -->
Participants were also asked to answer two attention check questions: one regarding the caliber of the recovered bullet to ensure the participant read the testimony, and another asking for the participant to select a specific value to ensure the participant read the question.
Participants who received the algorithm were asked additional questions regarding the algorithm evidence, as well as their perception of the the evidence as a whole (considering both the algorithm and the firearm examiner's comparison)



<!-- ### Prolific -->

<!-- Participants were recruited using Prolific, an online survey-taking website.  -->
<!-- \authorcol{From the Prolific website, participants were directed to a link containing our survey, created using RShiny.} -->
<!-- We selected options to recruit a representative sample of individuals located in the United States, and asked that participants self-screen for jury eligibility before completing the survey.  -->
<!-- Jury eligibility was defined as US citizens over the age of majority in their state who had not been convicted of a felony, were not active law enforcement, military, emergency response, or a judge, and who did not have a disability that would prevent them from serving on a jury.  -->
<!-- They were also required to have normal or corrected to normal vision, due to the images used in the study.  -->
<!-- Participants were compensated with \$8.40 for completing the study, for an hourly compensation rate of about \$27.79 (median completion time of approximately 18 minutes).   -->
<!-- Individuals who did not include their Prolific identification number and an individual whose notes indicated that they had progressed far enough into the survey to get a separate scenario before restarting were excluded from analysis. -->


<!--chapter:end:04-conclusion.Rmd-->

`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 


# Testimony Transcripts

## Firearm Examiner

Q: Please state your name. 

A: John Smith

Q: Who do you work for? [@azvscelaya, 6-7]

A: The local police department

Q: What do you do for the police department? [@azvscelaya, 6-7]

A: I am a firearms examiner [@azvscelaya, 6-7]

Q: How long have you been doing that? [@azvscelaya, 6-7]

A: X amount of years

Q: What is a firearms examiner? [@azvscelaya, 6-7]

A: A firearms examiner is someone who looks at cartridge cases and bullets to determine whether they were fired in a particular firearm.

Q: What training is required to become a firearms examiner with the local police department? [@azvscelaya, 6-7]

A: I received my bachelor’s degree in forensic science and in X year I transferred to the crime lab from the crime scene unit. [@azvscelaya, 6-7] 
I underwent a two-year training program, which was supervised by experienced firearms examiners [@flvssheppard, 798-799]; 
I’ve toured manufacturing facilities and saw how firearms and ammunition were produced; and I’ve attended several national and regional meetings of firearms examiners.[@azvscelaya, 6-7]

Q: And during the course of your career, were you tested in proficiency to make sure.. you were still conducting appropriate examinations? [@flvssheppard, 798-799]

A: Yes. I have undergone annual proficiency examinations.[@flvssheppard, 798-799]

Q: Is the local police department lab accredited? [@azvscelaya, 6-7]

A: Yes, it is accredited by ASCLID/LAB [@azvscelaya, 6-7]

Q: And in addition to what you just told us, do you have other qualifications? [@ilvsbanks, 2]

A: Yes, I do.[@ilvsbanks, 2]

Q: Can you tell us what those are? [@ilvsbanks, 2]

A: Yes. I received training in the use of a bullet matching algorithm. 
This is an algorithm that evaluates the characteristics of two fired bullets, in order to produce a score for the similarity of the bullets, where more similar bullets are more likely to have been fired from the same gun. 
I attended a workshop on the algorithm on [DATE], held by CSAFE – Center for Statistics and Applications in Forensic Evidence. 
This involved use of the algorithm alongside my personal judgement. 
I found that my conclusion was reflected in the similarity score produced by the algorithm in all ## cases.

Q: Where does the bullet matching algorithm come from? [@mivsjohnson, 5]

A: It was created by _____. [@mivsjohnson, 5]

Q: How long has the state police been using the bullet matching algorithm? [@mivsjohnson, 5]

A: They have been using it since ###.

Q: Have you testified in court previously regarding the bullet matching algorithm? [@mivsjohnson, 5]

A: Yes, I have, approximately ## times. [@mivsjohnson, 5]

Q: Have you, as a firearms examiner, have you testified about opinions, given your opinion about the results of your testing? [@mivsjohnson, 5]

A: Yes, I have. [@mivsjohnson, 5]

Prosecution: Your Honor, at this time I would ask that XXX be qualified as an expert in the field of firearms identification subject to cross examination. [@ilvsbanks, 4]

Court: Any cross on their credentials? [@ilvsbanks, 4]

Defense: No, Your Honor [@ilvsbanks, 4]

Court: This witness is an expert in the area of firearms identification. 
They can testify to their opinions as well as facts. 
Go ahead.[@ilvsbanks, 4]

Q: What work did you do on this case? [@azvscelaya, 8]

A: I was asked to compare a bullet from the crime scene to a test fire from XX gun.[@azvscelaya, 8]

Q: Did you examine how many lands and grooves the bullet had? [@azvscelaya, 17-18]

A: Yes. [@azvscelaya, 17-18]

Q: Can you explain for the jury what that means? [@azvscelaya, 17-18]

A: Yes... In the interior of a barrel there are raised portions called lands and depressed areas called grooves. 
When a bullet passes down the barrel, a bullet will spin and that gives it stability and accuracy over a distance. 
Those raised areas are designed by the manufacturer. 
They’re cut into the barrel. 
And each particular file has a different combination of lands and grooves.
But essentially what those lands do will grip a bullet and spin it and as that bullet passes down the barrel, it scratches the random imperfections of that barrel into the bullet.[@azvscelaya, 17-18]

Q: Now, for these bullets, you counted up the lands and grooves and determined the direction of the twist, correct? [@azvscelaya, 17-18]

A: Yes. This bullet had six lands. And the interior of the barrel, the barrel will either twist right or it will twist left. 
And this particular case, the barrel twists right. 
And you can see that by looking at the bullet. 
If you look at the base of the bullet, either it goes to the left or goes to the right.[@azvscelaya, 17-18]

---Test-fired bullets admitted into evidence---

Q: Can you describe the process of obtaining these test-fired bullets?

A: The test-fired bullets came from a test fire of XX gun.

Q: You mentioned test firings, can you explain what that means? [@ilvsbanks, 6]

A: In test firing first what I would do is make sure the firearm is safe to actually test fire. 
Then I would use lab ammunition and I would test fire it, meaning that I’m creating a fired bullet. 
Typically you do two at a time. 
That way you have a fired bullet to compare to another fired bullet. [@ilvsbanks, 6]

Q: Would you then have taken the test firings you created and did you compare those test firings to the fired evidence that you had also received? [@ilvsbanks, 6]

A: Yes. First what I would do is compare my test shot to test shot.
I’m looking for a detailed microscopic pattern. 
Once I have done that then I would compare it to the fired evidence.[@ilvsbanks, 6]

Q: And how about the number of lands and direction of twist for the test fires? [@azvscelaya, 19-20]

A: It also had six lands, and twisted to the right [@azvscelaya, 19-20]

Q: Okay. Now, did you compare the test-fired bullets to the fired evidence under the comparison microscope? [@azvscelaya, 19-20]

A: Yes, I did. [@azvscelaya, 19-20]

Q: What is your conclusion? [@azvscelaya, 19-20]

A: I found that there were sufficient individualizing characteristics to conclude that the two bullets were fired from the same barrel. 

Q: How were you able to conclude that? [@azvscelaya, 19-20]

A: I placed them under the comparison microscope, and I roll the bullet around ‘til I can see the agreement in a particular area, unique surface contour that has sufficient agreement. 
At that point, when I’ve seen that, I start to rotate the bullets around and I look at all the different lands and grooves, impressions, for that unique detail. 
When I can see those, that agreement on multiple areas of the bullet, I identify the bullet as having sufficient agreement.[@azvscelaya, 19-20]

Q: Did you use an algorithm to compare these bullets as well?

A: Yes, I used the algorithm to compare the two test fires to each other. 
I also used the algorithm to compare the better-marking test fire to the fired evidence that I received.

Q: Could you explain how this algorithm compares bullets?

A: The algorithm uses 3D measurements to make a comparison between the surface contours of each of the lands on each bullet. 
These comparisons result in a match score between 0 and 1, where 1 indicates a clear match, and 0 indicates that there is not a match.
The bullet is aligned based on the maximum agreement between the lands, and the average match score for the lands is computed. 
This average score gives an overall match score for the entire bullet.

Q: What was the match score between the two test-fired bullets?

A: The match score was 0.9-.

Q: What was the match score between the better-marked test fire bullet and the fired evidence?

A: The match score was 0.XX.

Q: What does this match score indicate about the bullets?

A: The match score indicates that there is substantial similarity between the two bullets, which suggests that they were most likely fired from the same barrel.

Q: Now, how many times have you compared bullets to determine if they were fired from the same gun? [@azvscelaya, 19-20]

A: I’d say thousands. [@azvscelaya, 19-20]

Q: And do you ever see two bullets that have agreement in every area of the bullet? [@azvscelaya, 19-20]

A: No. 
The firing process of a firearm is dynamic, kind of like a contained explosion. 
When the firing pin hits the primer, which is basically the initiator, what gets it going, it will explode, bur the gun powder inside the casing, and the bullet will travel down the barrel, picking up the microscopic imperfections of the barrel, and the cartridge case will slam rearward against the support mechanism.
During that dynamic process, each time it happens, a bullet will be marked slightly differently from one to the next. [@azvscelaya, 19-20]

Q: When you reached a conclusion, did you write up a report? [@azvscelaya, 23]

A: Yes, I did. [@azvscelaya, 23]

Q: Is it the local police department’s protocol to have somebody else who’s a firearms tool mark examiner in your lab review that report, review your work, and determine if it’s correct? [@azvscelaya, 23]

A: Yes. [@azvscelaya, 23]

Q: That’s what we call peer review? [@azvscelaya, 23]

A: Peer review, yes. [@azvscelaya, 23]

Q: Thank you, no further questions. [@azvscelaya, 40]

--Cross examination—-

Q: Now, are you telling the jury today that in your opinion there’s only one gun in the entire world that could have produced the markings that you saw on these bullets? [@azvscelaya, 65]

A: I’m saying that the probability that the two markings were made by different sources is so small that it is negligible. [@dojcompare, 2] [@azvscelaya, 65]

Q: Exclusive to any other gun? [@azvscelaya, 65]

A: I obviously haven’t tested every other gun, but it is a practical impossibility. [@azvscelaya, 65]

Q: That’s your opinion. And your opinion, by the way, is subjective, right, and it’s based on your experience? [@azvscelaya, 72-73]

A: Based on my training and experience, yes. [@azvscelaya, 72-73]

Q: Is there something fixed about the amount of what has to be found to constitute sufficient agreement? [@usvsharris, 4459-4460]

A: No, there is not a fixed amount or a numerical value. [@usvsharris, 4459-4460]

Q: How long did you say you’ve been trained in the bullet matching algorithm? [@mivsjohnson, 19 - 21]

A: Since XXX [@mivsjohnson, 19 - 21]

Q: Okay. So that’s fairly new; is that fair to say? [@mivsjohnson, 19 - 21]

A: It is still fairly new, yes. [@mivsjohnson, 19 - 21]

Q: The software uses modeling; is that correct? [@mivsjohnson, 19 - 21]

A: Yes, it does. [@mivsjohnson, 19 - 21]

Q: You, personally, don’t know the source code; is that correct? [@mivsjohnson, 19 - 21]

A: That’s correct.[@mivsjohnson, 19 - 21]

Q: And, in fact, you, personally, would not be able to tell us the specific math that goes into this program; is that fair to say? [@mivsjohnson, 19 - 21]

A: We did receive training on what the math is doing. [@mivsjohnson, 19 - 21]


The Court: Terry Smith, the jury has asked me to forward this question to you. Answer if you're able.
To what percentage is the science accurate is the first question. And then I think the rest of that explanation of that question goes on to say, to determine that the bullets were fired from the same firearm, are you 100 percent sure? [@azvscelaya, 100 -101]

A: My opinion, I am 100 percent sure that these bullets were fired from this firearm. 
There is a published error rate for firearms examiners. 
The positive identification is less than two percent. 
I believe it's about 1.5 to 1.9. That's just a general number that's out there.[@azvscelaya, 100 -101]
The false negative identification rate is less than three percent. [@chumbley2021]

## Algorithm Expert
Inspired by @azvscelaya, @ilvsbanks, @mivsjohnson, @usvsharris, @flvssheppard, @hare2017automatic, @vanderplasComparisonThreeSimilarity2020, and @cyberposter. Should create citations by line as above.

Q: Please state your name.

A: Adrian/Avery Jones

Q: Who do you work for? [@azvscelaya, 6-7]

A: XXX place

Q: What is your current occupation?

A: XXX

Q: How long have you been doing that? [@azvscelaya, 6-7]

A: X amount of years

Q: What training is required to hold this occupation? [@azvscelaya, 6-7]

A: I have a Ph.D. in XX, and have spent X time developing the bullet matching algorithm algorithms. 
I have spent X years collaborating with firearms examiners during the development and rollout of this algorithm. 

Q: Are you familiar with the bullet matching algorithm?

A: Yes. I was involved in the development of the algorithm.

Prosecution: Your Honor, at this time I would ask that XXX be qualified as an expert in the bullet matching algorithm subject to cross examination.
Court: Any cross on their credentials? [@ilvsbanks, 4]

Defense: No, Your Honor [@ilvsbanks, 4]

Court: This witness is an expert in the area of the bullet matching algorithm. They can testify to their opinions as well as facts. [@ilvsbanks, 4]

Q: How many times have you testified regarding this bullet matching algorithm?

A: XX times.

Q: Could you describe how this bullet matching algorithm compares bullets?

A: Yes. For certain types of guns, the barrel will have lands and grooves, known as rifling. 
This rifling spins the bullet in order to make its trajectory more stable. Due to the manufacturing process, this rifling can produce identifiable markings on the bullet, based on random differences between barrels. 
Because of these random imperfections, the striation marks left on bullets can be compared in order to determine if it is likely that they were fired from the same gun. 

(Description based on @hare2017automatic)
The first step is to determine where the lands on the bullet are located. 
These lands will be the sunken area that contains the striation marks between the smoother grooves. 
3D scans are then taken for each land, and the ‘shoulders’, or area transitioning from the land to the groove, is excluded from the analysis.

Next, a stable area of the 3D scan containing the striations is selected, and a cross-section of this area is used to show the striations along with the topology of the region. 
A smoothing function is applied to remove some of the imaging noise from the 3D scan, leaving the striae intact. 
A second smooth is subtracted from the striations in order to remove the curvature of the region, leaving only the striae – this is what we call a signature. 
The signature for the two bullets being compared are aligned such that the best fit between the two signatures is achieved. 
The striation marks between the two signatures are then compared by evaluating how many of the high points and low points correspond. 
The algorithm can calculate the number of consecutively matching striations (CMS), or consecutively matching high points and low points – these are features used directly by some examiners to characterize the strength of a match. 
It also calculates the cross correlation between the two signatures, which is a numerical measure of the similarity between the two lands ranging between –1 and 1. 

These traits are combined using what is known as a random forest. 
Each forest is composed of decision trees, which use a subset of the observed values in order to make a decision about whether or not the bullets constitute a match. 
The other observations are held out in order to determine an error rate. 
When the random forest makes a prediction, each decision tree “votes”, producing a numerical value between 0 and 1 corresponding to the percentage of trees which evaluate the features as being sufficiently similar to have come from the same source. 

Q: Have you tested this algorithm?

A: Yes. 
This algorithm was tested and validated on a number of different test sets of bullet scans. 
It was found that, as long as there are sufficient marks on the bullet, the algorithm could successfully distinguish between bullets fired by the same gun and those fired from different guns. 
Examiners’ visual comparisons are also limited by the presence or absence of individualizing marks. 
Two test sets were using consecutively rifled barrels, which should be the most difficult to assess, and it was shown that the algorithm could distinguish between the bullets fired from two separate guns with complete accuracy. [@vanderplasComparisonThreeSimilarity2020]

Q: Can this algorithm be used on XX bullets fired from an XX firearm, such as the firearm in question for this case?
(Discussion of limitations [@mivsjohnson, 11])

A: Yes, this algorithm can be used on these types of bullets, given that this type of firearm marks well.

Q: Has this algorithm been published? (@cyberposter for importance of publication)

A: Yes. The algorithm and its process has been discussed in peer reviewed journals such as Law, Probability, and Risk, The Annals of Applied Statistics, and Forensic Science International. 
The algorithm is also open source, which means that the full source code, documentation, and numerical weights are available online for anyone to examine.


--Cross examination—-

Q: How long has the bullet matching algorithm been used in court cases? [@mivsjohnson, 19 - 21]

A: Since XXX

Q: Okay. So that’s fairly new; is that fair to say? [@mivsjohnson, 19 - 21]

A: It is still fairly new, yes. [@mivsjohnson, 19 - 21]

Q: This algorithm requires some decision making on the part of the operator, such as how much the signature is smoothed and what part of the bullets are inputted into the system, correct? (subjective aspects [@mivsjohnson, 21])

A: Yes, there are certain parameters which must be specified, but the system defaults are usually sufficient and have been validated with a number of different firearms and ammunition types. 
There are also operating protocols for determining which parts of the bullet are scanned, so while this process is manual, there are clear criteria and the associated variability from scanning is well understood, and published in Forensic Science International. 

Q: Can this algorithm be applied to all bullets? (gun specific/issues with damaged bullets)

A: No, the algorithm only works on traditionally rifled bullets which are largely undamaged. 
The algorithm has not been validated to work on seriously damaged or fragmented bullets.


# Study 2 Changes

## Cautions Against Expert Witnesses

### Jury Instructions

You have heard testimony from Terry Smith and Adrian Jones who have testified to opinions and the reasons for their opinions. This opinion testimony is allowed because of the education or experience of this witness. 
Such opinion testimony should be judged like any other testimony. 
You may accept or reject it, and give it as much weight as you think it deserves, considering the witness's education and experience, the reasons given for the opinion, and all the other evidence in the case. [@OpinionEvidence]

### Opinion Witness

@OpinionEvidence suggests that witnesses should not be qualified as experts, as it may cause the jury to place undue weight in their testimony. Instead, @PROPOSALSELIMINATEPREJUDICIAL implements the term "opinion witness" in the place of "expert witness". This practice has been adopted in our revised testimony, when qualifying the witness based on experience and education.

### Cross Examination

The strength of evidence ("the probability that the two markings were made by different sources is so small that it is negligible") was moved from the cross examination to the direct examination, since this strength of evidnece should be introduced by the prosecution. Additional testimony regarding subjectivity was added to the cross examination as well, from [@usvsharris, 54]:

Q: You have a criterion in you head, a subjective criterion, of what will constitute an identification. Is that correct?

A: Yes.

Q: All right. If we brought three more people in, would their subjective criterion in their head be identical to yours?

A: They would look for that same sufficient agreement. However, based on their training and experience on difficult comparisons, they may or may not come to the same decision.

Q: Right. But they also could reach the same decision but have a different subjective criterion than you. Is that fair?

A: They may, possibly. I don't know. I can't speak for other examiners.

Additional testimony was also added to address the distinction between evidence that the gun was at the crime scene, and evidence that Cole was at the crime scene [@flvssheppard, 831]:

Q: You can't tell us anything about who shot the firearm, correct?

A: That is correct.

## Clarifying Sides

To make sure individuals are clear on who the firearms examiner is testifying for, testimony was added related to the calling and the swearing in of the witness. The testimony below is from @mivsjohnson.

Court: Your next witness, please.

Prosecution: The People would call John Smith.

Court: Hello.

Smith: Hi

Court: Would you raise your right hand for me? Do you sear the testimony you're about to give will be the truth, the whole truth, nothing but the truth, under penalty of perjury?

Smith: I do.

Court: Please be seated. State your full name for the record and spell your last name.

Witness: My name in John Smith, last name S-m-i-t-h

## Images

In order to clarify the speakers in the testimony and create a more engaging environment, we implemented character drawings (by Richy Meleus) to represent various actors that may be present in the courtroom. These characters' faces and clothing are largely exchangeable, allowing for a large combination of clothing and characteristics.

```{r demo_evidence, echo=FALSE, warning=FALSE}
library(gridExtra)
library(grid)
library(jpeg)

judge <- readJPEG("images/Judge.jpg")

forensic_science <- readJPEG("images/Forensic Scientist.jpg")

lawyer <- readJPEG("images/Lawyer.jpg")

analyst <- readJPEG("images/Analyst.jpg")

defendant <- readJPEG("images/Defendant.jpg")

hazmat <- readJPEG("images/Hazmat worker.jpg")
police <- readJPEG("images/Police Officer.jpg")

inmate <- readJPEG("images/Inmate.jpg")

grid.arrange(rasterGrob(judge),rasterGrob(forensic_science),rasterGrob(lawyer),
             rasterGrob(analyst),rasterGrob(defendant),rasterGrob(hazmat),
             rasterGrob(police),rasterGrob(inmate),
             nrow=2, ncol=4)
```

```{r jury, echo=FALSE, warning=FALSE}

include_graphics("images/Jury.jpg")
```

These images were placed in their respective speech bubbles in order to indicate the speaker, with color coding to indicate which side witnesses were testifying for: the prosecution's side had warmer colors, the defense had cooler colors, and the judge was a neutral grey. 

```{r screenshots, echo=FALSE, warning=FALSE}

include_graphics("images/Study2_Screenshot.jpg")

include_graphics("images/Study2_Screenshot2.jpg")
```

<!--chapter:end:05-appendix.Rmd-->

# Colophon {-}

This document is set in [EB Garamond](https://github.com/georgd/EB-Garamond), [Source Code Pro](https://github.com/adobe-fonts/source-code-pro/) and [Lato](http://www.latofonts.com/lato-free-fonts/). The body text is set at 11pt with $\familydefault$. 

It was written in R Markdown and $\LaTeX$, and rendered into PDF using [huskydown](https://github.com/benmarwick/huskydown) and [bookdown](https://github.com/rstudio/bookdown). 

This document was typeset using the XeTeX typesetting system, and the [University of Washington Thesis class](http://staff.washington.edu/fox/tex/) class created by Jim Fox. Under the hood, the [University of Washington Thesis LaTeX template](https://github.com/UWIT-IAM/UWThesis) is used to ensure that documents conform precisely to submission standards. Other elements of the document formatting source code have been taken from the [Latex, Knitr, and RMarkdown templates for UC Berkeley's graduate thesis](https://github.com/stevenpollack/ucbthesis), and [Dissertate: a LaTeX dissertation template to support the production and typesetting of a PhD dissertation at Harvard, Princeton, and NYU](https://github.com/suchow/Dissertate)

The source files for this thesis, along with all the data files, have been organised into an R repository, which is available at https://github.com/rachelesrogers/UNL_Thesis. <!-- A hard copy of the thesis can be found in the University of Washington library. -->

This version of the thesis was generated on `r Sys.time()`. <!--The repository is currently at this commit:-->

```{r echo = FALSE, eval = FALSE}
# I've set eval=FALSE to ensure Travis-CI can run
# if you're not using Travis-CI, then eval=TRUE will be fine
library(git2r)
if ( in_repository() ) {
       summary(commits()[[1]]) 
} else { 
       message("We are not in a git repository") 
}
```

The computational environment that was used to generate this version is as follows:

```{r echo = FALSE}
devtools::session_info()
```


<!--chapter:end:98-colophon.Rmd-->

<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# References {-}
<!--
This manually sets the header for this unnumbered chapter.
-->
<!-- \markboth{References}{References} -->
<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...



<!--chapter:end:99-references.Rmd-->

