# Jury Perception Revisited: This Time with Caricatures {#study2}

## Background


### Study 1

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales in this scenario, as well as the use of a transcript testimony format.

Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a micro study in order to compare various response types, in order to determine substitute questions that may be more sensitive to scenario differences.
Additional changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner and algorithm developer as they would any other witness.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well.
The transcript for the second study can be found in Appendix \@ref(study-2-changes).

Some participants left confused comments with regards to who the witnesses were testifying for.
The transcript format may lend an air of impartiality to the witnesses, when they are in fact testifying for a specific side in the case.
Also, the format of the testimony transcripts does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by a participant.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, one master file is used with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text.
Unique identification is used throughout the database files instead of relying on unique fingerprints, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study, as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a study in order to determine how these figures are perceived.

## Micro Study on Response Type

### Methods

#### Study Format


## Notes


- Reliability: Questions regarding the consistency of the comparison
  - How often to you think the firearms examiner makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - How often to you think the algorithm makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner
     - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison
   - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results?
       - [blank] out of [blank] runs would agree with the results of the bullet comparison
   <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual -->
   - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to
     - Did you find the algorithm or the bullet comparison to be more scientific?
     - Compared to the case description, how would you rate the scientificity of the bullet comparison
   - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun?
  
- Clarify if the examiner used the algorithm before or after their initial comparison.
 <!-- - If the algorithm is used first, this would potentially be confounding -->