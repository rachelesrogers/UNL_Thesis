# An Investigation of Response Types {#study2}

## Background


### Problems in Study 1

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales when evaluating jury perception of firearms experts, as well as the use of a transcript testimony format.
Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a follow-up study comparing various response types, in order to determine substitute questions that may result in a wider distribution of responses.

Further changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner as they would any other witness and removing the use of the term "expert".
We enacted these changes after investigating the scale compression in responses, and finding that @OpinionEvidence suggests that the use of the term "expert" may result in jurors giving the testimony more weight, and provides sample text for the judge to clarify the presence of the witnesses as opinion witnesses instead of experts.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well, due to a lack of distinction in participants between evidence tying the firearm to the crime scene, and evidence tying the defendant to the crime scene.
As in the initial study, these changes were directly related to trial transcripts or resources.
The transcript changes for the second study can be found in Appendix \@ref(study-2-changes).

In the initial study, some participants were confused about who the witnesses were testifying for, as evidenced by their written comments.
This may be due to the testimony format, which does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals cues for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by some participants.
The transcript format may also lend an air of impartiality to the witnesses due to the relatively unclear identity of the individual asking the questions.
This may have been compounded by the exclusion of testimony where the prosecution called the witness to testify, instead beginning with the swearing in of the witness.
This has been rectified in this second study.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies to provide visual representation of relevant actors, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, we used one master file with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text, and this format allows for more flexibility when implementing the R Shiny application.
We also use unique identification throughout the database files in the form of Prolific ID's and a random number assigned to the participant instead of relying on unique fingerprints, which resulted in technical difficulties in a preliminary run of this second study, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study (drawn by Richy Meleus), as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a follow-up study in order to determine how these figures are perceived.
For the current study, the same figures are used consistently across all conditions to prevent a confounding effect.

To investigate a which response type may be most informative for this study and to troubleshoot the use of figures and speech bubble format, we conducted a smaller micro study using various response types.

## Methods

### Study Format

Participants were presented with the same scenario described in Chapter \@ref(study1): participants read a trial transcript regarding an attempted convenience store robbery, where the only evidence presented is a bullet comparison between the defendant's gun and a bullet recovered from the scene of the crime (a scenario from @garrettMockJurorsEvaluation2020).
In this case, however, the only factor that was changed was the conclusion of the firearms examiner: either an indentification or exclusion.
In all conditions, the algorithm was absent and there were no images.
The testimony largely followed that of Chapter \@ref(study1), aside from differences in the format and additional testimony described above and outlined in Appendix \@ref(study-2-changes).
In this case, it was also explicitly stated that the testimony did not reflect all evidence presented in the case, because in the exclusion condition there may not be enough evidence to bring the case to trial.
Figure \@ref(fig:responsefigures) depicts the characters used in this study. 
Those speaking on behalf of the prosecution were coded with speech bubbles in warmer colors (red for the prosecutor and orange for the forensic scientist), while the defense was coded with a cooler color (purple), and the judge was coded as grey.

```{r}
#| responsefigures,
#| fig.cap= "Figures used in the response study. From left to right: the judge, forensic scientist, prosecutor, and defense lawyer.",
#| fig.width= 7,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

library(gridExtra)
library(grid)
library(jpeg)

judge <- readJPEG("images/Judge.jpg")

forensic_science <- readJPEG("images/Forensic Scientist.jpg")

lawyer <- readJPEG("images/Lawyer.jpg")

analyst <- readJPEG("images/Analyst.jpg")

grid.arrange(rasterGrob(judge),rasterGrob(forensic_science),
             rasterGrob(lawyer),rasterGrob(analyst),nrow=1, ncol=4)
```

At the end of the testimony, the participants were asked a variety of questions regarding the strength of evidence in the case.
Questions of probability, strength of evidence, and decision to convict were repeated from the Chapter \@ref(study1) study.
Participants were asked to evaluate the probability that the defendant committed the crime, both with a visible probability scale (allowing the participants to select the exact probability integer) and with a non-visible scale (only the numbers of 0 and 100 were available on the extremes of the scale).
The strength of evidence was a Likert scale with values from "Not at all strong" to "Extremely strong" with nine points.
In terms of conviction, participants were reminded of the decision criterion of "beyond a reasonable doubt" when making a decision in a criminal trial, and asked if they would choose to convict.

New questions for this response study asked individuals to give their opinion of the guilt of the defendant, as well as assessing the chances that the defendant committed the crime, how much they would be willing to bet that the defendant was either innocent or guilty, and their opinion of the defendant.
In addition to a question regarding whether or not the participant would choose to convict, we also asked the participants to give their personal opinion on the guilt of the defendant.
This provides a second threshold for assessing the strength of evidence, aside from the "beyond a reasonable doubt" standard.

Two different questions were asked with regards to the chances that the defendant committed the crime. 
One was in a multiple choice format, with extreme values of "Impossible to be guilty" and "Certain to be guilty", and intermediate values ranging from "About 1 chance in 10,000" to "About 9,999 chances in 10,000" with denominator values changing by a decimal place for each choice, and a middle value of "1 chance in 2".
This format was taken from @thompsonLayUnderstanding's study of DNA, which consisted of larger scaled values in the denominator (up to 1 in 10 million).
The second question allowed participants to select both the numerator and denominator: "About ___ chance(s) in ___ that the defendant is (innocent/guilty)", where participants were able to select to express the value as innocence or guilt (the default value was "innocent").
In the preliminary study, participants had to express chances that corresponded with their opinion on the defendant committing the crime, but this resulted in some scale confusion.
In both cases, the numerator had to be less than or equal to the denominator.

Individuals who thought that the defendant was guilty were asked how much they were willing to bet that the defendant was guilty, if hypothetically researchers provided them with \$50 and they would double their money if they were correct, and vice versa.
A final new question was open ended, and asked participants to provide their opinion of the defendant.
We first asked individuals to provide their opinion of the defendant, their conviction decision, and their personal opinion of the guilt of the defendant.
All other questions were randomized in a way that guaranteed the two probability questions and the two chance questions were not asked directly following each other.


### Prolific

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(scales)
library(ggridges)
microstudy <- read_csv("data/mircrostudy_response_redo_clean.csv")

# Removing record with unusually low completion time

microstudy_complete <- 
  microstudy[complete.cases(microstudy[ , c("convict", "guilt_opinion", 
                                            "betting", "hidden_probability",
                                            "chances_fixed", 
                                            "evidence_strength",      
                                            "numeric_chance", 
                                            "check", "visible_probability")]), ] %>% dplyr::filter(round(randomnumber,5) != 12.59938)

microstudy_clean <- microstudy_complete %>% 
  dplyr::filter(check=="9mm", round(randomnumber,5) != 12.59938)

prolific_data <- read_csv("data/microstudy_redo_prolific_clean.csv")

approved_prolific <- prolific_data %>% dplyr::filter(Status == "APPROVED")

```

Participants were recruited in a similar manner as Study 1 in terms of the representative sample and self-screened jury requirements through Prolific.
Participants were paid \$4.07 with a median completion time of 15 minutes and 15 seconds according to Prolific, for an average reward of \$16.01 per hour.
There were 300 paid participants who completed the survey, 22 individuals who decided to not complete the survey ("returned"), and 8 participants who did not complete the survey before Prolific's cutoff time ("Timed-out").
Figure \@ref(fig:completiontime2) shows the time spent on the survey, according to Prolific.
One individual took `r round(min(approved_prolific$'Time taken'/60), 2)` minutes to complete the survey, which is an unusually short amount of time (the next smallest time was `r round(head(sort(approved_prolific$'Time taken'/60))[2], 2)` minutes).
While this completion time is within three standard deviations of the mean (mean = `r round(mean(approved_prolific$'Time taken'/60), 2)`, sd = `r round(sd(approved_prolific$'Time taken'/60), 2)`) and does not fall within Prolific's criterion for rejection, we do not feel that this participant would have had adequate time to appropriately complete the survey, and have removed them from analysis.


```{r}
#| completiontime2,
#| fig.cap= "Prolific Completion Time",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(subset(prolific_data, Status=="APPROVED"), aes(x = `Time taken`/60)) +
    geom_histogram(alpha=0.75) +
  ggtitle("Completion Time") +
  xlab("Completion Time in Minutes")+
  theme_bw()


```

## Results

### Participants

```{r}
#| demographics2,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_complete$age = factor(
  microstudy_complete$age,
  levels = c(
    "18 - 25",
    "26 - 35",
    "36 - 45",
    "46 - 55",
    "56 - 65",
    "Over 65"
  )
)
microstudy_complete$gender = factor(microstudy_complete$gender,
                           levels=c("Other/non-binary", "Man", "Woman"))
# table(microstudy_complete$age)

  ggplot(microstudy_complete,
         aes(x = age, fill = gender)) +
  geom_bar(mapping = aes(y = after_stat(count), group = gender), color="black") +
  #  geom_histogram(stat="count", position="dodge")+
  ggtitle("Age and Gender of Participants") +
  scale_fill_manual(values = c("#E69F00", "#009E73", "#F0E442")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```

Of the `r dim(microstudy_complete)[1]` surveys completed, `r sum(microstudy_complete$gender=="Man")` participants were men, `r sum(microstudy_complete$gender=="Woman")` participants were women, and `r sum(microstudy_complete$gender=="Other/non-binary")` participants identified as other or non-binary. 
The median age category was 46 - 55.
Age and gender are shown in Figure \@ref(fig:demographics2).
Individuals were asked a single attention check question with regards to the caliber of gun used in the attempted robbery.
`r dim(microstudy_clean)[1]` participants passed the attention check, meaning that only `r sum(microstudy_complete$check!="9mm")` participants failed the attention check, and will not be included in the analysis.
There were `r sum(microstudy_clean$conclusion=="Match")` participants in the identification condition and `r sum(microstudy_clean$conclusion=="NoMatch")` participants in the elimination condition who passed the attention check.
We adjusted the probability of receiving each condition throughout the survey to ensure categories were relatively equal.

### Questions from Study 1

```{r echo=FALSE, warning=FALSE, message=FALSE}
merged_results_allresponses<-read_csv("data/study1_merged_results_clean.csv")

merged_results_allresponses <-merged_results_allresponses %>% subset(clean_prints!=231)

merged_results <- merged_results_allresponses %>% filter(check1 =="9mm" & check2 == "Moderately reliable")

match_convict_compare <- prop.test(x=c(sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="Match"),sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")), n=c(sum(microstudy_clean$conclusion=="Match"), sum(merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")),alternative="two.sided")
```

There were some procedural changes made to this study compared to the original study that may influence results: namely, more detailed cross examination and the inclusion of jury instructions.
Another significant difference between the two studies is the time that has passed between the two studies.
The first study was conducted in July 2022, while this study was conducted in January 2024.
In this time, notoriety of firearms evidence has not remained the same - in this response study, a participant commented that they had recently read an article about the unreliability of firearms evidence.
More widespread criticism of firearms examination may lead to lower feelings of reliability.

In both studies, participants are asked if they would choose to convict, given the "beyond a reasonable doubt" threshold.
In this study, `r round(sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="Match")/sum(microstudy_clean$conclusion=="Match")*100, 2)`\% (or `r sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="Match")` out of `r sum(microstudy_clean$conclusion=="Match")`) individuals who received the identification condition chose to convict, while `r round(sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="NoMatch")/sum(microstudy_clean$conclusion=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="NoMatch")` out of `r sum(microstudy_clean$conclusion=="NoMatch")`) participants who received the elimination condition chose to convict.
The conviction percentage for the identification condition in this study is slightly lower than the comparable condition in the original study (without the use of the algorithm and images): `r round(sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")/sum(merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")*100, 2)`\% (or `r sum(merged_results$guilty=="Yes" & merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")` out of `r sum(merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No")`).
However, there is not a significant difference between studies (p-value `r round(match_convict_compare$p.value, 2)`).
None of the `r sum(merged_results$conclusion=="NoMatch" & merged_results$algorithm=="No" & merged_results$picture=="No")` individuals in the original study who received the elimination condition without the use of the algorithm or images chose to convict.

```{r}
#| strength2,
#| fig.cap= "Microstudy Strength of Evidence",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$evidence_strength = factor(
  microstudy_clean$evidence_strength,
  levels = c(
    "1 Not at all strong",
    "2",
    "3",
    "4",
    "5 Moderately strong",
    "6",
    "7",
    "8",
    "9 Extremely strong"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=evidence_strength, fill=conclusion), position="dodge") +
  ggtitle("What is the Strength of Evidence against the Defendant?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition",
                    labels=c("Identification", "Elimination"))+
  ylab("Count")+
  xlab("Strength")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

Figure \@ref(fig:strength2) shows participant responses to the strength of evidence in this study.
As can be seen, those in the elimination category tended to choose the smallest value for the strength of evidence ("Not at all strong"), while in the case of the identification condition, individuals tended to distribute their views of the strength of evidence more evenly.
This graph resembles the strength of evidence results from the initial study, shown in Figure \@ref(fig:strength).
While not as dramatic as the scale compression for questions of reliability, credibility, and scientificity, the elimination results for strength of evidence do appear to show a scale limitation because individuals tended to overwhelmingly select the lowest category.

```{r}
#| prob2,
#| fig.cap= "Probability Cole Committed Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

colors <-  c("Hidden"="red", "Visible"="grey")

conclusion_labs <- c("Identification", "Elimination")
names(conclusion_labs) <- c("Match", "NoMatch")

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=as.numeric(hidden_probability), fill="Hidden")) +
  geom_density(alpha=0.75, aes(x=as.numeric(visible_probability), fill="Visible")) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = colors, name="Probability")+
  ylab("Density")+
  xlab("Probability")+
  facet_grid(.~conclusion, labeller = labeller(conclusion = conclusion_labs))+
  theme_bw()


```

```{r echo=FALSE, warning=FALSE, message=FALSE}

match_prob_ttest<-t.test(microstudy_clean[microstudy_clean$conclusion=="Match",]$visible_probability, merged_results[merged_results$conclusion=="Match" & merged_results$algorithm=="No" & merged_results$picture=="No",]$probability)

nomatch_prob_ttest<-t.test(microstudy_clean[microstudy_clean$conclusion=="NoMatch",]$visible_probability, merged_results[merged_results$conclusion=="NoMatch" & merged_results$algorithm=="No" & merged_results$picture=="No",]$probability)

```

Figure \@ref(fig:prob2) demonstrates the participants' selected probabilities that the defendant committed the crime.
As in the case of the strength of evidence, the graph resembles the probability graph found in the initial study (Figure \@ref(fig:probalgorithm)), where there is a higher peak of extreme values for the elimination conditions than for the identification condition.
When using a t-test to compare the mean visible probabilities for the identification and elimination condition to those comparable conditions of the initial study, there is not a significant difference for the identification condition, but is a significant difference in the elimination condition (p-values of `r round(match_prob_ttest$p.value,2)` and `r round(nomatch_prob_ttest$p.value,2)`, respectively).
The estimated mean is higher in this follow up study (`r round(nomatch_prob_ttest$estimate[1], 2)`\%) than the initial study (`r round(nomatch_prob_ttest$estimate[2], 2)`\%) in the elimination condition.
For the identification condition, the estimated probabilities are `r round(match_prob_ttest$estimate[1], 2)`\% for the follow-up study and `r round(match_prob_ttest$estimate[2], 2)`\% for the initial study.
In this study, the density curves for when the probability was visible or hidden remain similar.

### New Study Questions

In addition to asking participants if they would choose to convict, participants were also asked about their opinion of the guilt of the defendant.
In this case, `r round(sum(microstudy_clean$guilt_opinion=="Yes" & microstudy_clean$conclusion=="Match")/sum(microstudy_clean$conclusion=="Match")*100, 2)`\% (or `r sum(microstudy_clean$guilt_opinion=="Yes" & microstudy_clean$conclusion=="Match")` out of `r sum(microstudy_clean$conclusion=="Match")`) individuals who received the identification condition thought the defendant was guilty, while `r round(sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="NoMatch")/sum(microstudy_clean$conclusion=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$convict=="Yes" & microstudy_clean$conclusion=="NoMatch")` out of `r sum(microstudy_clean$conclusion=="NoMatch")`) participants who received the elimination condition thought the defendant was guilty.
Figure \@ref(fig:opinionguilt) shows the comparison between the participant's decision to convict and their personal opinion.
Approximately half of the participant in the identification condition who chose not to convict thought the defendant was in fact guilty.
This indicates that participants have different thresholds for their personal opinion of guilt compared to the "beyond a reasonable doubt" threshold.
Of the `r sum(microstudy_clean$conclusion=="Match" & microstudy_clean$convict=="Yes")` individuals who chose to convict in the identification condition, `r sum(microstudy_clean$conclusion=="Match" & microstudy_clean$convict=="Yes" & microstudy_clean$guilt_opinion=="No")` participants thought the defendant was in fact innocent.
This discrepancy may be due to participants reversing the strength of the thresholds - they feel that the evidence is strong enough to convict, but are not wholly convinced.


```{r}
#| opinionguilt,
#| fig.cap= "Comparison of opinions of guilt and choice to convict",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=convict, fill=guilt_opinion), 
           position=position_dodge(preserve="single")) +
  ggtitle("") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Convict?")+
  facet_grid(.~conclusion, labeller = labeller(conclusion = conclusion_labs))+
  theme_bw()

```

Figure \@ref(fig:betting) indicates how much participants said they would be willing to bet that the defendant was either guilty or innocent, if provided with \$50.
If the participant indicated that they thought the defendant was innocent, they were asked how much they would be willing to bet that the defendant was innocent, and vice versa.
This figure indicates that most people who thought Cole was innocent in the elimination condition were willing to bet the full amount (`r sum(microstudy_clean$conclusion=="NoMatch" & microstudy_clean$guilt_opinion=="No" & microstudy_clean$betting==50)` out of `r sum(microstudy_clean$conclusion=="NoMatch" & microstudy_clean$guilt_opinion=="No")` participants).
It also appears that people tended to select values multiples of 5 (`r sum(microstudy_clean$betting %in% c(5,10,15,20,25,30,35,40,45,50))` out of `r sum(!is.na(microstudy_clean$betting))`, with `r sum(microstudy_clean$betting==0)` additional individuals selecting 0).
Those in the identification condition selected less extreme values (for both those who thought the defendant was guilty and those who thought the defendant was innocent) than what is seen in those with the elimination condition who thought the defendant was innocent.
In this way, the betting response is similar both to the strength of evidence response and the probability of committing the crime response.

```{r}
#| betting,
#| fig.cap= "If the researchers provided 50 dollars, how much would you be willing to bet?",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_histogram(aes(x=betting, fill=guilt_opinion), 
                 binwidth=5, color="black",
           position = position_dodge(preserve = "single")) +
  ggtitle("How much would you bet that the Defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Bet Amount")+
  xlim(c(0,50))+
  facet_grid(conclusion~., labeller = labeller(conclusion = conclusion_labs))+
  theme_bw()


```

The remaining questions relate to the chance that the defendant committed the crime.
One question, shown in Figure \@ref(fig:fixedlike), allowed participants to select the chance that the defendant committed the crime from a multiple choice scale.
This scale does not provide a linear distance between intervals, but instead changes by multiples of 10 in the denominator (ex. one category is 1 in 10, and the next category is 1 in 100).
The exceptions are the endpoints, which are "Impossible to be guilty" and "Certain to be guilty", as well as the midpoint of 1 chance in 2.
In this case, the scale in the elimination condition does not encounter the ceiling or floor effect that has been seen in previous scales, as participants did not overwhelmingly select the lowest value.
Instead, participants are distributed mainly throughout the lower half of the scale, while those in the identification condition tended to be a little closer to the center.
In both cases, participants are not crowded into a small number of categories.
Thus, the multiple choice chance scale has less scale compression than seen previously in the other response types.
This wider distribution may be the result of decompressing the end points - individuals have a variety of values to select that reflect their judgement of a low probability that the defendant committed the crime, or a low strength of evidence.


```{r}
#| fixedlike,
#| fig.cap= "Multiple Choice Chance",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$chances_fixed = factor(
  microstudy_clean$chances_fixed,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=chances_fixed, fill=conclusion),  
           position = position_dodge(preserve = "single")) +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition",
                    labels = conclusion_labs)+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```


```{r}
#| freelike,
#| fig.cap= "Free Response Chance",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean <- microstudy_clean %>% 
  separate(numeric_chance, c("chance_of", "numerator", "denominator"))

microstudy_clean$num_chance <- as.numeric(microstudy_clean$numerator)/as.numeric(microstudy_clean$denominator)

ggplot(microstudy_clean) +
  geom_density(alpha=0.5, aes(x=num_chance, fill=chance_of), position="dodge") +
  ggtitle("What is the chance that the defendant is...") +
  scale_fill_manual(values = c("#CC79A7" ,"#D55E00"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Chance")+
  facet_grid(.~conclusion, labeller = labeller(conclusion = conclusion_labs))+
  #scale_x_continuous(trans='log10')+
  theme_bw()+
  xlim(c(0,1))



######## Choice of Response format #####################

innocent_chance <- 
  c(dim(microstudy_clean[microstudy_clean$guilt_opinion=="No" &
                           microstudy_clean$chance_of=="innocent",])[1],
    dim(microstudy_clean[microstudy_clean$guilt_opinion=="Yes" &
                           microstudy_clean$chance_of=="innocent",])[1])

opinion_totals <- 
    c(dim(microstudy_clean[microstudy_clean$guilt_opinion=="No",])[1],
    dim(microstudy_clean[microstudy_clean$guilt_opinion=="Yes",])[1])

response_choice <- prop.test(x=innocent_chance, n=opinion_totals)


```

A final question asks individuals to provide a numerical chance that the defendant is either innocent or guilty, depending on the participants' choice (the question defaulted to chances of innocence, but participants had the option to switch the scale).
Their responses were limited so that the numerator was smaller than the denominator, resulting in a range of values from 0 to 1, in most cases.
There were `r sum(microstudy_clean$num_chance>1)` responses with values greater than 1 that will not be considered in this analysis.
These results are shown in Figure \@ref(fig:freelike).
As seen in previous response types, in the case of the elimination condition, individuals gave small values for the chance that the defendant had in fact committed the crime, resulting in a sharp peak.
Alternatively, individuals were less extreme when they expressed their opinion as the chance that the defendant did not commit the crime in the elimination condition, even though the same sentiment is being expressed.
In the case of the identification condition, those who expressed their opinions as either guilt or innocence gave similar results, resulting in symmetry between the two distributions.
As shown in Figure \@ref(fig:opinionchanceplot), those who thought the defendant was innocent were more likely to favor inputting chance values in terms of innocence(`r innocent_chance[1]/opinion_totals[1]`\%, or `r innocent_chance[1]` out of `r opinion_totals[1]`), while those who thought the defendant was guilty were slightly less likely to express chance in terms of innocence (`r innocent_chance[2]/opinion_totals[2]`\% expressed their beliefs in terms of innocence, or `r innocent_chance[2]` out of `r opinion_totals[2]`).
This resulted in a significant difference between the two groups (p-value of `r round(response_choice$p.value, 3)`).

```{r}
#| opinionchanceplot,
#| fig.cap= "Participants who chose to express chance in terms of guilt or innocence, based on their opinion of the guilt of the defendant.",
#| fig.width= 6,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

library(ggmosaic)
ggplot(microstudy_clean)+
  geom_mosaic(aes(x=product(chance_of, guilt_opinion),
                  fill=chance_of))+
  theme_bw()+
  scale_fill_manual(values = c("#CC79A7" ,"#D55E00"), name="Chance of:", labels=c("Guilt","Innocence"))+
  ggtitle("Opinion of Guilt vs. Choice to Express Chance")+
  xlab("Guilty?")+
  theme(axis.text.y = element_blank(), axis.ticks.y=element_blank(),
        axis.title.y = element_blank())

```


```{r}
#| opinionchance,
#| echo= FALSE,
#| message= FALSE,
#| eval= FALSE

library(knitr)
# library(kableExtra)
table(microstudy_clean$chance_of, microstudy_clean$guilt_opinion) %>%
kable(col.names=c("Innocent", "Guilty"), align="lcc",
      caption = "Participants who chose to express Chance in terms of guilt or innocence, based on their opinion of the guilt of the defendant.") %>%
  add_header_above(c("Chances of:"=1,"Opinion"=2), align=c("l","c")) %>%
  kable_styling()

```

<!-- Based on the results shown in Figure \@ref(fig:fixedlike), it seems that the results from this free chance scale may benefit from a transformation of the scale. -->
<!-- The log scale transformation is shown in \@ref(fig:freelike10). -->
<!-- This scale transformation shifted the observations from the far left side of the graph to the right side of the graph. -->
<!-- The distribution of the scale is more spread out than that shown in Figure \@ref(fig:freelike), and may lend itself better to analysis. -->

```{r}
#| freelike10,
#| fig.cap= "Free Response Chance, Log 10 Scale",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=num_chance, fill=guilt_opinion), position="dodge") +
  ggtitle("What is the chance that the defendant is...") +
  scale_fill_manual(values = c("#CC79A7" ,"#D55E00"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Chance")+
  facet_grid(.~conclusion)+
  scale_x_continuous(trans='log10')+
  theme_bw()


```

### Scale Comparison

In addition to resolving scale compression, consistency across response types is another important aspect of this study.
If different question types result in responses that are inconsistent, it would be difficult to tell which questions truly capture the attitudes of the participants, in order to most accurately answer the research questions.
As mentioned in the Chapter \@ref(litreview), there have been studies to suggest that individuals may struggle with the interpretation of chance scales.
Other scales, such as how much a participant is willing to bet, may be highly subjective - depending their personal feel for risk, and betting hypothetical money may have different results than betting real money.
Due to this potential difficulty in interpretation, response scales must be compared.
Because individuals may be influenced by the order of the questions (such as choosing a numeric chance value that aligns with their multiple choice selection), the order was randomized and recorded.

#### Numeric and Multiple Choice Chance Comparison

```{r}
#| likecomp1,
#| fig.cap= "Chance of Guilt Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


set_values <- 
  data.frame(chances_fixed=c("Impossible that he is guilty",
                             "About 1 chance in 10,000",
                             "About 1 chance in 1,000",
                             "About 1 chance in 100",
                             "About 1 chance in 10",
                             "1 chance in 2 (fifty-fifty chance)",
                             "About 9 chances in 10",
                             "About 99 chances in 100",
                             "About 999 chances in 1,000",
                             "About 9,999 chances in 10,000",
                             "Certain to be guilty"),
                         value=c(0,1/10000,1/1000,1/100,1/10,0.5,9/10,99/100,999/1000,9999/10000,1))
clean_results_merged<- dplyr::left_join(microstudy_clean, set_values)
clean_results_merged$chances_fixed = factor(
  microstudy_clean$chances_fixed,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)
ggplot(subset(clean_results_merged, chance_of=="guilty"), aes(x=chances_fixed))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Chances of Guilt") +
  geom_jitter(aes(y=num_chance),
    size = 1
  ) +
  geom_boxplot(aes(y=num_chance),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Chance")+
  xlab("Closed Response Chance")+
  scale_x_discrete(labels = wrap_format(10))+
  ylim(c(0,1))

# Somehow 3 people got values over 1

```


Figure \@ref(fig:likecomp1) shows the comparison of the multiple choice chance responses to the numeric chance responses, in the case that the participants chose to express the numeric chance in terms of guilt.
The red dots represent the actual value corresponding to their selection of the multiple choice chance (for example, the red dot for "About 1 chance in 10" is located at the y-value of 0.10).
This allows for comparison of the consistency between the multiple choice and the numeric scales.
For values of "1 chance in 2", "About 1 chance in 10", and "About 9 chances in 10", the responses seem fairly consistent - however, it is difficult to tell if these values are consistent for more extreme values, because they are compressed on the linear scale.


Figure \@ref(fig:likecomp1scale) demonstrates the lower half of the multiple choice scale, when the x-axis is transformed by log 10.
The lines indicate the numerical equivalent to the multiple choice wording, including a black line for the ungraphed category of "1 chance in 2 (fifty-fifty chance)".
Based on this transformed scale, there is some consistency between participants' numeric chance choice and their multiple choice answer.
While the log transformation makes it difficult to measure distances between values on the density curves and the nearest multiple choice category, it would be safe to assume that there is some correspondence between multiple choice and numeric categories when the numeric value falls in the region contained by the next highest or lowest multiple choice categories.
For example, if an individual selected "About 1 chance in 10" on the multiple choice scale, and selected a numeric chance value between 0.01 and 0.2, there is some correspondence between their multiple choice selection and the numeric value.
However, many individuals did not show consistent choices between scales - particularly in the "About 1 chance in 10,000" condition, where several numerical choices would have better aligned with different selections in the multiple choice section, as demonstrated by the closeness of several observations to other multiple choice chance values.

```{r}
#| likecomp1scale,
#| fig.cap= "Guilt Chance Comparison Log 10 Scale",
#| fig.width= 12,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

# ggplot(subset(clean_results_merged, chance_of=="guilty"), aes(x=chances_fixed))+ #,fill=conclusion
#   geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
#   ggtitle("Chances of Guilt") +
#   geom_jitter(aes(y=num_chance),
#     size = 1
#   ) +
#   geom_boxplot(aes(y=num_chance),
#                position = position_dodge(1),
#                alpha = 0.5,
#                outlier.shape = NA)+ 
#   ylab("Open Response Chance")+
#   xlab("Closed Response Chance")+
#  # scale_y_continuous(trans='reciprocal')+
#   scale_x_discrete(labels = wrap_format(10))+
#   ylim(c(0,1))+
#   scale_y_log10()

clean_results_merged$chance_wo_0 <- clean_results_merged$num_chance

clean_results_merged <- clean_results_merged %>%
    mutate(chance_wo_0 = ifelse(chance_wo_0 == 0, .Machine$double.eps, chance_wo_0))

ggplot(subset(clean_results_merged, chance_of=="guilty" & 
              chances_fixed %in% c("Impossible that he is guilty", 
                                   "About 1 chance in 10,000", 
                                   "About 1 chance in 1,000",
                                   "About 1 chance in 100",
                                   "About 1 chance in 10")), aes(x=chance_wo_0, fill=chances_fixed, y=chances_fixed))+
  ggtitle("Chances of Guilt") +
  ylab("Count")+
  xlab("Open Response Chance")+
  geom_density_ridges(alpha=0.5)+
  scale_fill_manual(values=c("#D81B60","#1E88E5","#FFC107","#004D40", "#CCA785"))+
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position="none")+
  scale_x_log10(breaks=c(1/10000, 1/1000, 1/100, 1/10, 0.5),
                labels=label_number(drop0trailing=TRUE), 
                 limits = c(.Machine$double.eps, NA))+
  geom_vline(aes(xintercept=1/10000), color="#1E88E5")+
  geom_vline(aes(xintercept=1/1000), color="#FFC107")+
  geom_vline(aes(xintercept=1/100), color="#004D40")+
  geom_vline(aes(xintercept=.Machine$double.eps), color="#D81B60")+
  geom_vline(aes(xintercept=1/10), color="#CCA785")+
  geom_vline(aes(xintercept=0.5), color="black")+
  scale_y_discrete(labels = wrap_format(15))

```

Figure \@ref(fig:likecomp1scalerev) demonstrates the upper half of the multiple choice scale.
In order to visualize these compressed values, the numerical chance selected by participants was subtracted from 1 and a log 10 transformation was used on the x-axis. 
Thus, values can be interpreted as the chance that the defendant was innocent, even though participants chose to express their values in terms of guilt.
The lines indicate the numerical equivalent to the multiple choice wording, if expressed in terms of innocence.
While those selecting "About 9 chances in 10" were fairly consistent with the expected value, there is a fair amount of inconsistency between participants' multiple choice selection and their numerical values for all other categories.
Those who selected "Certain to be guilty" tended to choose guilt values of 1 (or, alternatively, chance of innocence values of 0), as expected.
Other numerical values tended to cluster around 0.01 - 0.1 as the chance of innocence (individuals selected values between 0.9 and 0.99 in their numerical calculation of guilt).
For values aside from "About 9 chances in 10", this trend indicates that participants tended to provide numerical values of guilt that were lower than the values they indicated on the multiple choice scale.

```{r}
#| likecomp1scalerev,
#| fig.cap= "Guilt Chance Comparison Log 10 Scale, where values were subtracted from 1",
#| fig.width= 12,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

clean_results_merged$chance_subtracted <- 1 - clean_results_merged$num_chance

clean_results_merged <- clean_results_merged %>%
    mutate(chance_subtracted = ifelse(chance_subtracted == 0, .Machine$double.eps, chance_subtracted))

ggplot(subset(clean_results_merged, chance_of=="guilty" & 
              chances_fixed %in% c("Certain to be guilty", 
                                   "About 9,999 chances in 10,000", 
                                   "About 999 chances in 1,000",
                                   "About 99 chances in 100",
                                   "About 9 chances in 10")), aes(x=chance_subtracted, fill=chances_fixed, y=chances_fixed))+
  ggtitle("Chances of Innocence") +
  ylab("Count")+
  xlab("1 - Open Response Chance of Guilt")+
  geom_density_ridges(alpha=0.5)+
  scale_fill_manual(values=c("#CC79A7","#D55E00","#009E73","#F0E442", "#A4A3B3"))+
  theme_bw()+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position="none")+
   scale_x_log10(breaks=c(1/10000, 1/1000, 1/100, 1/10, 0.5),
                 labels=label_number(drop0trailing=TRUE), 
                 limits = c(.Machine$double.eps, NA))+
  geom_vline(aes(xintercept=1/10000), color="#F0E442")+
  geom_vline(aes(xintercept=1/1000), color="#009E73")+
  geom_vline(aes(xintercept=1/100), color="#D55E00")+
  geom_vline(aes(xintercept=.Machine$double.eps), color="#A4A3B3")+
  geom_vline(aes(xintercept=1/10), color="#CC79A7")+
  geom_vline(aes(xintercept=0.5), color="black")+
  scale_y_discrete(labels = wrap_format(15))

```

This same procedure can be repeated for those who thought the defendant was guilty, as shown in Figure \@ref(fig:likecomp2).
In this case, because individuals chose to supply the chance that the defendant did not commit the crime, the multiple choice chance indicators were reversed by subtracting the given value from 1.
There appears to be some confusion on the question wording in this case, as demonstrated by numerous responses in the "About 1 chance in 1,000" and "About 1 chance in 100" categories that appear as low chance values - indicating that individuals selected a low chance of innocence when given the numerical format, but a high chance of innocence when given the multiple choice question.
Most median numeric values on the left half of the multiple choice scale do not align with the translated value from the multiple choice scale.
The reduced consistency seen in these graphs may related to the change in the question wording, where the multiple choice question considers the chance of guilt, while their numeric chance considered the chance of innocence.
Most individuals who had inconsistent responses between the numeric and multiple choice chance question thought that the defendant was innocent, indicating that the multiple choice scale is more consistent with their beliefs than their numeric selections.

```{r}
#| likecomp2,
#| fig.cap= "Chance of Innocence Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(subset(clean_results_merged, chance_of=="innocent"), aes(x=chances_fixed))+ #,fill=conclusion
  geom_point(aes(y=1-value),color="red",size=5,alpha=0.5)+
  ggtitle("Chances of Innocence") +
  geom_jitter(aes(y=num_chance, color=guilt_opinion),
    size = 1
  ) +
  geom_boxplot(aes(y=num_chance),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Numeric Chance of Innocence")+
  xlab("Chance of Guilt")+
  scale_x_discrete(labels = wrap_format(10))+
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylim(c(0,1))

```

This apparent confusion is reinforced in Figure \@ref(fig:likecomp2scale), displaying the values corresponding to the lower half of the multiple choice scale. 
As in Figure \@ref(fig:likecomp1scalerev), I have reversed the numeric values so a log transformation can be used to better visualize the data.
While both graphs show inconsistency in responses, leading to a clustering of density distributions, in this case the effect is more severe.
The only multiple choice value that shows some consistency with the numeric counterpoints is "About 1 chance in 10", while for all other multiple choice options participants tended to provide a lower chance of numerical innocence than their multiple choice selection would indicate (shown by values further right on the scale).
In two cases, for those who selected "About 1 chance in 100" and "About 1 chance in 1,000" of guilt, the most popular category is above a fifty-fifty chance, indicating confusion on the scale.
Those who selected "Impossible that he is guilty" also tended to select chance of guilt values at 0, also demonstrating consistency (although several individuals also chose "Impossible that he is guilty" and assigned higher chances of guilt).

```{r}
#| likecomp2scale,
#| fig.cap= "Innocence Chance Comparison Log 10 Scale, where values were subtracted from 1",
#| fig.width= 12,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

# ggplot(subset(clean_results_merged, chance_of=="innocent"), aes(x=chances_fixed))+ #,fill=conclusion
#   geom_point(aes(y=1-value),color="red",size=5,alpha=0.5)+
#   ggtitle("Chances of Innocence") +
#   geom_jitter(aes(y=num_chance),
#     size = 1
#   ) +
#   geom_boxplot(aes(y=num_chance),
#                position = position_dodge(1),
#                alpha = 0.5,
#                outlier.shape = NA)+ 
#   ylab("Open Response Chance")+
#   xlab("Closed Response Chance")+
#   scale_x_discrete(labels = wrap_format(10))+
#   scale_y_continuous(trans="sqrt")+
#   ylim(c(0,1))

ggplot(subset(clean_results_merged, chance_of=="innocent" & 
              chances_fixed %in% c("Impossible that he is guilty", 
                                   "About 1 chance in 10,000", 
                                   "About 1 chance in 1,000",
                                   "About 1 chance in 100",
                                   "About 1 chance in 10")),
       aes(x=chance_subtracted, 
           fill=chances_fixed, y = chances_fixed))+
  geom_density_ridges(alpha=0.5)+
  ggtitle("Chances of Guilt") +
  ylab("Count")+
  xlab("1 - Open Response Chance of Innocence")+
  scale_fill_manual(values=c("#D81B60","#1E88E5","#FFC107","#004D40", "#CCA785"))+
  theme_bw()+
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position="none")+
   scale_x_log10(breaks=c(1/10000, 1/1000, 1/100, 1/10, 0.5),
                 labels=label_number(drop0trailing=TRUE), 
                 limits = c(.Machine$double.eps, NA))+
  geom_vline(aes(xintercept=1/10000), color="#1E88E5")+
  geom_vline(aes(xintercept=1/1000), color="#FFC107")+
  geom_vline(aes(xintercept=1/100), color="#004D40")+
    geom_vline(aes(xintercept=.Machine$double.eps), color="#D81B60")+
  geom_vline(aes(xintercept=1/10), color="#CCA785")+
  geom_vline(aes(xintercept=0.5), color="black")

```

Interestingly, this inconsistency did not hold as strongly for the upper part of the multiple choice chance scale, shown in Figure \@ref(fig:likecomp2scalerev).
Here, lines indicate the appropriate level chance of innocence corresponding to the multiple choice scale of guilt.
In this case, the bottom two categories ("About 9 out of 10" and "About 99 out of 100") correspond quite well between the multiple choice answer and the numeric counterpart.
Although this consistency is not as present in the other categories, due to a larger variety of inputs, they are closer to the ballpark than what is shown in Figure \@ref(fig:likecomp2scale).
None of the individuals who selected "Certain to be guilty" on the multiple choice scale chose the corresponding 0 chance value of innocence, instead largely choosing values consistent with "About 99 chances in 100".

Overall, individuals tended to provide more consistent responses when evaluating numeric chance in terms of guilt instead of in terms of innocence.
While the upper parts of both innocent and guilty scales demonstrated less consistency than the lower parts of the scales, the trends for those who expressed chance in terms of guilt more closely followed their multiple choice selections.


```{r}
#| likecomp2scalerev,
#| fig.cap= "Innocence Chance Comparison Log 10 Scale",
#| fig.width= 12,
#| fig.height= 5,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(subset(clean_results_merged, chance_of=="innocent" & 
              chances_fixed %in% c("Certain to be guilty", 
                                   "About 9,999 chances in 10,000", 
                                   "About 999 chances in 1,000",
                                   "About 99 chances in 100",
                                   "About 9 chances in 10")), aes(x=chance_wo_0, fill=chances_fixed, y=chances_fixed))+
    geom_density_ridges(alpha=0.5)+
  ggtitle("Chances of Innocence") +
  ylab("Count")+
  xlab("Open Response Chance of Innocence")+
  scale_fill_manual(values=c("#CC79A7","#D55E00","#009E73","#F0E442", "#A4A3B3"))+
  theme_bw()+
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position="none")+
   scale_x_log10(breaks=c(1/10000, 1/1000, 1/100, 1/10, 0.5),
                 labels=label_number(drop0trailing=TRUE), 
                 limits = c(.Machine$double.eps, NA))+
  geom_vline(aes(xintercept=1/10000), color="#F0E442")+
  geom_vline(aes(xintercept=1/1000), color="#009E73")+
  geom_vline(aes(xintercept=1/100), color="#D55E00")+
    geom_vline(aes(xintercept=.Machine$double.eps), color="#A4A3B3")+
  geom_vline(aes(xintercept=1/10), color="#CC79A7")+
  geom_vline(aes(xintercept=0.5), color="black")

```

```{r}
#| onevaluesmosaic,
#| fig.cap= "Participants who selected a chance of 1",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE

ggplot(subset(clean_results_merged, num_chance==1))+
  geom_bar(aes(x=chances_fixed, fill=chance_of))+
  theme_bw()+
  scale_fill_manual(values = c("#CC79A7" ,"#D55E00"), name="Chance of:", labels=c("Guilt","Innocence"))+
  ggtitle("Correspondence Between Chances of 1 and the Multiple Choice Scale")+
  xlab("Guilty?")+
    scale_x_discrete(labels = wrap_format(10))

```

#### Probability and Chance Scale Comparison

The comparison of multiple choice chance and visible probability scales is shown in Figure \@ref(fig:likeprob).
In the case of the probabilities, individuals were only able to select integers between 0 and 100, meaning that this scale would not translate to the more extreme values of the multiple choice chance scale (outside of the values of "About 1 in 100" and "About 99 in 100").
While there is some discrepancy between choice selection, particularly in participants who selected a lower chance that the defendant is guilty, responses seem to overall be consistent.
Probability values tend toward the middle, pulling away from the extreme values on the probability scale that would correspond to their multiple choice chance answers.
Those who selected high probability values but low multiple choice chance values expressed that their opinion was that the defendant was guilty.
This indicates that their selected probability value is more consistent with their opinion than the multiple choice chance value.
In this case, it seems that a few individuals may have found the use of the chance scale to be more confusing than the probability scale.

```{r}
#| likeprob,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=chances_fixed))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Probability vs. Chance That Cole is Guilty") +
  geom_jitter(aes(y=(as.numeric(hidden_probability)/100), color=guilt_opinion),
             # position = position_jitterdodge(
             #   jitter.width = 0.2,
             #   #jitter.height = 0.4,
             #   dodge.width = 1
             # ),
             size = 1
  ) +
  geom_boxplot(aes(y=(as.numeric(hidden_probability)/100)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Hidden Probability")+
  xlab("Closed Response Chance")+
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  scale_x_discrete(labels = wrap_format(10))

```

```{r}
#| probchancescale1,
#| fig.cap= "Guilt Probability Comparison Log 10 Scale",
#| fig.width= 10,
#| fig.height= 9,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

clean_results_merged$prob_wo_0 <- as.numeric(clean_results_merged$visible_probability)/100

clean_results_merged <- clean_results_merged %>%
    mutate(prob_wo_0 = ifelse(prob_wo_0 == 0, .Machine$double.eps, prob_wo_0))

ggplot(subset(clean_results_merged, 
              chances_fixed %in% c("Impossible that he is guilty", 
                                   "About 1 chance in 10,000", 
                                   "About 1 chance in 1,000",
                                   "About 1 chance in 100",
                                   "About 1 chance in 10")), aes(x=prob_wo_0, fill=chances_fixed))+
  ggtitle("Probability of Guilt") +
  ylab("Count")+
  xlab("Visible Probability")+
  geom_histogram(alpha=0.5)+
  facet_grid(chances_fixed~.)+
  scale_fill_manual(values=c("#D81B60","#1E88E5","#FFC107","#004D40", "#CCA785"))+
  theme_bw()+
  scale_x_log10(breaks=c(1/10000, 1/1000,
                              1/100, 1/10, 0.5), labels=label_number(drop0trailing=TRUE))+
  geom_vline(aes(xintercept=1/10000), color="#1E88E5", size=1)+
  geom_vline(aes(xintercept=1/1000), color="#FFC107", size=1)+
  geom_vline(aes(xintercept=1/100), color="#004D40", size=1)+
  geom_vline(aes(xintercept=.Machine$double.eps), color="#D81B60", size=1)+
  geom_vline(aes(xintercept=1/10), color="#CCA785", size=1)+
  geom_vline(aes(xintercept=0.5), color="black", size=1)

# sort(table(clean_results_merged$visible_probability))

```

Figure \@ref(fig:probchancescale1) shows the log-transformed comparison of probability values and the participant's multiple choice chance decision. 
If participants were consistent in their choices of chance category and visible probability, we would expect that individuals selecting "About 1 chance in 1,000" or lower would exclusively select probability of guilt values of 1\% or 0\%, because 1\% corresponds to "About 1 chance in 100".
While most individuals in the "Impossible he is guilty" category did select 0\%, this prediction does not hold true for the other categories.
In the case of "About 1 chance in 10,000", approximately an equal number of individuals selected values between 1\% and 10\%, as did those who selected the "About 1 chance in 1,000" and "About 1 chance in 100" categories.
While this is expected in "About 1 chance in 100", the estimate appears to be high for "About 1 chance in 1,000" and "About 1 chance in 10,000".
Those in the "About 1 chance in 10" category selected relatively expected probabilities, demonstrating that this issue mostly affects the extreme values.
While 50\% was the most popular (and default) value on the visible probability scale (selected by 33 participants), the next two most popular values are 0\% and 1\% (both selected by 21 participants).

```{r}
#| probchancescale2,
#| fig.cap= "Guilt Probability Comparison Log 10 Scale, where probability values were subtracted from 1",
#| fig.width= 10,
#| fig.height= 9,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

clean_results_merged$prob_subtracted <- 1 - (clean_results_merged$visible_probability/100)

clean_results_merged <- clean_results_merged %>%
    mutate(prob_subtracted = ifelse(prob_subtracted == 0, .Machine$double.eps, prob_subtracted))

ggplot(subset(clean_results_merged,
              chances_fixed %in% c("Certain to be guilty", 
                                   "About 9,999 chances in 10,000", 
                                   "About 999 chances in 1,000",
                                   "About 99 chances in 100",
                                   "About 9 chances in 10")), aes(x=prob_subtracted, fill=chances_fixed))+
  ggtitle("Probability of Innocence") +
  ylab("Count")+
  xlab("1 - Visible Probability of Guilt")+
  geom_histogram(alpha=0.5)+
  facet_grid(chances_fixed~.)+
  scale_fill_manual(values=c("#CC79A7","#D55E00","#009E73","#F0E442", "#A4A3B3"))+
  theme_bw()+
   scale_x_log10(breaks=c(1/10000, 1/1000,
                              1/100, 1/10, 0.5), labels=label_number(drop0trailing=TRUE))+
  geom_vline(aes(xintercept=1/10000), color="#F0E442", size=1)+
  geom_vline(aes(xintercept=1/1000), color="#009E73", size=1)+
  geom_vline(aes(xintercept=1/100), color="#D55E00", size=1)+
  geom_vline(aes(xintercept=.Machine$double.eps), color="#A4A3B3", size=1)+
  geom_vline(aes(xintercept=1/10), color="#CC79A7", size=1)+
  geom_vline(aes(xintercept=0.5), color="black", size=1)

```

Figure \@ref(fig:probchancescale2) is similar to Figure \@ref(fig:probchancescale1) in its less-frequent-than-expected choice of extreme values, and the accurate alignment of the most extreme ("Certain to be guilty") and least extreme ("About 9 chances in 10") categories.
Those who selected "About 99 chances in 100" of guilt tended to favor close to a 10\% probability of innocence (by selecting a 90\% probability of guilt), as opposed to the expected 1\% probability of innocence (or 99\% probability of guilt); the same can be said for those who selected "About 999 chances in 1,000" that the defendant is guilty.
Individuals who selected the "About 9,999 chances in 10,000" favored 99\% probability of guilt (or 1\% probability of innocence).

Overall, Figures \@ref(fig:probchancescale1) and \@ref(fig:probchancescale2) demonstrate some inconsistency between selected multiple choice scale and participants' assigned probability.
Results in both cases generally indicate consistent beliefs in guilt: By far most individuals who selected a multiple choice chance value corresponding to the defendant being more likely to be guilty than innocent selected a corresponding probability value in the same range (i.e. less than a 50\% probability of being innocent), and vice versa.
However, the inconsistency in the scales is present in the form of magnitude: participants tended to select less extreme probability values than was indicated by their multiple choice chance selections.


```{r}
#| likeprobcon,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

chanceslope <- data.frame(slopes = c(100, -100), intercepts = c(0, 100), chance_of = c("guilty", "innocent"))

ggplot(microstudy_clean, aes(x=num_chance, color=guilt_opinion, y=visible_probability)) +
  geom_jitter(alpha=0.5) +
  ggtitle("Probability vs Chance") +
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion",
                     labels=c("Innocent","Guilty"))+
  ylab("Visible Probability")+
  xlab("Chance")+
  facet_grid(.~chance_of, labeller = "label_both")+
  theme_bw()+
  xlim(c(0,1))+
  geom_abline(aes(intercept=intercepts, slope=slopes), chanceslope)

```

Figure \@ref(fig:likeprobcon) shows a comparison of the perceived probability that the defendant committed the crime compared to the perceived chance that the defendant is guilty, when expressed on a continuous scale.
The plot lines would indicate a direct correspondence between chance value and probability
Values are clustered near the corresponding corners of each plot.
This may be due to the compression on the probability scale - values of chance that are more extreme than 1 in 100 or 99 in 100 are represented with a single probability value.
Most responses that do not follow the line are below it when participants chose to express chance of innocence.
The clustering of low chance of innocence and low probability of guilt shown in the bottom left of the plot indicate participants who were confused about the chance scale - their opinion indicated that they thought the defendant was innocent, yet they assigned a low chance of innocence in the numerical scale.
The correlation between the probability and the chances of guilt is `r round(cor(microstudy_clean[!is.na(microstudy_clean$num_chance) & microstudy_clean$chance_of=="guilty",]$visible_probability, microstudy_clean[!is.na(microstudy_clean$num_chance) & microstudy_clean$chance_of=="guilty",]$num_chance),2)`,
while the correlation between the probability and the chances of innocence is `r round(cor(microstudy_clean[!is.na(microstudy_clean$num_chance) & microstudy_clean$chance_of=="innocent",]$visible_probability, microstudy_clean[!is.na(microstudy_clean$num_chance) & microstudy_clean$chance_of=="innocent",]$num_chance),2)`.
While there is a fairly strong correlation in the case of chances of guilt and probability, the correlation between the probability and the chances of innocence is weaker.
This may be a result of confusion in the scale reversal of the chance of innocence scale, while the chance of guilt smoothly corresponds with expressing the probability of guilt.

<!-- Figure \@ref(fig:likeconvict) demonstrates the relationship between chance, probability, and the decision to convict. -->
<!-- Those who chose to convict tended to select high probabilities -->

```{r}
#| likeconvict,
#| fig.cap= "Probability and Conviction Decision",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= FALSE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean, aes(x=num_chance, color=convict, y=visible_probability)) +
  geom_jitter(alpha=0.75) +
  ggtitle("Probability vs Chance") +
  scale_color_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Chance of Committing Crime")+
  facet_grid(.~chance_of, labeller = "label_both")+
  theme_bw()+
  xlim(c(0,1))

```

Figure \@ref(fig:coordstrcat) shows the relationship between individuals' ratings of the strength of evidence, and the categorical selection of the chance that the defendant committed the crime.
While strength of evidence is on a 9-point Likert scale, the multiple choice chance scale has 11 values, giving participants more selection.
The left graph is for the identification condition, while the right graph is for the elimination condition.
On the y-axis, lower values correspond to weaker strength of evidence and a lower chance of committing the crime, while higher values correspond to stronger strength of evidence and a higher chance of committing the crime.
In the case of the identification condition, individuals in the strength condition selected various values for the chance that the defendant committed the crime, indicating that participants do not interpret the chances of committing the crime as the same amount of strength of evidence uniformly.
Responses for the identification condition are also spread across many values on the scale, consistent with what we saw in the strength of evidence and multiple choice chance graphs.
In the case of the elimination condition, most individuals chose the smallest level for the strength of evidence.
However, participant responses diverged in terms of the multiple choice chance that the defendant committed the crime: values corresponding to the lowest strength of evidence span four categories for the multiple choice chance, ranging from "Impossible that he is guilty" to "About 1 chance in 100".
The most popular categories corresponding to the weakest level of evidence strength were "About 1 chance in 10,000" and "About 1 chance in 100".
Similar to the relationship between probability and numeric chance, this graphical comparison indicates compression on the strength of evidence scale.
The lowest strength of evidence category can be mapped to several different values for the chance that the defendant committed the crime.


```{r}
#| coordstrcat,
#| fig.cap= "Plots of perceived strength of evidence and categorical likelihood",
#| fig.width= 7,
#| fig.height= 6,
#| fig.align= "center",
#| echo= FALSE,
#| message= FALSE,
#| warning= FALSE

library(GGally)

strlike_tab<-table(microstudy_clean$evidence_strength, microstudy_clean$chances_fixed, microstudy_clean$conclusion)
strlike_df<-as.data.frame(strlike_tab)
colnames(strlike_df) <- c("Strength", "Chances", "Conclusion", "Freq")


ggparcoord(data=subset(strlike_df, Freq != 0), columns=c("Strength","Chances"), order=c(1,2,3,4,5,6,7,8,9), scale="globalminmax")+
  geom_line(aes(linewidth=Freq, alpha=Freq))+ 
  ggtitle("Strength of Evidence Vs Chances of Committing the Crime")+
  facet_grid(.~Conclusion, drop=FALSE, labeller=label_both)

```

Figure \@ref(fig:convictlike) depicts the results for our study comparing the chances that the defendant committed a crime to the decision to convict.
Participants cross the threshold of being more likely to convict when they select "About 9 chances in 10" in the identification condition, with roughly equal numbers above "About 9 chances in 10" in the elimination condition.
This indicates that participants set a threshold of "beyond a reasonable doubt" to be at "About 9 chances in 10".
These results are consistent with @thompsonJurorsGiveAppropriate2013, who found a threshold of about 9 chances in 10, where participants who selected 9 chances in 10 or a higher chance amount were more likely to select a guilty verdict, while those below 9 chances in 10 were more likely to select a not guilty verdict.


```{r}
#| convictlike,
#| fig.cap= "Multiple Choice Chance and Conviction Choice",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=chances_fixed, fill=convict), position=position_dodge(preserve = "single")) +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  facet_grid(conclusion~.)+
  scale_x_discrete(labels = wrap_format(10))


```

Another threshold that we consider in this study is participants' personal feelings of the guilt of the defendant.
We would overall expect the threshold for this opinion to be lower than the "beyond a reasonable doubt" threshold used in their conviction decision.
Figure \@ref(fig:opinionlike) demonstrates the relationship between the chance scale and the participant's opinion of the guilt of the defendant.
In this case, individuals generally thought the defendant committed the crime if they were above the "fifty-fifty chance" value for the identification condition, showing a lower threshold for their opinion of guilt than seen in Figure \@ref(fig:convictlike), when they were asked for their conviction choice.
Individuals who selected the "fifty-fifty chance" appear to be fairly evenly split on their opinion of the guilt of the defendant in the identification condition.
This is to be expected when the participant believes that there is an approximately equal chance that the defendant committed the crime, but are asked to make a binary decision.

```{r}
#| opinionlike,
#| fig.cap= "Multiple Choice Chance and Opinion of Guilt",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=chances_fixed, fill=guilt_opinion), position=position_dodge(preserve = "single")) +
  ggtitle("What is the Chance that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Guilty?")+
  ylab("Count")+
  xlab("Chance")+
  theme_bw()+
  facet_grid(conclusion~.)+
  scale_x_discrete(labels = wrap_format(10))


```

#### Probability Comparison

Figure \@ref(fig:probcomp) shows the correspondence between the visible and hidden probabilities.
The values appear to be fairly 1:1.
In the visible probability scale, there is a slight trend of values clustering on multiples of 5 that is not present in the hidden probability scale.
For the identification condition, those who thought the defendant was guilty tended to select probability values above 50/%.
The correlation between the probabilities is `r round(cor(as.numeric(microstudy_clean$hidden_probability), microstudy_clean$visible_probability, use="complete.obs"),2)`, indicating a strong relationship between the hidden and visible scales.

```{r}
#| probcomp,
#| fig.cap= "Probability and Chance Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x=as.numeric(hidden_probability), color=guilt_opinion, y=visible_probability)) +
  geom_jitter(alpha=0.5) +
  ggtitle("Visible and Hidden Probability Comparison") +
  scale_color_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Visible Probability")+
  xlab("Hidden Probability")+
  scale_y_continuous(breaks = seq(0, 100, by = 10))+
  facet_grid(.~conclusion)+
  coord_fixed(ratio=1)+
  geom_abline(intercept = 0, slope = 1)+
  theme_bw()

```

Figure \@ref(fig:probbet) compares the amount that individuals were willing to bet to their hidden probability that the defendant had/had not committed the crime.
Whether the participants were betting on the guilt or innocence of the defendant depended on their personal opinion of guilt - those who thought the defendant was guilty bet on the defendant's guilt, while those who thought the defendant was innocence bet on the defendant's innocence.
Values are clustered at two points.
For those who thought the defendant was innocent, they were willing to bet almost all of the \$50 and assigned a low probability to the defendant committing the crime.
For those who thought the defendant was guilty, they were also willing to bet almost all of the \$50, but assigned a high probability to the defendant committing the crime.
There is no clear pattern to individuals who fell outside of these two extreme points.
The correlation is `r round(cor(microstudy_clean[!is.na(microstudy_clean$betting) & microstudy_clean$guilt_opinion=="Yes",]$visible_probability, microstudy_clean[!is.na(microstudy_clean$betting) & microstudy_clean$guilt_opinion=="Yes",]$betting),2)` in the case that the participants were asked to bet on if the defendant committed the crime, and `r round(cor(microstudy_clean[!is.na(microstudy_clean$betting) & microstudy_clean$guilt_opinion=="No",]$visible_probability, microstudy_clean[!is.na(microstudy_clean$betting) & microstudy_clean$guilt_opinion=="No",]$betting),2)` when participants were asked to bet on if the defendant did not commit the crime.
These correlations are lower than both correlations between numeric chance and probability.
Based on the graph, this may indicate that both the probability scale and the betting scale suffer from compression in the most extreme values, with little correspondence on the remainder of the scale.


```{r}
#| probbet,
#| fig.cap= "Betting and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x=betting,
                             y=as.numeric(hidden_probability))) +
  # geom_jitter(alpha=0.75) +
  stat_density_2d(aes(fill = ..density..), geom = "raster", contour = FALSE) +
  ggtitle("Betting and Probability Comparison") +
  scale_color_manual(values = c("#004D40","#D81B60"), name="Bet Defendant is...", labels=c("Innocent","Guilty"))+
  # scale_fill_viridis_c(option = "magma") +
  scale_fill_distiller(palette=3, direction=1)+
  ylab("Visible Probability")+
  xlab("Amount (Dollars)")+
  scale_y_continuous(breaks = seq(0, 100, by = 10))+
  facet_grid(.~guilt_opinion, labeller="label_both")+
  xlim(c(0,50))+
  theme_bw()

# Log transformed contours - maybe polygon approach

```

@mattijssen2020 compared likelihood and strength of evidence ratings from 10 examiners who regularly used likelihood scales.
While this sample size is rather small, they found that examiners tended to give higher verbal degrees of support than they should have, based on their likelihood ratio.
A similar effect can be investigated in the potential jurors of this study, when asked to rate the strength of evidence in the case as compared to their perceived probability that the defendant committed the crime.

Figure \@ref(fig:strengthprob) shows the relationship between the strength of evidence compared to the probability that the defendant committed the crime.
In the identification condition, there is a generally increasing correspondence between the perceived strength of evidence and the participants' predicted probability that the defendant committed the crime.
This correspondence is less clear in the elimination condition, perhaps due to scale compression.

```{r}
#| strengthprob,
#| fig.cap= "Strength of Evidence and Numerical Chance of Committing the Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean, aes(x = evidence_strength, y = visible_probability, fill = guilt_opinion)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Strength of Evidence") +
  facet_grid(conclusion~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion: Guilty?")+
  ylab("Visible Probability")+
  xlab("Strength of Evidence")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

### Demographic Comparison

Demographic information, such as income, race, gender, and education level, was collected on the participants.
This demographic information can be compared with the participants' responses.

#### Income
Figure \@ref(fig:probincome) explores a potential relationship between income and believed probability that the defendant committed the crime.
There does not appear to be a relationship between the two variables.

```{r}
#| probincome,
#| fig.cap= "Income and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$income = factor(
  microstudy_clean$income,
  levels = c(
    "Less than $10,000",
    "$10,000 - $19,999",
    "$20,000 - $29,999",
    "$30,000 - $39,999",
    "$40,000 - $49,999",
    "$50,000 - $59,999",
    "$60,000 - $69,999",
    "$70,000 - $79,999",
    "$80,000 - $89,999",
    "$90,000 - $99,999",
    "$100,000 - $149,999",
    "More than $150,000"
  )
)

ggplot(microstudy_clean, aes(x = income, y = visible_probability, fill = convict)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Income") +
  facet_grid(conclusion~.)+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Income")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Bases on Figure \@ref(fig:convictsincome), some individuals in the match condition chose to convict across all income levels.
There does not appear to be a clear trend between conviction choice and income.
Several income brackets chose to convict over the choice not to convict, but these brackets are spread throughout the scale.
In the case of the elimination condition, the individuals who did choose to convict came from different income brackets.

```{r}
#| convictsincome,
#| fig.cap= "Income and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_bar(aes(x=income, fill=convict), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Income")+
  facet_grid(conclusion~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

One may expect income to make a difference in how much individuals choose to bet on their opinion in the case, either in the amount of risk an individual is willing to take or in their desire to keep hypothetical money.
This does not appear to be the case, however, based on Figure \@ref(fig:incomebet).
As discussed earlier, many participants in the elimination condition were willing to bet the full amount that the defendant was innocent.
This is reflected across most income categories as well.
In the case of the identification condition, we saw earlier that participants were not as extreme in their bets as is seen in the elimination condition.
This trend is also reflected in the boxplots, with a fairly similar distribution of both the innocent and guilty bets across all income categories.

```{r}
#| incomebet,
#| fig.cap= "Income and Betting Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x = income, y = betting, fill = guilt_opinion)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Amount Bet Compared with Income") +
  facet_grid(conclusion~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Bet Defendant is...", labels=c("Guilty","Innocent"))+
  ylab("Visible Probability")+
  xlab("Income")+
  scale_x_discrete(labels = wrap_format(10))+
  ylim(c(0,50))+
  theme_bw()

```

#### Education

Figure \@ref(fig:probeduc) shows the education level along with the participants' probability that the defendant committed the crime.
In this case, there does not appear to be a relationship between education level and probability, as the boxplots for probability are similarly distributed across education levels.

```{r}
#| probeduc,
#| fig.cap= "Education Level and Probability Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$education = factor(
  microstudy_clean$education,
  levels = c(
    "Less than high school",
    "High school graduate",
    "Some college",
    "2 year degree",
    "4 year degree",
    "Professional degree",
    "Doctorate"
  )
)

ggplot(microstudy_clean, aes(x = education, y = visible_probability, fill = convict)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Probability Cole Commited the Crime Compared with Education") +
  facet_grid(conclusion~.)+
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Visible Probability")+
  xlab("Education")+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Figure \@ref(fig:convictseduc) shows the relationship between the decision to convict and education level.
As before, there doesn't appear to be much of a relationship between education level and conviction decision.
In the identification condition, individuals chose to convict across all educational categories.

```{r}
#| convictseduc,
#| fig.cap= "Education and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_bar(aes(x=education, fill=convict), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Education")+
  facet_grid(conclusion~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

Figure \@ref(fig:educbet) shows the relationship between the betting and the education level.
There does not appear to be a trend between the education level and the amount that participants were willing to bet.
Based on these graphs, it does not appear that there is a relationship between education level and responses.

```{r}
#| educbet,
#| fig.cap= "Education and Betting Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean, aes(x = education, y = betting, fill = guilt_opinion)) +
  geom_point(
    position = position_jitterdodge(
      jitter.width = 0.2,
      jitter.height = 0.4,
      dodge.width = 1
    ),
    size = 0.5
  ) +
  geom_boxplot(position = position_dodge(preserve = "single"),
               alpha = 0.5,
               outlier.shape = NA) +
  ggtitle("Amount Bet Compared with Education") +
  facet_grid(conclusion~.)+
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Bet Defendant is...", labels=c("Guilty","Innocent"))+
  ylab("Visible Probability")+
  xlab("Education")+
  scale_x_discrete(labels = wrap_format(10))+
  ylim(c(0,50))+
  theme_bw()

```

<!-- #### Order of the Questions -->

<!-- The order of the questions was assigned through the use of a random sampler, which was re-sampled if the questions of the chance or guilt or the questions regarding probability were placed side by side. -->
<!-- This is because it is possible that the participants would be influenced by the values they chose on the first scale when choosing a value for the second scale. -->

<!-- ```{r} -->
<!-- #| echo= FALSE, -->
<!-- #| eval= TRUE, -->
<!-- #| warning= FALSE, -->
<!-- #| message= FALSE -->

<!-- microstudy_clean$first_chance <- NA -->
<!-- microstudy_clean$first_probability<- NA -->

<!-- microstudy_clean$freechance_position <- unlist(gregexpr("2", microstudy_clean$order)) -->
<!-- microstudy_clean$fixedchance_position <- unlist(gregexpr("3", microstudy_clean$order)) -->
<!-- microstudy_clean$probhide_position <- unlist(gregexpr("5", microstudy_clean$order)) -->
<!-- microstudy_clean$probvis_position <- unlist(gregexpr("6", microstudy_clean$order)) -->

<!-- microstudy_clean[microstudy_clean$freechance_position < microstudy_clean$fixedchance_position,]$first_chance <- "Numeric" -->

<!-- microstudy_clean[microstudy_clean$freechance_position > microstudy_clean$fixedchance_position,]$first_chance <- "Multiple Choice" -->

<!-- microstudy_clean[microstudy_clean$probhide_position < microstudy_clean$probvis_position,]$first_probability <- "Hidden" -->

<!-- microstudy_clean[microstudy_clean$probhide_position > microstudy_clean$probvis_position,]$first_probability <- "Visible" -->

<!-- ``` -->

<!-- Figure \@ref(fig:chanceorder) shows the number of participants who received each condition. -->
<!-- In the case of the non-match condition, the counts for each question category appears to be rather even. -->
<!-- However, in the case of the match condition, there is a large discrepancy in which probability question individuals received first. -->
<!-- There are `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Visible")` participants in the match condition who saw the visible probability question first, and `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Hidden")` participants in the match condition who saw the hidden probability question first. -->
<!-- This provides a difference of `r sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Visible") - sum(microstudy_clean$conclusion.x=="Match" & microstudy_clean$first_probability=="Hidden")` participants. -->


<!-- ```{r} -->
<!-- #| chanceorder, -->
<!-- #| fig.cap= "Order of Chance and Probability Questions", -->
<!-- #| fig.width= 10, -->
<!-- #| fig.height= 4, -->
<!-- #| fig.align= "center", -->
<!-- #| echo= FALSE, -->
<!-- #| eval= TRUE, -->
<!-- #| warning= FALSE, -->
<!-- #| message= FALSE -->

<!-- chanceplot <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=first_chance), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   xlab("First Chance Question")+ -->
<!--   facet_grid(conclusion.x~.)+ -->
<!--   scale_x_discrete(labels = wrap_format(10))+ -->
<!--   theme_bw() -->

<!-- probplot <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=first_probability), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   xlab("First Probability Question")+ -->
<!--   facet_grid(conclusion.x~.)+ -->
<!--   scale_x_discrete(labels = wrap_format(10))+ -->
<!--   theme_bw() -->

<!-- chanceplot + probplot -->


<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo= FALSE, -->
<!-- #| eval= FALSE, -->
<!-- #| warning= FALSE, -->
<!-- #| message= FALSE -->

<!-- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=first_chance), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   xlab("First Chance Question")+ -->
<!--   facet_grid(conclusion.x~first_probability)+ -->
<!--   scale_x_discrete(labels = wrap_format(10))+ -->
<!--   theme_bw() -->


<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo= FALSE, -->
<!-- #| eval= FALSE, -->
<!-- #| warning= FALSE, -->
<!-- #| message= FALSE -->

<!-- microstudy_clean$first_question <- substring(microstudy_clean$order, 1, 1) -->
<!-- microstudy_clean$second_question <- substring(microstudy_clean$order, 2, 2) -->
<!-- microstudy_clean$third_question <- substring(microstudy_clean$order, 3, 3) -->
<!-- microstudy_clean$fourth_question <- substring(microstudy_clean$order, 4, 4) -->
<!-- microstudy_clean$fifth_question <- substring(microstudy_clean$order, 5, 5) -->
<!-- microstudy_clean$sixth_question <- substring(microstudy_clean$order, 6, 6) -->
<!-- microstudy_clean$seventh_question <- substring(microstudy_clean$order, 7, 7) -->

<!-- first <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=first_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- second <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=second_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- third <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=third_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- fourth <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=fourth_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- fifth <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=fifth_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- sixth <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=sixth_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- seventh <- ggplot(microstudy_clean) + -->
<!--   geom_bar(aes(x=seventh_question), position = position_dodge(preserve = "single")) + -->
<!--   ggtitle("") + -->
<!--   ylab("Count")+ -->
<!--   facet_grid(.~conclusion.x)+ -->
<!--   theme_bw() -->

<!-- first/second/third -->

<!-- fourth/fifth/sixth/seventh -->

<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$first_question)) -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$second_question)) -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$third_question)) -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$fourth_question)) -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$fifth_question)) -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$sixth_question)) -->

<!-- chisq.test(table(microstudy_clean$first_chance, microstudy_clean$conclusion.x)) -->
<!-- chisq.test(table(microstudy_clean$first_probability, microstudy_clean$conclusion.x)) -->

<!-- #Significant difference in conclusion -->
<!-- chisq.test(table(microstudy_clean$conclusion.x)) -->

<!-- #borderline significant -->
<!-- chisq.test(table(microstudy_clean$seventh_question, microstudy_clean$conclusion.x)) -->

<!-- #Significant when only looking at the match condition - not true for other numbers -->
<!-- # The betting question (4) and strength question (7) are less represented -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$seventh_question)) -->

<!-- #borderline significant when only looking at the match condition -->
<!-- chisq.test(table(microstudy_clean[microstudy_clean$conclusion.x=="Match",]$first_probability)) -->
<!-- ``` -->

#### Gun Comfort

Participants were asked to rate how comfortable they are with guns.
Figure \@ref(fig:convictscomfort) shows the relationship between choice to convict and comfort with guns.
There does not appear to be a big trend in conviction rate.
However, it can be seen in the identification condition that those who were extremely uncomfortable with guns were more likely to convict, while those who were extremely comfortable with gones were less likely to convict.

```{r}
#| convictscomfort,
#| fig.cap= "Gun Comfort and Conviction Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$guncomfort = factor(
  microstudy_clean$guncomfort,
  levels = c(
    "Extremely Uncomfortable",
    "Moderately Uncomfortable",
    "Slightly Uncomfortable",
    "Neither Comfortable nor Uncomfortable",
    "Slightly Comfortable",
    "Moderately Comfortable",
    "Extremely Comfortable"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=guncomfort, fill=convict), position = position_dodge(preserve = "single")) +
  ggtitle("") +
  scale_fill_manual(values = c("grey20", "plum1"), name="Convict?")+
  ylab("Count")+
  xlab("Gun Comfort")+
  facet_grid(conclusion~.)+
  scale_x_discrete(labels = wrap_format(10))+
  theme_bw()

```

### Notepad Analysis

While the notepad analysis for this study still needs to be conducted, analysis of preliminary results indicate that individuals were less likely to directly copy testimony compared to the first study.
This may be due to the change in format - the use of speech bubbles breaks up the text, which may prevent highlighting the entire page of testimony.
Due to this potential change, the potential length of the collocation may need to be shortened.

## Conclusion

Several things can be determined through this exploration of response type.
Probability and strength of evidence scales suffer from scale compression in the elimination condition, as can be seen by the proclivity of participants to overwhelmingly select near the most extreme values.
The amount that participants were willing to bet was also compressed around the maximum amount, with no clear relationship between probability and betting amount, aside from individuals generally betting near the maximum and selecting extreme values on the probability scale.
The chance scales were less compressed, with the best results from the multiple choice scale.
Individuals who chose to numerically express chance of innocence as opposed to chance of guilt demonstrated more of a tendency to reverse the scale.

<!-- ## Notes -->


<!-- - Reliability: Questions regarding the consistency of the comparison -->
<!--   - How often to you think the firearms examiner makes mistakes? -->
<!--      - [blank] out of [blank] bullet comparisons -->
<!--    - How often to you think the algorithm makes mistakes? -->
<!--      - [blank] out of [blank] bullet comparisons -->
<!--    - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner -->
<!--      - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison -->
<!--    - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results? -->
<!--        - [blank] out of [blank] runs would agree with the results of the bullet comparison -->
<!--    <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual --> -->
<!--    - What if we used a blank slider for credibility/scientificity? -->
<!--    - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to -->
<!--      - Did you find the algorithm or the bullet comparison to be more scientific? -->
<!--      - Compared to the case description, how would you rate the scientificity of the bullet comparison -->
<!--    - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun? -->

<!-- - Clarify if the examiner used the algorithm before or after their initial comparison. -->
<!--  <!-- - If the algorithm is used first, this would potentially be confounding --> -->