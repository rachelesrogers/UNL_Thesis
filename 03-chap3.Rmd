# Jury Perception Revisited: This Time with Caricatures {#study2}

## Background


### Study 1 and Scale Compression

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales in this scenario, as well as the use of a transcript testimony format.

Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a micro study in order to compare various response types, in order to determine substitute questions that may be more sensitive to scenario differences.
Additional changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner and algorithm developer as they would any other witness.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well.
The transcript for the second study can be found in Appendix \@ref(study-2-changes).

Some participants left confused comments with regards to who the witnesses were testifying for.
The transcript format may lend an air of impartiality to the witnesses, when they are in fact testifying for a specific side in the case.
Also, the format of the testimony transcripts does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by a participant.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, one master file is used with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text.
Unique identification is used throughout the database files instead of relying on unique fingerprints, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study, as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a study in order to determine how these figures are perceived.

## Micro Study on Response Type

To investigate a which response type may be most informative for this study and to troubleshoot the use of figures and speech bubble format, we conducted a smaller micro study using various response types.

### Methods

#### Study Format

Participants were presented with the same scenario described in Study 1.
In this case, however, the only factor that was changes was the conclusion of the firearms examiner: either a match or not a match.
In all conditions, the algorithm was absent and there were no images.
The testimony largely followed that of Study 1, aside from differences in the format and additional testimony described above.
In this case, it was also explicitly state that the testimony did not reflect all evidence presented in the case, because in the non match condition there may not be enough evidence to bring the case to trial.

At the end of the testimony, the participants were asked a variety of questions regarding the strength of evidence in the case.
Questions of probability, strength of evidence, and decision to convict were also asked in Study 1.
They were asked to evaluate the probability that the defendant committed the crime, both with a visible probability scale (allowing the participants to select the exact probability value) and with a non-visible scale (only the numbers of 0 and 100 were available on the extremes of the scale).
The strength of evidence was a Likert scale with values from "Not at all strong" to "Extremely strong" with nine points.
In terms of conviction, participants were reminded of the decision criterion of "beyond a reasonable doubt" when making a decision in a criminal trial, and asked if they would choose to convict.

New questions for this micro study asked for individuals to give their opinion of the guilt of the defendant, as well as assessing the chances that the defendant committed the crime, how much they would be willing to bet that the defendant was either innocent or guilty, and their opinion of the defendant.
In addition to a question regarding whether or not the participant would choose to convict, we also asked the participants to give their personal opinion on the guilt of the defendant.
This provides a second threshold for assessing the strength of evidence, aside from the "beyond a reasonable doubt" standard.
Two different questions were asked with regards to the chances that the defendant committed the crime. 
One was in a multiple choice format, with extreme values of "Impossible to be guilty" and "Certain to be guilty", and intermediate values ranging from "About 1 chance in 10,000" to "About 9,999 chances in 10,000" with denominator values changing by a decimal place for each choice, and a middle value of "1 chance in 2".
The second question allowed participants to select both the numerator and denominator: "About ___ chance(s) in ___".
The format of the numeric likelihood question depends on the participant's opinion of guilt.
If the participant thought that the defendant was guilty, they were asked to provide the likelihood that the defendant was innocent.
If the participant thought that the defendant was innocent, they were asked to provide the likelihood that the defendant was guilty.
In both cases, the numerator had to be less than or equal to the denominator.
Similarly, if individuals thought that the defendant was guilty, they were asked how much they were willing to be that the defendant was guilty, if hypothetically researchers provided them with \$50 and they would double their money if they were correct, and vice versa.
A final new question was open ended, and asked participants to provide their opinion of the defendant.

Individuals were first asked to provide their opinion of the defendant, their conviction decision, and their personal opinion of the guilt of the defendant.
All other questions were randomized in a way that guaranteed the two probability questions and the two likelihood questions were not asked directly following each other.


#### Prolific

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(readr)
microstudy <- read_csv("data/microstudy_cleanish_results.csv")

microstudy_clean <- microstudy %>% dplyr::filter(check=="9mm")

```

Participants were recruited in a similar manner as Study 1 in terms of the representative sample and self-screened jury requirements through Prolific.
In the first part of the study, some individuals encountered technical issues.
These technical issues appeared to happen to more individuals in the match condition compared to the non match condition, with a total of `r sum(microstudy$conclusion.x=="Match")` individuals in the match condition and `r sum(microstudy$conclusion.x=="NoMatch")` participants in the non-match condition, for a total of `r dim(microstudy)[1]` participants.
Participants were paid \$4.00 with a median completion time of 14 minutes and 47 seconds according to Prolific, for an average reward of \$16.23 per hour.
Figure \@ref(fig:completiontime2) shows the time spent after the completion of the first demographics page to the completion of the final results page.
Because it does not include the informed consent and the time to complete the first demographics page, its time estimates appear to be less than those found via Prolific.


```{r}
#| completiontime2,
#| fig.cap= "Completion Time by Condition",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$completion_time <- microstudy$actual_time3-microstudy$actual_time1

ggplot(microstudy, aes(x = completion_time, fill=conclusion.x)) +
    geom_density(alpha=0.75, color=NA) +
  ggtitle("Histogram of Completion Time") +
  xlab("Completion Time in Minutes")+
  scale_fill_manual(name="Conclusion", values = c("#FF8E00", "#037AC7"))+
  theme_bw()


```

### Results

#### Participants

Of the `r dim(microstudy)[1]` participants, `r sum(microstudy$gender=="Male")` participants identified as male, `r sum(microstudy$gender=="Female")` participants identified as female, and `r sum(microstudy$gender=="Other/non-binary")` participants identified as other or non-binary. 
The categories of "Male" and "Female" are currently more associated with sex than with gender, and this categorization will be relabeled in future studies.
The median age category was 46 - 55.
Age and gender are shown in Figure \@ref(fig:demographics2).
Individuals were asked a single attention check question with regards to the caliber of gun used in the attempted robbery.
`r dim(microstudy_clean)[1]` participants passed the attention check, meaning that only `r sum(microstudy$check!="9mm")` participants failed the attention check, and will not be included in the analysis.


```{r}
#| demographics2,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$age = factor(
  microstudy$age,
  levels = c(
    "18 - 25",
    "26 - 35",
    "36 - 45",
    "46 - 55",
    "56 - 65",
    "Over 65"
  )
)

  ggplot(microstudy,
         aes(x = age, fill = gender)) +
  geom_bar(mapping = aes(y = after_stat(count), group = gender),
           position = position_dodge(preserve = "single"), color="black") +
  #  geom_histogram(stat="count", position="dodge")+
  ggtitle("Algorithm Expert Testimony") +
  scale_fill_manual(values = c("#E69F00", "#009E73", "#F0E442")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```
#### Questions from Study 1

`r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition chose to convict, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition chose to convict.
This is a slightly lower proportion of individuals choosing to convict than that seen in the Match condition in the original study, although there is no algorithm present in this case.

Figure \@ref(fig:strength2) shows participant responses to the strength of evidence in this study.
As can be seen, those in the non-match category tended to choose the smallest value for the strength of evidence ("Not at all strong"), while in the case of the match condition, individuals tended to distribute their views of the strength of evidence more evenly.
This graph resembles the strength of evidence results from the initial study.
While not as dramatic as the scale compression for questions of reliability, credibility, and scientificity, the non-match results for strength of evidence do appear to show a scale limitation in the way that individuals tended to select the lowest category.

```{r}
#| strength2,
#| fig.cap= "Microstudy Strength of Evidence",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$strength = factor(
  microstudy_clean$strength,
  levels = c(
    "1 <br/> Not at all strong",
    "2",
    "3",
    "4",
    "5 <br/> Moderately strong",
    "6",
    "7",
    "8",
    "9 <br/> Extremely strong"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=strength, fill=conclusion.x), position="dodge") +
  ggtitle("What is the Strength of Evidence against the Defendant?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition")+
  ylab("Count")+
  xlab("Strength")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

Figure \@ref(fig:prob2) demonstrates the participants' selected probabilities that the defendant committed the crime.
As in the case of the strength of evidence, the graph resembles the probability graph found in Study 1, where there is a higher peak of extreme values for the non-match conditions than for the match condition.
There does not appear to be much difference in the density curves based on the visibility of the probability scale.

```{r}
#| prob2,
#| fig.cap= "Probability Cole Committed Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

colors <-  c("Hidden"="red", "Visible"="grey")

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=prob_hide, fill="Hidden")) +
  geom_density(alpha=0.75, aes(x=prob_vis, fill="Visible")) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = colors, name="Probability")+
  ylab("Density")+
  xlab("Probability")+
  facet_grid(.~conclusion.x)+
  theme_bw()


```

`r round(sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition thought the defendant was guilty, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition thought the defendant was guilty.
Figure \@ref(fig:opinionguilt) shows the comparison between the participant's decision to convict and their personal opinion.
Approximately half of the participant in the match condition who chose not to convict thought the defendant was in fact guilty.

```{r}
#| opinionguilt,
#| fig.cap= "Comparison of opinions of guilt and choice to convict",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=guilty, fill=opinion_guilt), position="dodge") +
  ggtitle("") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Convict?")+
  facet_grid(.~conclusion.x)+
  theme_bw()

```

Figure \@ref(fig:betting) indicates how much participants said they would be willing to bet that the defendant was either guilty or innocent, if provided with \$50.
If the participant indicated that they thought the defendant was innocent, they were asked how much they would be willing to bet that the defendant was innocent, and vice versa.
Two individuals did not answer the question, while three individuals selected values larger than 50: two in the non-match condition who thought Cole was innocent (\$58 and \$100), and one in the match condition who thought Cole was guilty (\$100).
This figure indicates that most people who thought Cole was innocent in the non-match condition were willing to bet the full amount.
It also appears that people tended to select values around multiples of 5, and those in the match condition selected less extreme values (for both those who thought the defendant was guilty and those who thought the defendant was innocent) than what is seen in those with the non-match condition who thought the defendant was innocent.
In this way, the betting response is similar both to the strength of evidence response and the probability of committing the crime response.

```{r}
#| betting,
#| fig.cap= "If the researchers provided 50 dollars, how much would you be willing to bet?",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$bet<- NA

microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$bet<- 
  microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$guilt_bet

microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$bet<- microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$innocent_bet

ggplot(microstudy_clean) +
  geom_histogram(alpha=0.75, aes(x=bet, fill=opinion_guilt), position="dodge") +
  ggtitle("How much would you bet that the Defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Bet Amount")+
  xlim(c(0,50))+
  facet_grid(.~conclusion.x)+
  theme_bw()


```

The remaining questions relate to the likelihood that the defendant committed the crime.
One question, shown in Figure \@ref(fig:fixedlike), allowed participants to select the likelihood that the defendant committed the crime from a multiple choice scale.
Based on the scale used, this question remained the same regardless of the participant's opinion on the guilt of the defendant.
This scale does not provide a linear distance between intervals, but instead changes by multiples of 10 in the denominator (ex. one category is 1 in 10, and the next category is 1 in 100).
The exception is the endpoints, which are "Impossible to be guilty" and "Certain to be guilty", as well as the midpoint of 1 chance in 2.
In this case, the non-match scale is not encountering the ceiling or floor effect that has been seen in previous scales, as participants did not overwhelmingly select the lowest value.
Instead, participants are distributed mainly throughout the lower half of the scale, while those in the match condition tended to be a little closer to the center.
Thus, the multiple choice likelihood scale appears to have less scale compression than seen previously in the other response types.

```{r}
#| fixedlike,
#| fig.cap= "Multiple Choice Likelihood",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$fixed_like = factor(
  microstudy_clean$fixed_like,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=fixed_like, fill=conclusion.x), position="dodge") +
  ggtitle("What is the Likelihood that the Defendant is Guilty?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition")+
  ylab("Count")+
  xlab("Likelihood")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

A final question asks individuals to provide a numerical likelihood that the defendant is either innocent or guilty, depending on their expressed opinion.
If the participant thought that the defendant was innocent, they were asked to supply the likelihood that the defendant was guilty, and vice versa.
Their responses were limited so that the numerator was smaller than the denominator, resulting in a range of 0 to 1.
These results are shown in Figure \@ref(fig:freelike).
As seen in previous graphs, in the case of the non-match condition, those who thought the defendant was innocent gave small likelihoods that the defendant had in fact committed the crime.
Note that this graph consists of the density in each group.
In the case of the match condition, those who thought the defendant was guilty tended to give lower likelihoods that the defendant had in fact committed the crime, although it is not as extreme as the non-match condition case.

```{r}
#| freelike,
#| fig.cap= "Free Response Likelihood",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$likelihood<- NA

microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                !is.na(microstudy_clean$innocent_free_denom),]$likelihood<-
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) &
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_num/
  microstudy_clean[!is.na(microstudy_clean$innocent_free_num) & 
                  !is.na(microstudy_clean$innocent_free_denom),]$innocent_free_denom

microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                !is.na(microstudy_clean$guilt_free_denom),]$likelihood<-
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) &
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_num/
  microstudy_clean[!is.na(microstudy_clean$guilt_free_num) & 
                  !is.na(microstudy_clean$guilt_free_denom),]$guilt_free_denom

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=likelihood, fill=opinion_guilt), position="dodge") +
  ggtitle("What is the likelihood that the defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Likelihood")+
  facet_grid(.~conclusion.x)+
  #scale_x_continuous(trans='log10')+
  theme_bw()


```

Based on the results shown in Figure \@ref(fig:fixedlike), it seems that the results from this free likelihood scale may benefit from a transformation of the scale.
The log scale transformation is shown in \@ref(fig:freelike10).
This scale transformation shifted the observations from the far left side of the graph to the right side of the graph.
The distribution of the scale is more spread out than that shown in Figure \@ref(fig:freelike), and may lend itself better to analysis.

```{r}
#| freelike10,
#| fig.cap= "Free Response Likelihood, Log 10 Scale",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=likelihood, fill=opinion_guilt), position="dodge") +
  ggtitle("What is the likelihood that the defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Guilty","Innocent"))+
 # ylab("Count")+
  xlab("Likelihood")+
  facet_grid(.~conclusion.x)+
  scale_x_continuous(trans='log10')+
  theme_bw()


```

#### Scale Comparison

Consistency across response types is another important aspect of this study.
If different question types result in responses that are inconsistent, it would be difficult to tell which questions truly capture the attitudes of the participants, in order to most accurately answer the research questions.
As mentioned in the previous literature review, there have been studies to suggest that individuals may struggle with the interpretation of likelihood scales.
Questions of how much a participant is willing to bet may also depend on their personal feel for risk, and betting hypothetical money may have different results than betting real money, or betting money for someone else.
\authorcol{There is a source for making decisions for others somewhere, and it may have had something to do with betting or something to do with plea deals. TBD}
Because individuals may be influenced by which scale they see first, the order of the questions were randomized and recorded, so that the orders can be compared.

Figure \@ref(fig:likecomp1) shows the comparison of the multiple choice likelihood responses to the numeric likelihood responses, in the case that the participants thought the defendant was innocent.
Because these individuals thought that the defendant was innocent, there are few responses higher than a "fifty-fifty chance".
The red dots represent the actual value represented by the multiple choice likelihood (for example, the red dot for "About 1 chance in 10" is located at the y-value of 0.10).
This allows for comparison on the consistency between the multiple choice and the numeric scales.
For values of "1 chance in 2" and "About 1 chance in 10", the responses seem fairly consistent - however, it is difficult to tell if these values are consistent for smaller values, because they are seen as small on the linear scale.

```{r}
#| likecomp1,
#| fig.cap= "Innocent Likelihood Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


set_values <- data.frame(fixed_like=c(    "Impossible that he is guilty",
                                          "About 1 chance in 10,000",
                                          "About 1 chance in 1,000",
                                          "About 1 chance in 100",
                                          "About 1 chance in 10",
                                          "1 chance in 2 (fifty-fifty chance)",
                                          "About 9 chances in 10",
                                          "About 99 chances in 100",
                                          "About 999 chances in 1,000",
                                          "About 9,999 chances in 10,000",
                                          "Certain to be guilty"),
                         value=c(0,1/10000,1/1000,1/100,1/10,0.5,9/10,99/100,999/1000,9999/10000,1))
clean_results_merged<- dplyr::left_join(microstudy_clean, set_values)
clean_results_merged$fixed_like = factor(
  microstudy_clean$fixed_like,
  levels = c(
    "Impossible that he is guilty",
    "About 1 chance in 10,000",
    "About 1 chance in 1,000",
    "About 1 chance in 100",
    "About 1 chance in 10",
    "1 chance in 2 (fifty-fifty chance)",
    "About 9 chances in 10",
    "About 99 chances in 100",
    "About 999 chances in 1,000",
    "About 9,999 chances in 10,000",
    "Certain to be guilty"
  )
)
ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Likelihood for those who thought Cole was innocent") +
  geom_jitter(aes(y=(guilt_free_num/guilt_free_denom)),
    # position = position_jitterdodge(
    #   jitter.width = 0.2,
    #   #jitter.height = 0.4,
    #   dodge.width = 1
    # ),
    size = 1
  ) +
  geom_boxplot(aes(y=(guilt_free_num/guilt_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Likelihood")+
  xlab("Closed Response Likelihood")+
#  scale_y_continuous(trans='log10')+
  scale_x_discrete(labels = wrap_format(10))

```

This transformation is shown in Figure \@ref(fig:likecomp1scale).
From this transformed scale, there still appears to be consistency between when participants choose their own likelihood numerically and when they were offered a multiple choice.
There does, however, appear to be a difference in those who selected "Impossible that he is guilty" from the multiple choice scale.
Many of the numerical responses appear to be more consistent to "About 1 chance in 10,000" than the value of 0 that would indicate impossibility.

```{r}
#| likecomp1scale,
#| fig.cap= "Innocent Likelihood Comparison Log 10 Scale",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Likelihood for those who thought Cole was innocent") +
  geom_jitter(aes(y=(guilt_free_num/guilt_free_denom)),
    # position = position_jitterdodge(
    #   jitter.width = 0.2,
    #   #jitter.height = 0.4,
    #   dodge.width = 1
    # ),
    size = 1
  ) +
  geom_boxplot(aes(y=(guilt_free_num/guilt_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Likelihood")+
  xlab("Closed Response Likelihood")+
  scale_y_continuous(trans='log10')+
  scale_x_discrete(labels = wrap_format(10))

```

This same procedure can be repeated for those who thought the defendant was guilty, as shown in Figure \@ref(fig:likecomp2).
In this case, because individuals who thought the defendant was guilty were asked to supply the likelihood that the defendant did not commit the crime, their numerical likelihood was changed to the likelihood that the defendant did commit the crime by subtracting their likelihood from 1.
In a reverse of the likelihood for those who thought Cole was innocent, in this case most responses are at or above "1 chance in 2".
Even without a transformation, it can be seen that there is a large spread of those who selected "Certain to be guilty", and that those who selected "fifty-fifty chance" tended to select higher likelihoods of guilt when they were free to choose their own response, while those who selected "About 9 chances in 10" tended to align well between the numerical and multiple choice options.

The reduced consistency seen in the guilty likelihood may related to the change in the question wording, where the multiple choice question considers the likelihood of guilt, while their numeric likelihood considered the likelihood of innocence.

```{r}
#| likecomp2,
#| fig.cap= "Guilty Likelihood Comparison",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

ggplot(clean_results_merged, aes(x=fixed_like))+ #,fill=conclusion
  geom_point(aes(y=value),color="red",size=5,alpha=0.5)+
  ggtitle("Likelihood for those who thought Cole was guilty") +
  geom_jitter(aes(y=(1-innocent_free_num/innocent_free_denom)),
             # position = position_jitterdodge(
             #   jitter.width = 0.2,
             #   #jitter.height = 0.4,
             #   dodge.width = 1
             # ),
             size = 1
  ) +
  geom_boxplot(aes(y=(1-innocent_free_num/innocent_free_denom)),
               position = position_dodge(1),
               alpha = 0.5,
               outlier.shape = NA)+ 
  ylab("Open Response Likelihood")+
  xlab("Closed Response Likelihood")+
  scale_x_discrete(labels = wrap_format(10))

```


## Notes


- Reliability: Questions regarding the consistency of the comparison
  - How often to you think the firearms examiner makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - How often to you think the algorithm makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner
     - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison
   - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results?
       - [blank] out of [blank] runs would agree with the results of the bullet comparison
   <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual -->
   - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to
     - Did you find the algorithm or the bullet comparison to be more scientific?
     - Compared to the case description, how would you rate the scientificity of the bullet comparison
   - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun?
  
- Clarify if the examiner used the algorithm before or after their initial comparison.
 <!-- - If the algorithm is used first, this would potentially be confounding -->