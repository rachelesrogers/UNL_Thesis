# Jury Perception Revisited: This Time with Caricatures {#study2}

## Background


### Study 1 and Scale Compression

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales in this scenario, as well as the use of a transcript testimony format.

Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a micro study in order to compare various response types, in order to determine substitute questions that may be more sensitive to scenario differences.
Additional changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner and algorithm developer as they would any other witness.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well.
The transcript for the second study can be found in Appendix \@ref(study-2-changes).

Some participants left confused comments with regards to who the witnesses were testifying for.
The transcript format may lend an air of impartiality to the witnesses, when they are in fact testifying for a specific side in the case.
Also, the format of the testimony transcripts does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by a participant.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, one master file is used with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text.
Unique identification is used throughout the database files instead of relying on unique fingerprints, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study, as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a study in order to determine how these figures are perceived.

## Micro Study on Response Type

To investigate a which response type may be most informative for this study and to troubleshoot the use of figures and speech bubble format, we conducted a smaller micro study using various response types.

### Methods

#### Study Format

Participants were presented with the same scenario described in Study 1.
In this case, however, the only factor that was changes was the conclusion of the firearms examiner: either a match or not a match.
In all conditions, the algorithm was absent and there were no images.
The testimony largely followed that of Study 1, aside from differences in the format and additional testimony described above.
In this case, it was also explicitly state that the testimony did not reflect all evidence presented in the case, because in the non match condition there may not be enough evidence to bring the case to trial.

At the end of the testimony, the participants were asked a variety of questions regarding the strength of evidence in the case.
Questions of probability, strength of evidence, and decision to convict were also asked in Study 1.
They were asked to evaluate the probability that the defendant committed the crime, both with a visible probability scale (allowing the participants to select the exact probability value) and with a non-visible scale (only the numbers of 0 and 100 were available on the extremes of the scale).
The strength of evidence was a Likert scale with values from "Not at all strong" to "Extremely strong" with nine points.
In terms of conviction, participants were reminded of the decision criterion of "beyond a reasonable doubt" when making a decision in a criminal trial, and asked if they would choose to convict.

New questions for this micro study asked for individuals to give their opinion of the guilt of the defendant, as well as assessing the chances that the defendant committed the crime, how much they would be willing to bet that the defendant was either innocent or guilty, and their opinion of the defendant.
In addition to a question regarding whether or not the participant would choose to convict, we also asked the participants to give their personal opinion on the guilt of the defendant.
This provides a second threshold for assessing the strength of evidence, aside from the "beyond a reasonable doubt" standard.
Two different questions were asked with regards to the chances that the defendant committed the crime. 
One was in a multiple choice format, with extreme values of "Impossible to be guilty" and "Certain to be guilty", and intermediate values ranging from "About 1 chance in 10,000" to "About 9,999 chances in 10,000" with denominator values changing by a decimal place for each choice, and a middle value of "1 chance in 2".
The second question allowed participants to select both the numerator and denominator: "About ___ chance(s) in ___".
The format of the numeric likelihood question depends on the participant's opinion of guilt.
If the participant thought that the defendant was guilty, they were asked to provide the likelihood that the defendant was innocent.
If the participant thought that the defendant was innocent, they were asked to provide the likelihood that the defendant was guilty.
In both cases, the numerator had to be less than or equal to the denominator.
Similarly, if individuals thought that the defendant was guilty, they were asked how much they were willing to be that the defendant was guilty, if hypothetically researchers provided them with \$50 and they would double their money if they were correct, and vice versa.
A final new question was open ended, and asked participants to provide their opinion of the defendant.

Individuals were first asked to provide their opinion of the defendant, their conviction decision, and their personal opinion of the guilt of the defendant.
All other questions were randomized in a way that guaranteed the two probability questions and the two likelihood questions were not asked directly following each other.


#### Prolific

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(readr)
microstudy <- read_csv("data/microstudy_cleanish_results.csv")

microstudy_clean <- microstudy %>% dplyr::filter(check=="9mm")

```

Participants were recruited in a similar manner as Study 1 in terms of the representative sample and self-screened jury requirements through Prolific.
In the first part of the study, some individuals encountered technical issues.
These technical issues appeared to happen to more individuals in the match condition compared to the non match condition, with a total of `r sum(microstudy$conclusion.x=="Match")` individuals in the match condition and `r sum(microstudy$conclusion.x=="NoMatch")` participants in the non-match condition, for a total of `r dim(microstudy)[1]` participants.
Participants were paid \$4.00 with a median completion time of 14 minutes and 47 seconds according to Prolific, for an average reward of \$16.23 per hour.
Figure \@ref(fig:completiontime2) shows the time spent after the completion of the first demographics page to the completion of the final results page.
Because it does not include the informed consent and the time to complete the first demographics page, its time estimates appear to be less than those found via Prolific.


```{r}
#| completiontime2,
#| fig.cap= "Completion Time by Condition",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$completion_time <- microstudy$actual_time3-microstudy$actual_time1

ggplot(microstudy, aes(x = completion_time, fill=conclusion.x)) +
    geom_density(alpha=0.75, color=NA) +
  ggtitle("Histogram of Completion Time") +
  xlab("Completion Time in Minutes")+
  scale_fill_manual(name="Conclusion", values = c("#FF8E00", "#037AC7"))+
  theme_bw()


```

### Results

#### Participants

Of the `r dim(microstudy)[1]` participants, `r sum(microstudy$gender=="Male")` participants identified as male, `r sum(microstudy$gender=="Female")` participants identified as female, and `r sum(microstudy$gender=="Other/non-binary")` participants identified as other or non-binary. 
The categories of "Male" and "Female" are currently more associated with sex than with gender, and this categorization will be relabeled in future studies.
The median age category was 46 - 55.
Age and gender are shown in Figure \@ref(fig:demographics2).
Individuals were asked a single attention check question with regards to the caliber of gun used in the attempted robbery.
`r dim(microstudy_clean)[1]` participants passed the attention check, meaning that only `r sum(microstudy$check!="9mm")` participants failed the attention check, and will not be included in the analysis.


```{r}
#| demographics2,
#| fig.cap= "Demographic Information",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy$age = factor(
  microstudy$age,
  levels = c(
    "18 - 25",
    "26 - 35",
    "36 - 45",
    "46 - 55",
    "56 - 65",
    "Over 65"
  )
)

  ggplot(microstudy,
         aes(x = age, fill = gender)) +
  geom_bar(mapping = aes(y = after_stat(count), group = gender),
           position = position_dodge(preserve = "single"), color="black") +
  #  geom_histogram(stat="count", position="dodge")+
  ggtitle("Algorithm Expert Testimony") +
  scale_fill_manual(values = c("#E69F00", "#009E73", "#F0E442")) +
  theme_bw()+
  theme(axis.title.x = element_blank())

```
#### Questions from Study 1

`r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition chose to convict, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition chose to convict.
This is a slightly lower proportion of individuals choosing to convict than that seen in the Match condition in the original study, although there is no algorithm present in this case.

Figure \@ref(fig:strength2) shows participant responses to the strength of evidence in this study.
As can be seen, those in the non-match category tended to choose the smallest value for the strength of evidence ("Not at all strong"), while in the case of the match condition, individuals tended to distribute their views of the strength of evidence more evenly.
This graph resembles the strength of evidence results from the initial study.
While not as dramatic as the scale compression for questions of reliability, credibility, and scientificity, the non-match results for strength of evidence do appear to show a scale limitation in the way that individuals tended to select the lowest category.

```{r}
#| strength2,
#| fig.cap= "Microstudy Strength of Evidence",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$strength = factor(
  microstudy_clean$strength,
  levels = c(
    "1 <br/> Not at all strong",
    "2",
    "3",
    "4",
    "5 <br/> Moderately strong",
    "6",
    "7",
    "8",
    "9 <br/> Extremely strong"
  )
)

ggplot(microstudy_clean) +
  geom_bar(aes(x=strength, fill=conclusion.x), position="dodge") +
  ggtitle("What is the Strength of Evidence against the Defendant?") +
  scale_fill_manual(values = c("grey80","seagreen"), name="Condition")+
  ylab("Count")+
  xlab("Strength")+
  theme_bw()+
  scale_x_discrete(labels = wrap_format(10))


```

Figure \@ref(fig:prob2) demonstrates the participants' selected probabilities that the defendant committed the crime.
As in the case of the strength of evidence, the graph resembles the probability graph found in Study 1, where there is a higher peak of extreme values for the non-match conditions than for the match condition.
There does not appear to be much difference in the density curves based on the visibility of the probability scale.

```{r}
#| prob2,
#| fig.cap= "Probability Cole Committed Crime",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

colors <-  c("Hidden"="red", "Visible"="grey")

ggplot(microstudy_clean) +
  geom_density(alpha=0.75, aes(x=prob_hide, fill="Hidden")) +
  geom_density(alpha=0.75, aes(x=prob_vis, fill="Visible")) +
  ggtitle("Probability Cole Commited the Crime") +
  scale_fill_manual(values = colors, name="Probability")+
  ylab("Density")+
  xlab("Probability")+
  facet_grid(.~conclusion.x)+
  theme_bw()


```

`r round(sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")/sum(microstudy_clean$conclusion.x=="Match")*100, 2)`\% (or `r sum(microstudy_clean$opinion_guilt=="Yes" & microstudy_clean$conclusion.x=="Match")` out of `r sum(microstudy_clean$conclusion.x=="Match")`) individuals who received the match condition thought the defendant was guilty, while `r round(sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")/sum(microstudy_clean$conclusion.x=="NoMatch")*100, 2)`\% (or `r sum(microstudy_clean$guilty=="Yes" & microstudy_clean$conclusion.x=="NoMatch")` out of `r sum(microstudy_clean$conclusion.x=="NoMatch")`) participants who received the non-match condition thought the defendant was guilty.
Figure \@ref(fig:opinionguilt) shows the comparison between the participant's decision to convict and their personal opinion.
Approximately half of the participant in the match condition who chose not to convict thought the defendant was in fact guilty.

```{r}
#| opinionguilt,
#| fig.cap= "Comparison of opinions of guilt and choice to convict",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE


ggplot(microstudy_clean) +
  geom_bar(aes(x=guilty, fill=opinion_guilt), position="dodge") +
  ggtitle("") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="Opinion", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Convict?")+
  facet_grid(.~conclusion.x)+
  theme_bw()

```

Figure \@ref(fig:betting) indicates how much participants said they would be willing to bet that the defendant was either guilty or innocent, if provided with \$50.
If the participant indicated that they thought the defendant was innocent, they were asked how much they would be willing to bet that the defendant was innocent, and vice versa.
Two individuals did not answer the question, while three individuals selected values larger than 50: two in the non-match condition who thought Cole was innocent (\$58 and \$100), and one in the match condition who thought Cole was guilty (\$100).

```{r}
#| betting,
#| fig.cap= "If the researchers provided 50 dollars, how much would you be willing to bet?",
#| fig.width= 10,
#| fig.height= 4,
#| fig.align= "center",
#| echo= FALSE,
#| eval= TRUE,
#| warning= FALSE,
#| message= FALSE

microstudy_clean$bet<- NA

microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$bet<- 
  microstudy_clean[!is.na(microstudy_clean$guilt_bet),]$guilt_bet

microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$bet<- microstudy_clean[!is.na(microstudy_clean$innocent_bet),]$innocent_bet

ggplot(microstudy_clean) +
  geom_histogram(alpha=0.75, aes(x=bet, fill=opinion_guilt), position="dodge") +
  ggtitle("How much would you bet that the Defendant is...") +
  scale_fill_manual(values = c("#D81B60","#004D40"), name="", labels=c("Innocent","Guilty"))+
  ylab("Count")+
  xlab("Bet Amount")+
  xlim(c(0,50))+
  facet_grid(.~conclusion.x)+
  theme_bw()


```

#### Scale Comparison

## Notes


- Reliability: Questions regarding the consistency of the comparison
  - How often to you think the firearms examiner makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - How often to you think the algorithm makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner
     - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison
   - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results?
       - [blank] out of [blank] runs would agree with the results of the bullet comparison
   <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual -->
   - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to
     - Did you find the algorithm or the bullet comparison to be more scientific?
     - Compared to the case description, how would you rate the scientificity of the bullet comparison
   - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun?
  
- Clarify if the examiner used the algorithm before or after their initial comparison.
 <!-- - If the algorithm is used first, this would potentially be confounding -->