# Jury Perception Revisited: This Time with Caricatures {#study2}

## Background


### Study 1 and Scale Compression

The results from the initial study found in Chapter \@ref(study1) call into question the use of Likert response scales in this scenario, as well as the use of a transcript testimony format.

Responses pertaining to the credibility of the expert, as well as the reliability and scientificity of the evidence, suffered from scale compression when a Likert scale was used - participants indicated overall confidence in credibility, reliability, and scientificity by mainly selecting the two highest categories of each scale.
This lack of variation in scale responses makes it difficult to discern potential differences between treatment conditions.
Due to this difficulty, we conducted a micro study in order to compare various response types, in order to determine substitute questions that may be more sensitive to scenario differences.
Additional changes include the addition of jury instructions on the part of the judge, instructing members of the jury to treat the firearms examiner and algorithm developer as they would any other witness.
Additional cross examination testimony with regards to the firearms examiner's inability to specifically tie the defendant to the crime scene is included as well.
The transcript for the second study can be found in Appendix \@ref(study-2-changes).

Some participants left confused comments with regards to who the witnesses were testifying for.
The transcript format may lend an air of impartiality to the witnesses, when they are in fact testifying for a specific side in the case.
Also, the format of the testimony transcripts does not clearly identify the speaker with each line, instead using "Q:" and "A:" in most cases, as shown in Appendix \@ref(study-2-changes).
This lack of visuals for tracking speakers is not representative of the courtroom setting, and may have contributed to the confusion expressed by a participant.
Due to this potential issue, we strove to develop a general tool that can be used in online courtroom studies, making it easier to track speakers, and with versatility in terms of individual characteristics.

In order to minimize the chance of confounding typos as seen in Study 1, one master file is used with all lines of testimony, labelled by the scenario in which the testimony occurs.
Thus, there is no longer multiple files of repeated text.
Unique identification is used throughout the database files instead of relying on unique fingerprints, so the demographics section can more easily be linked to the final results.

### Study Visualization

Appendix \@ref(study-2-changes) shows the characters that have been developed for this study, as well as a screenshot of the study format.
These characters were drawn so that the clothes and heads can be interchanged, providing a wide variety of characters for potential scenarios.
One difficulty in including drawn figures with realistic skin tones and figures is the influence of perceived race or gender on the participants' judgements.
@ImplicitRaceAttitudes found a relationship between implicit racial bias and the perceived trustworthiness of individuals based on images of black and white males.
They also found that 80% of participants exhibited pro-white implicit bias.
Because of this potential bias introduced by visual figures, we plan to conduct a study in order to determine how these figures are perceived.

## Micro Study on Response Type

To investigate a which response type may be most informative for this study and to troubleshoot the use of figures and speech bubble format, we conducted a smaller micro study using various response types.

### Methods

#### Study Format

Participants were presented with the same scenario described in Study 1.
In this case, however, the only factor that was changes was the conclusion of the firearms examiner: either a match or not a match.
In all conditions, the algorithm was absent and there were no images.
The testimony largely followed that of Study 1, aside from differences in the format and additional testimony described above.
In this case, it was also explicitly state that the testimony did not reflect all evidence presented in the case, because in the non match condition there may not be enough evidence to bring the case to trial.

At the end of the testimony, the participants were asked a variety of questions regarding the strength of evidence in the case.
Questions of probability, strength of evidence, and decision to convict were also asked in Study 1.
They were asked to evaluate the probability that the defendant committed the crime, both with a visible probability scale (allowing the participants to select the exact probability value) and with a non-visible scale (only the numbers of 0 and 100 were available on the extremes of the scale).
The strength of evidence was a Likert scale with values from "Not at all strong" to "Extremely strong" with nine points.
In terms of conviction, participants were reminded of the decision criterion of "beyond a reasonable doubt" when making a decision in a criminal trial, and asked if they would choose to convict.

New questions for this micro study asked for individuals to give their opinion of the guilt of the defendant, as well as assessing the chances that the defendant committed the crime, how much they would be willing to bet that the defendant was either innocent or guilty, and their opinion of the defendant.
In addition to a question regarding whether or not the participant would choose to convict, we also asked the participants to give their personal opinion on the guilt of the defendant.
This provides a second threshold for assessing the strength of evidence, aside from the "beyond a reasonable doubt" standard.
Two different questions were asked with regards to the chances that the defendant committed the crime. 
One was in a multiple choice format, with extreme values of "Impossible to be guilty" and "Certain to be guilty", and intermediate values ranging from "About 1 chance in 10,000" to "About 9,999 chances in 10,000" with denominator values changing by a decimal place for each choice, and a middle value of "1 chance in 2".
The second question allowed participants to select both the numerator and denominator: "About ___ chance(s) in ___".
The format of the numeric likelihood question depends on the participant's opinion of guilt.
If the participant thought that the defendant was guilty, they were asked to provide the likelihood that the defendant was innocent.
If the participant thought that the defendant was innocent, they were asked to provide the likelihood that the defendant was guilty.
In both cases, the numerator had to be less than or equal to the denominator.
Similarly, if individuals thought that the defendant was guilty, they were asked how much they were willing to be that the defendant was guilty, if hypothetically researchers provided them with $50 and they would double their money if they were correct, and vice versa.
A final new question was open ended, and asked participants to provide their opinion of the defendant.

Individuals were first asked to provide their opinion of the defendant, their conviction decision, and their personal opinion of the guilt of the defendant.
All other questions were randomized in a way that guaranteed the two probability questions and the two likelihood questions were not asked directly following each other.


#### Prolific

Participants were recruited in a similar manner as Study 1 in terms of the representative sample and self-screened jury requirements through Prolific.

## Notes


- Reliability: Questions regarding the consistency of the comparison
  - How often to you think the firearms examiner makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - How often to you think the algorithm makes mistakes?
     - [blank] out of [blank] bullet comparisons
   - If other examiners were asked to make the same bullet comparison, how many do you believe would agree with the firearms examiner
     - [blank] out of [blank] examiners would agree with Smith's results of bullet comparison
   - If the algorithm were re-run on the same bullet comparison, how many times do you believe the algorithm would agree with these results?
       - [blank] out of [blank] runs would agree with the results of the bullet comparison
   <!-- - Credibility: Perhaps questions regarding the ability to testify? Or something on credentials, or comparison to another individual -->
   - Scientificity: can directly compare to each other, or have a non-scientific portion to compare to
     - Did you find the algorithm or the bullet comparison to be more scientific?
     - Compared to the case description, how would you rate the scientificity of the bullet comparison
   - How much would you be willing to bet that the crime scene bullet (did/did not) match Cole's gun?
  
- Clarify if the examiner used the algorithm before or after their initial comparison.
 <!-- - If the algorithm is used first, this would potentially be confounding -->