---
# UNL thesis fields
title: "MEASURING JURY PERCEPTION OF EXPLAINABLE MACHINE LEARNING AND DEMONSTRATIVE EVIDENCE"
author: "Rachel Edie Sparks Rogers"
month: "April"
year: "2024"
location: "Lincoln, Nebraska"
major: "Statistics"
adviser: "Susan Vanderplas"
abstract: |
  Subjective pattern comparison has been subject to increased scrutiny by the courts and by the general public, resulting in an increased interest in pattern comparison algorithms that provide quantitative assessments of similarity for use by forensic scientists. While these algorithms would mark an improvement over current subjective comparison methods, individuals without a statistical background may struggle with the statistical concepts and language necessary for describing algorithmic methods. If algorithms are to be used, examiners must be able to testify about their use in a way that is accessible to the jury. In a series of studies, we conduct an assessment of language and supporting visual aids which might be used to explain bullet matching algorithms. In the initial study, we encountered a response type calibration issue - individuals thought highly of the forensic witness and evidence regardless of experimental conditions, 'maxing out' Likert response scales and leaving us unable to tell if the conditions had any effect. While this study indicated that individuals overall found the testimony to be reliable, credible, and scientific, it did not readily provide information about our question of interest. Additional data from this study was found in the participants’ note pads. Through cleaning sequential notes and designing a method for highlighting study transcripts according to the frequency of collocations in participant notes, we can determine which portions of testimony participants found ‘noteworthy’. We also conducted a study on response types to determine the consistency of participant responses across response types, compare a variety of response types, and determine which response type may be appropriately calibrated for addressing the initial research question of jury perception of algorithms and demonstrative evidence. The response types used in this investigation include the participant's interpretation of the strength of evidence (Likert scale), conviction decision (binary), opinion of guilt (binary), willingness to bet on their opinion of guilt (numeric), probability of guilt (numeric), and chance of guilt/innocence (numeric or multiple choice). The note cleaning, text analysis, and testimony tools we developed throughout this series of experiments will benefit our future research in jury perception, as well as future transcript studies.

acknowledgments: |
  I owe my success to my academic community, family, friends and mentors.
  
  I am deeply indebted to my advisor, Dr. Susan VanderPlas, for constantly encouraging me to grow both personally and academically. I could not have asked for a better advisor.
  
  I would like to express my deep appreciation to my supervisory committee, Dr. Erin Blankenship, Dr. Kent Eskridge, and Dr. Ashley Votruba. Their guidance and support has been invaluable.
  
  Members of the Center for Statistics and Applications in Forensic Evidence have also provided me with vital advice and encouragement over the years when working on this project, and I am thankful that I have been able to be a part of such a welcoming community.
  
  I cannot begin to express my gratitude to my husband and partner, Richy Meleus, for his steadfast support. He has been by my side through all of the ups and downs of my post-secondary education. He also supplied his artistic talent to create courtroom cartoons for this work.
  
  I am extremely grateful for my parents (Jeff Rogers an Sally Sparks) and siblings (Dylan and Betty Rogers), who have always believed in me. In particular, I am thankful to my mom for introducing me to the field of statistics.  I know that they are only a phone call away.
  
  When I joined the Statistics Department at University of Nebraska-Lincoln, I was fortunate enough to make two great friends in my master's degree cohort, Eddie Gomez and Kory Heier. Their companionship was incredibly helpful in my first two years of graduate school. I also appreciate my friend and classmate Sarah Aurit for her support and encouragement.
  
  I am incredibly thankful for my friends, Dr. Emily Robinson and Dr. Alison Kleffner, who have assisted me innumerous times throughout the years. I know that I can always look to them for support and advice.
  
  I would also like to extend my gratitude to my graduate mentors, Dr. Jessica Hauschild and Dr. Kelsey Karnik. Through their mentorship, I have become a better statistician.
  
  This work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.
dedication: |
 Dedicated to my mom, who taught my first statistics course.
# End of UNL thesis fields
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    pandoc_args: --top-level-division=chapter
    keep_tex: yes
    latex_engine: xelatex
    template: template.tex
  huskydown::thesis_gitbook: 
    style: style.css
#  huskydown::thesis_word: default
#  huskydown::thesis_epub: default
urlcolor: black
linkcolor: black
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: bib/apa.csl
# Download your specific csl file and refer to it in the line above.
lot: true
lof: true
header-includes: |
  
#- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r setup, include = F}
options(width = 60)
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 4, 
  out.width = "\\linewidth", dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45)
)
```

```{r include_packages, include = FALSE}
# This chunk ensures that the huskydown package is
# installed and loaded. This huskydown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", 
                   repos = "http://cran.rstudio.com")
if(!require(huskydown))
  devtools::install_github(
    "benmarwick/huskydown"
  )
library(huskydown)

library(readr)
#library(ordinal)
library(tidyr)
library(plyr)
library(dplyr)
#library(gofcat)
library(ggplot2)
#library(forcats)
library(ggmosaic)
require(gridExtra)
library(GGally)
library(forcats)
library(ggpcp)
library(ggblend)
library(knitr)
library(patchwork)
library(grid)
library(jpeg)
```

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

<!-- # Introduction {.unnumbered} -->

# Literature Review {#litreview}

## Introduction

There are many forms of forensic evidence that have been generally accepted for use in court cases as unique and identifying (without proof).
There are even some methods - such as bite mark analysis - that have been used in the courtroom as evidence, but would later be shown to not be scientifically valid [@pcast].
 <!-- [@pcast, 86]. -->
When a defendant's future hangs in the balance of such evidence, we must know that the type of evidence being presented is something that can be relied upon, and that the jury can weigh it appropriately when reaching a conclusion.
One way to assure that these goals are met is through the use of statistical methods in analysis, which allows for the quantification of comparisons and the establishment of accurate error rate calculations.
However, several studies have indicated that these statistical methods are more difficult for jurors to understand.

By testing jury perception in bullet matching, and developing tools for assessing jury perception, we hope to expand what can be learned about jury perception in order to present statistical methods in a way that can be understood and used to make decisions by the wider public.
Bullet matching has a history similar to many areas of forensic pattern evidence, and we have to statistically validate its use before it should be used in the courtroom or treated as reliable evidence.
However, we must also establish that jurors can treat the evidence with appropriate weight.

## Bullet Matching Background

In firearms evidence, there are two pieces of fired evidence that may be evaluated: the cartridge casing and the bullet itself. 
The structure of the bullet is shown in Figure \@ref(fig:structure) [@glrx].
Striation marks are created on the bullet as it travels down the barrel due to rifling, defects, and impurities [@hare2017automatic], while the head of the cartridge casing is marked by the breech face (a part of the gun located at the head of the cartridge case when firing [@FirearmToolmarkIdentificationa]) and aperture sheer (created by the movement of the cartridge case against the hole in the breech face for the firing pin [@cartridgeproc]) [@chapnick2021].
```{r structure, echo=FALSE, fig.cap="Image of bullet structure. 1 indicates the bullet, 2 indicates the cartridge casing, 3 indicates the powder, 4 indicates the head of the cartridge casing, and 5 indicates the primer. Glrx (2021).", fig.cap="Image of bullet structure.", out.width="50%", fig.align="center"}
include_graphics(path = "images/Cartridge_cross_section.png")
```

The practice of bullet matching is based on the concept that rifling in a gun barrel can produce uniquely identifying marks on bullets fired from the gun, due to random variation [@pcast]. 
 <!-- [@pcast, 104].  -->
Rifling is defined as the spiral ridges in a gun barrel that stabilize the bullet, as shown in Figure \@ref(fig:rifling) [@105mmTank2005].
The raised areas are known as lands, while the depressed areas are known as grooves.

```{r rifling, echo=FALSE, fig.cap="Cross section of a gun. The rifling is the spiral pattern of lands (raised portions) and grooves (indented portions). baku13 (2005).", fig.scap="Cross section of a gun.", out.width="50%", fig.align="center"}
include_graphics(path = "images/rifling.jpg")
```

When a bullet is fired, it travels down the gun barrel and marks from the rifling are scratched into the bullet's surface.
This results in indented areas that correspond to the land impressions, as shown in Figure \@ref(fig:fired) [@gremi].

```{r fired, echo=FALSE, fig.cap="Fired bullet. Indented portion correspond to the gun lands. Gremi-ch (2009).", fig.cap="Fired bullet", out.width="50%", fig.align="center"}
include_graphics(path = "images/fired_bullet.jpg")
```

The diagram in Figure \@ref(fig:fireddiagram) from @hare2017automatic demonstrates the structure of these land and groove impressions on a bullet.
As there are multiple lands on a gun barrel, this results in multiple land impressions on a fired bullet.

```{r fireddiagram, echo=FALSE, fig.cap="Diagram of a fired bullet. Hare et al. (2017).", fig.cap="Diagram of a fired bullet.", out.width="50%", fig.align="center"}
include_graphics(path = "images/bulletdiagram.jpg")
```

Striation marks, or scratches in the bullet surface caused by the rifling, can be seen on these land impressions, as shown in Figure \@ref(fig:firedland) (provided by @hare2017automatic). 
Firearms examiners can compare these striation marks across bullets in order to determine if they were fired from the same gun.

```{r firedland, echo=FALSE, fig.cap="Side view of a fired bullet, demonstrating striation marks on the bullet's lands. Hare et al. (2017).", fig.cap="Side view of a fired bullet, demonstrating striation marks on the bullet's lands.", out.width="50%", fig.align="center"}
include_graphics(path = "images/bulletland.jpg")
```

```{r microscope, echo=FALSE, fig.cap="Image of a comparison microscope and aligned bullets", out.width="50%", fig.align="center"}
include_graphics(path = "images/microscope.jpg")
```

Firearms examiners use comparison microscopes, such as Figure \@ref(fig:microscope), to compare these striation marks between two bullets, shown in the top right of the figure.
An early attempt to distinguish guns based on the rifling can be found in two issues of "The Saturday Evening Post" from 1925, in an article entitled “Fingerprinting Bullets", whose title demonstrates an acceptance of fingerprinting by the public that wouldn't be considered foundationally valid by PCAST (President's Council of Advisors on Science and Technology) until an FBI black box study in 2011 [@pcast].
<!-- [@pcast, 91]. -->
Stout describes the wrongful conviction and subsequent exoneration of Stielow, who was accused of murder [@SaturdayEveningPost]. 
At the trial where Stielow was found guilty, an expert for the prosecution stated that there were nine abnormalities on Stielow's gun that corresponded with marks on the fired bullet [@SaturdayEveningPost].
<!-- [@SaturdayEveningPost, 7]. -->
It was later established that the bullet found at the crime scene indicated the gun was missing a land – leaving a distinctive pattern – while the alleged gun showed no such evidence; the alleged gun also showed evidence of not being fired in more than 3 years [@SaturdayEveningPost]. 

While this trial did not require 'uniquely identifying' marks to rule out the alleged weapon (only a mismatch in lands), it motivated Charles Waite to attempt to catalog the manufacturing methods of guns by various companies around the country, with the goal of connecting a gun to its manufacturer. 
Waite soon encountered issues due to the multitude of guns produced abroad, as well as “cheap knock-offs” with no clear record, making it difficult to trace a gun or bullet to its manufacturer [@SaturdayEveningPosta].
While Waite's early attempt to match barrel markings to specific gun brands was focused on the creation of a 'database', more recent firearms identification methods have relied on the direct comparison of bullets, such as comparing fired evidence to a suspected gun.

## Issues in Pattern Analysis

### Subjectivity of Comparisons

A major issue in firearms comparison is the subjectivity of comparisons.
Because there is no objective criteria used to determine what constitutes a 'match' between two samples, experts may view the same evidence and arrive at different conclusions, potentially due to their subjective comparison, differing laboratory procedures, or biases.
Earlier researchers attempted to quantify the bullet matching process through the use of consecutively matching striae, but this process has yet to become standard.

One of the issues discussed in @SaturdayEveningPost still resonates today: disagreement among experts; this can be especially problematic in countries that use the adversarial judicial system, where experts can testify on the side of the prosecution or the defense. 
This could lead to multiple, potentially baised, experts with differing conclusions on the same comparison.
@nationalresearchcouncilusStrengtheningForensicScience2009 states that subjectivity is involved in determining “sufficient agreement” among firearms. 
<!-- (155). -->
This issue is also reflected in @imwinkelried, who suggest that bullet identification is based more on personal experience than on the scientific method. 
In plain terms, there is not a set standard for what constitutes sufficient agreement between compared evidence.

Because there is no quantitative score describing the strength of a bullet match, it is possible for experts to use different criterion when making a conclusion about the same evidence. 
Disagreement among experts concerning strength of evidence can even be found within a single lab, where experts undergo the same training, use the same equipment and operate under the same procedures, as demonstrated by @montani2019. 
This effect was also present in a bullet comparison study at the Netherlands Forensic Institute, where they found that, of 568 comparison conclusions in a study of the peer review process involving 8 examiners, where the secondary examiner based their conclusion on photos provided by the first examiner, there were 100 disagreements between the two examiners with regards to the strength of evidence, including 6 instances where one examiner found the evidence inconclusive, while the other determined a that the bullets originated from the same source [@mattijssenCognitiveBiasesPeer2020].

Research has been conducted to better understand the factors that drive firearms examiners to make their conclusions.
The use of virtual comparisons allowed @chapnick2021 to investigate what examiners considered important when evaluating cartridge cases by including a feature to highlight important aspects of comparisons. 
They found that examiners generally tended to highlight the same features when making match conclusions [@chapnick2021].
<!-- [@chapnick2021, 8]. -->
However, not all examiners who correctly identified the samples used 'irregularly shaped marks' to distinguish the known matches: the number was around 60%-70% [@chapnick2021].
<!-- [@chapnick2021, 6]. -->
This is far from a unanimous consensus on important features for determining a match in this type of firearm evidence.
In bullet comparison, Todd Weller discusses the use of consecutively matching striation marks between two bullets as a bullet matching standard that is used by approximately 25\% of examiners, but this standard is not used by either the Association of Firearm and Toolmark Examiners or the Organization of Scientific Area Committees (administered by the National Institute of Standards and Technology) [@AuimatagiHearing].
<!-- [@AuimatagiHearing, 98]. -->
While standards for forensic sciences, such as firearm analysis, have been created and published by Scientific Working Groups established by the FBI, these standards are voluntary guidelines that are not uniformly enforced [@nationalresearchcouncilusStrengtheningForensicScience2009].
<!-- [@nationalresearchcouncilusStrengtheningForensicScience2009, 202]. -->
Thus, the criteria used by experts in firearms analysis varies between individuals.

Procedural issues may also lead to differing expert opinions across laboratories; one system may not allow an exclusion to be documented if the class characteristics match, while another system may not have such a limitation [@baldwin2014]. 
<!-- [@baldwin2014, 6]. -->
In their study, this led to 45 of 218 examiners labeling all different source comparisons as inconclusive, and 96 examiners labeling none of the comparisons as inconclusive [@baldwin2014].
<!-- [@baldwin2014, 16]. -->
Without a standardized procedure, experts may evaluate the evidence similarly when comparing two bullets, but must draw different conclusions based on the procedures of their laboratory. 
This leads to inconsistencies in results based on location, which are compounded with inconsistencies that may arise due to the subjectivity of bullet comparisons. 

Aside from the difference in evaluation procedures and identification criterion, subjectivity can lead to bias in pattern recognition. 
The need for an unbiased method of evaluation is aptly demonstrated by the case of the Madrid bombing, in which the FBI incorrectly matched a latent fingerprint to an innocent individual, despite the use of a verification process [@stacey]. 
The individual incorrectly identified was Muslim American and had been on an FBI watch list [@kassinForensicConfirmationBias2013]. 
<!-- [@kassinForensicConfirmationBias2013, 42].  -->
@kassinForensicConfirmationBias2013 suggest that this incorrect conclusion may relate to confirmation bias, and outline other contributing factors, such as extraneous details, external pressures, and order of presentation. 
While this example is based on fingerprint analysis, the same issues of subjectivity are present in bullet matching.

@mattijssenCognitiveBiasesPeer2020 studied bias in blind and non-blind peer review.
They found that individuals were significantly more likely to disagree about the strength of evidence in blind peer review (where they were initially unaware of the first examiner's proposed conclusion) than they were in non-blind peer review (where they received the initial examiner's analysis along with the comparison photos).
Of those who disagreed, @mattijssenCognitiveBiasesPeer2020 found that, when it came to discussion between the two examiners to report a final conclusion, the conclusion was more likely to align with the examiner who had the higher perceived professional status (a reporting examiner as opposed to a non-reporting examiner), although the status of the individual was not randomized in this experiment.
@montani2019 discuss the importance of independent verification, and how to present results when experts do not agree.
They suggest that, when experts disagree, a third expert should be consulted, and all conclusions should be documented and presented as evidence.
The use of three experts in the case of disagreement in categorical conclusions is also suggested by @mattijssenCognitiveBiasesPeer2020, although they feel this approach may stifle discussion and peer learning.

While there are issues in subjective bullet matching, there have been attempts to establish quantifiable standards. 
@biasotti1959 suggests that statistical methods can be used in order to determine whether two bullets were fired from the same gun. 
He concludes that evaluation of individual striation marks (or striae) is not sufficient, but counting the number of consecutively matching lines may provide enough information to distinguish between matching and non-matching bullets, in the case of the .38 Special Smith and Wesson revolvers used in this study [@biasotti1959].
<!-- [@biasotti1959, 37 – 47]. -->
@biasotti1959 hoped that such methods may lead to a statistical model for evaluating firearms.
<!-- (47).  -->
The idea of consecutively matching striae (CMS) was addressed again in @bunch2000, but a completely standardized or quantifiable method of examination has yet to be developed. 
According to Weller, consecutively matching striae is still used today by some examiners; however, this process has not been nationally adopted [@AuimatagiHearing]. 
<!-- [@AuimatagiHearing, 98].  -->
@biasotti1959's idea of using consecutively matching lines is used and extended in the bullet matching algorithm developed by @hare2017automatic. 

### Scientific Validity

In recent years, the scientific validity of many forensic science methods have been called into question, as shown in the @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast reports on feature comparison methods.
General concerns in both reports include foundational evidence for scientific validity, general inability to determine error rates for conclusions, and subjectivity in analysis methods.
Some issues outlined by @pcast are the circular nature of the identification guidelines put forth by the Association of Firearm and Tool Mark Examiners (AFTE) and the lack of appropriately designed error rate studies.
<!-- (104).  -->
AFTE defines sufficient agreement as "...the examiner being convinced that the items are extremely unlikely to have a different origin" [@pcast, 104].
Thus, items are classified as having sufficient agreement if they agree sufficiently enough to conclude that they did not come from different sources. 
This guideline is in itself subjective - there are no benchmarks for determining what one means by "extremely unlikely to have a different origin".
Both @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast suggest a move away from subjective methods.
@pcast states the importance of the development of an objective method for firearms comparisons.
<!-- (113).  -->

These reports also highlighted the lack of studies that produced accurate error rates due to several issues, such as the reporting of inconclusive decisions as well as simple design issues [@pcast]. 
<!-- [@pcast, 104 - 112].  -->
For example, @chapnick2021's report does not include inconclusive results as a potential source of error. 
They reported three errors (false positives) in their study, and reported a false negative error rate of 0%, ignoring the wide range of inconclusive decisions.
For participants from the United States and Canada, 38 of 491 comparisons were reported as inconclusive for known matches, while 254 of 693 comparisons were reported as inconclusive for known non-matches [@chapnick2021].
<!-- [@chapnick2021, 6]. -->
The issues of error rate calculation are addressed by @hofmannTreatmentInconclusivesAFTE2021, particularly with regards to whether or not inconclusive decisions should be treated as errors in calculations. <!-- (325) --> 
They suggest to not consider inconclusive decisions as errors for the calculation of examiner error rates, used in the lab setting; but to consider inconclusive decisions as errors for process error rates, used for determining if the evidence is relevant enough for judging the guilt of the suspect. <!-- (343) -->
This distinction would allow an examiner to report an inconclusive finding without a negative reflection on their work, as inconclusive decisions are sometimes necessary.
But it would also provide jurors with relevant information for evaluating the reliability of firearms evidence.

In another evaluation of inconclusive results, @drorMisUseScientific2020 describe different ways in which inconclusive results are produced.
They differentiate between inconclusive decisions based on the amount of evidence present; an examiner reaching an inconclusive decision when there is not sufficient evidence to reach a conclusion would be correct, whereas an examiner reaching an inconclusive decision when there is sufficient evidence to reach a conclusion would be incorrect. <!-- (334) --> 
This definition relies on the hitherto undetermined standard of the quantifiable amount of evidence necessary to reach a conclusive decision.
By failing to distinguish between correct and incorrect inconclusive decisions in research studies, examiners may opt for the inconclusive choice on hard conclusive comparisons, resulting in error rates that are only valid for easy comparisons when inconclusive decisions are not considered an error [@drorMisUseScientific2020].
<!-- [@drorMisUseScientific2020, 336]. -->
@drorMisUseScientific2020 would consider those inconclusive choices to be "incorrect" - there is enough evidence to reach a conclusion.
Similarly, another potential error would be the examiner reaching a conclusive decision when there is not sufficient evidence to reach a conclusion [@drorMisUseScientific2020].
<!-- [@drorMisUseScientific2020, 334]. -->
This delineates conclusion and errors into distinct categories, which are not seen in error rate studies.

Both inconclusive and conclusive results for the same analysis cannot be correct – there either is enough evidence to make a conclusion, or there is not enough evidence to make a conclusion; to say otherwise may deflate actual error rates [@drorMisUseScientific2020].
<!-- [@drorMisUseScientific2020, 334-335]. -->
@drorMisUseScientific2020 indicate that this inconclusive issue is more prominent in studies than in casework, as inconclusive decisions are more common in error rate studies. <!-- (336) -->
@hofmannTreatmentInconclusivesAFTE2021 also found that examiners made far more inconclusive decisions when the ground truth was a non-match compared to a match when using the AFTE range of conclusions.
While the ability to correctly classify inconclusive decisions as correct or incorrect would be useful, this distinction is not commonly made in studies due to the lack of a quantifiable method for separating the two categories, leading to inconclusive decisions lowering the error rate in some studies.

In other studies, it is impossible to calculate an accurate error rate based on the study design - if examiners are asked to compare unknown bullets to multiple known sources in a set, we do not know if the examiner compared each unknown bullet to all of the knowns in order to reach a conclusion, or if they stopped performing comparisons with the unknown bullet once they found the matching known; this limits information on how often examiners correctly identify different source comparisons [@hofmannTreatmentInconclusivesAFTE2021].
As @hofmannTreatmentInconclusivesAFTE2021 state, this study design makes it impossible to calculate error rates for different-source comparisons.

Other issues in the computation of error rates include the use of closed set studies, which result in significantly lower rates of inconclusive decisions and false positives [@pcast]. 
<!-- [@pcast, 109].  -->
In closed set studies, the source gun is always present, and examiners are asked to match a set of known bullets to a set of unknown bullets - allowing for them to use a process of elimination to make matches; conversely, open set studies do not include all source guns [@pcast].
<!-- [@pcast, 108-110]. -->
The Ames Laboratory study is described by @pcast as an appropriate closed set black-box study.
<!-- (111).  -->
These types of analyses are classified as black-box studies because reported match results are based on an examiner's subjective opinion rather than a list of objective steps [@pcast].
<!-- [@pcast, 5]. -->
When comparing closed-set studies to non closed-set studies, @pcast found that closed-set studies had a much lower rate of inconclusive decisions as well as false positives.
<!-- (111). -->

This debate regarding scientific validity continues to this day, as @vanderplas2022 shows. 
In this declaration, they discuss the current issues with error rate studies, in which they conclude that, until valid error rate studies have been conducted, they cannot support the use of firearms analysis in the courtroom [@vanderplas2022].
<!-- [@vanderplas2022, 10]. -->
Others have called to scale back conclusions that attest to “individualization” in the courtroom due to the inability to tie a specific piece of evidence to a specific source, to the exclusion of all other sources [@nationalresearchcouncilusStrengtheningForensicScience2009;@imwinkelried]. 
There has been some resistance to scaling back conclusions, however, as demonstrated in a memo sent out by Jim Agar II, an FBI attorney. 
They stated that less conclusive language would not be truthful on the part of the firearm expert and to ask firearms examiners to use this language would result in perjury [@agarmemo], despite the reliance on subjective methods with unclear error rates.

There has also been some push back against the conclusions of @pcast, as shown in @osacresponse (provided by the Firearms and Toolmarks subcommittee).
According to @davisosac2014, the Organization of Scientific Area Committees (OSAC) subcommittees are "generally composed of 70% practitioners, 20% researchers, and 10% research and development technology partners and providers".
@osacresponse argue that a false positive error rate is not necessary for determining foundational validity, and that closed set error rate studies are fine for calculating error rates.
@osacresponse also suggests that @pcast ignores the importance of peer review in studies that instruct firearms examiners to work alone, and that black box studies may not properly reflect casework.
However, in order to accurately represent casework, blind testing (where the examiner does not know they are being tested) would be needed - but due to logistical issues and case load, this may not be feasible [@pcast].
<!-- [@pcast, 59]. -->
@osacresponse argues that AFTE language is not circular - the basis of a practical impossibility is based on knowledge of best known non-matches conveyed through training. 
Here decisions are based on "knowledge and experience", which does not counteract the subjective nature of this decision making.
Despite these disagreements, @osacresponse also promotes the use of more objective methods.

Legal standards regulate the admissibility of expert testimony in the courtroom.
In 1923, *Frye vs. United States* established a standard of 'general acceptance' in the scientific community for the admissibility of scientific evidence [@nationalresearchcouncilusStrengtheningForensicScience2009].
<!-- [@nationalresearchcouncilusStrengtheningForensicScience2009, 88]. -->
In 1975, the Federal Rule of Evidence 702 instead promoted "...mere 'assistance' to the trier of fact.." as the basis of admissibility, although whether *Frye* or Rule 702 should be used for admissibility was not clarified until *Daubert v. Merrell Dow Pharmaceuticals, Inc.* [@nationalresearchcouncilusStrengtheningForensicScience2009].
This Supreme Court decision sided with Rule 702 for federal courts, with the added caveat that 'evidentiary reliability' should be established; however some states still use the *Frye* standard [@nationalresearchcouncilusStrengtheningForensicScience2009].
<!-- [@nationalresearchcouncilusStrengtheningForensicScience2009, 89-91]. -->
The NRC clarifies that: "The expert's testimony must be grounded in an accepted body of learning or experience in the expert's field, and the expert  must explain how the conclusion is so grounded" [@nationalresearchcouncilusStrengtheningForensicScience2009, 94].
However, this standard of evidence often does not lead to inadmissible determinations, as demonstrated in *United States v. Green*, where Judge Green stated that, while toolmark identification should not be admissible in terms of the *Daubert* ruling, it would be difficult to exclude due to precedent; he did, however, limit the examiner's conclusion language such that the examiner could not testify to a "definitive match" [@nationalresearchcouncilusStrengtheningForensicScience2009, 108].

### Language in the Courtroom

These differing views amongst individuals in the field are represented in @swoffordProbabilisticReportingAlgorithms2022.
@swoffordProbabilisticReportingAlgorithms2022 interviewed 15 individuals with a range of expertise in forensic science and the court system in order to assess their feelings about the use of algorithms and the presentation of probabilistic reporting, as opposed to categorical reporting and traditional analysis methods. 
They found that the interviewed prosecutors supported the use of match terms such as “identification” and “individualization” instead of probabilistic terms, but two of the prosecutors also rejected the use of terms that convey “absolute certainty” (p. 7). 
All participants felt that that examiner testimony should accurately reflect scientific limitations, although there was not a consensus on whether this practice was in fact followed in the courtroom - defense attorneys felt that the examiners usually do not uphold this expectation, while lab managers expressed frustration on the part of the court.
<!-- (p. 19). -->
The scale to which firearm evidence is used in the courtroom is rather contentious, and views may differ based on occupation.

In general, individuals want to guarantee that the information presented in the courtroom is correct and valid, without causing too much imbalance between the limitations and the categorical conclusion.
However, there isn't agreement on where the balancing point between these two factors are - somewhere between giving too much credit with a match of "absolute certainty", and introducing unfounded reasonable doubt by overstating the limitations.
If accurate numerical information on error rates or an objective method of evaluation that can be described to the court were available, it may be possible to have factual statements of limitations and scope of evidence that both sides of the argument would find satisfactory. 
In an investigation on the effect of the inclusion of error rate on either voice or fingerprint evidence, @garrettErrorRatesLikelihood2020 found that the inclusion of an error rate reduced convictions for fingerprint evidence presented categorically, but had no effect in the case of voice comparison (a less established science) or when the fingerprint comparison was presented as a likelihood ratio.
They state, "This suggests that error rate information is particularly important for types of forensic evidence that people may assume are highly reliable." (p. 1206).
Including error rates for evidence methods seen as reliable may call into question some assumptions participants make about the science, resulting in a closer consideration of the facts presented at trial.

Even if the forensic sciences quantify both limitations (in terms of error rates) and conclusion thresholds, there is another important piece of the puzzle that may influence the perceived strength of evidence: the language experts use to present evidence.
While there is much debate over the language used in these conclusions, @garrett2013 found that the specific language used to describe the strength of the match (when a match was present) had little effect on participants’ judgement of guilt. They tested 11 different forms of match language, such as 'individualization', 'match', or 'very likely that the defendant was the source', as well as language that included match conclusions 'bolstered' by certainty or likelihood language [@garrett2013].
<!-- [@garrett2013, 489]. -->
@mcquistonsurrett2009 also found that there was not a significant difference in participants' view of the likelihood the defendant committed the crime when the examiner presented the conclusion as a 'match' compared to presenting the conclusion as 'similar in all microscopic characteristics.'
@garrett2013 suggests that this lack of difference may relate to the overall view of firearms evidence as reliable, meaning that even a weak match simply boils down to being a match in the eyes of the participants.
<!-- (p. 507). -->

They also found some tempering effect of the expert admitting the possibility that their conclusion was made in error.
<!-- (p. 507). -->
Thus, the use of accurate error rates may moderate the reliability of experts for a more considered weighing of evidence.
In fact, in the Ninth Judicial Circuit Court, there are suggested jury instructions regarding the need for juries to treat 'expert' testimony, such as firearms comparisons, in the same manner as other opinion testimony, where they must decide how much weight the testimony should receive [@OpinionEvidence]. 
@OpinionEvidence also suggest to avoid labeling such testimony as expert testimony altogether, since it may cause jurors to give the testimony undue weight.
One way to fix these issues of subjectivity is by developing a more objective method of evaluation.
One objective method introduced to the forensic sciences is bullet matching algorithms, such as the one described in @hare2017automatic.

## Quantitative Methods

### Bullet Matching Algorithm

@hare2017automatic developed a bullet matching algorithm that uses a random forest in order to determine a match score. 
<!-- (p. 2352).  -->
This algorithm can be described as follows:

* The algorithm first takes a 3D scan of the two bullet lands, then identifies a cross section, or a stable area of the bullet land that can be used in comparison. 

* This cross section is used in order to identify the signatures of the bullets, or a 2D line that captures the marks scratched onto the bullet's surface. 

* The shoulders (the raised area on either side of the scratched pattern) must be removed from the cross section. 

* A loess smoothing is applied twice in order to remove the curvature of the bullet. 
What is left is the bullet land’s signature (which will show the peaks and valleys produced by the barrel of the gun). 

* The two signatures are compared using a variety of attributes, such as consecutively matching striae and the relative height of the peaks/valleys, as shown in Figure \@ref(fig:signaturecompare).

```{r signaturecompare, echo=FALSE, fig.cap="Left image depicts two matching signatures, while the right image depicts two non-matching signatures. Generated by the author.", fig.scap="Left image depicts two matching signatures, while the right image depicts two non-matching signatures.", fig.align="center", fig.show="hold", out.width="49%"}

include_graphics(path = c("images/Match_Signatures.jpg", "images/K995_NoMatch_Signatures.jpg"))
```
* A random forest is used to compare the two signatures and generate a match score for the lands. 

* Lands are then aligned across the bullet for the maximal random forest score, and these values are averaged together to produce the match score for the bullet.
Figure \@ref(fig:gridcompare) shows two grids of land match scores computed in two different bullet comparisons.

```{r gridcompare, echo=FALSE, fig.cap="Left image depicts two matching bullets (indicated by high land-to-land match scores in a diagonal formation), while the right image depicts two non-matching bullets. Generated by the author.", fig.scap="Left image depicts two matching bullets, while the right image depicts two non-matching bullets.", fig.align="center", fig.show="hold", out.width="49%"}

include_graphics(path = c("images/F526_Match_SingleGrid.png", "images/K995_NoMatch_SingleGrid.png"))
```

In multiple tests, it was found that the algorithm is able to successfully distinguish between known matches and known non-matches without the use of an inconclusive decision [@vanderplasComparisonThreeSimilarity2020].
<!-- [@vanderplasComparisonThreeSimilarity2020, 10]. -->
However, this process does not eliminate the use of inconclusive decisions.
If the bullet is not fit for algorithmic comparison, then an 'inconclusive' result may still be reached. 
This could be the case for gun types that have not been tested for the algorithm. 
The use of an algorithm allows for quantification of bullet matching, for cases where the algorithm has been verified.

There are some concerns about the explainability of algorithms.
@garrettInterpretableAlgorithmicForensics2023 explain one main issue that may result from "black box" algorithmic methods: the results from the algorithm may not be accurate, and this may be difficult to discern when individuals do not understand the method used.
They instead suggest the use of "glass box" methods, where the algorithm's model is interpretable (people can see how it works, and what information is used to make decisions).
@garrettInterpretableAlgorithmicForensics2023 state that "...interpretability is particularly important in legal settings, where human users of a system... cannot fairly and accurately use what they cannot understand." (p. 4).
This brings up a particularly salient point in jury trials - jurors should understand the pattern comparison process if they are making decisions based on its results.

In a combination of computerized results and human interpretation, @montani2019 suggest presenting the algorithm as a “second independent expert” whose comparison and conclusion can be compared to and presented alongside the examiner's.
Supplementing the decision of the forensic expert with an algorithm would allow for verification of results in two independent methods.
The three laboratory managers interviewed by @swoffordProbabilisticReportingAlgorithms2022 suggested using algorithms as a supplement to the forensic examiner, similar to @montani2019’s suggestion. <!---(p. 6)--->

The three prosecutors varied in their feelings of algorithms in the courtroom: one thought that they would add unnecessary complications, while two suggested that the algorithms may be useful for the forensic expert [@swoffordProbabilisticReportingAlgorithms2022]. 
<!-- [@swoffordProbabilisticReportingAlgorithms2022, 8].  -->
The three defense attorneys, as well as the three judges, also supported the use of algorithms as empirical evidence, while stating concerns of transparency [@swoffordProbabilisticReportingAlgorithms2022]. 
<!-- [@swoffordProbabilisticReportingAlgorithms2022, 10-13].  -->
Three academic scholars supported the use of algorithms, so long as the algorithms can be understood, have been validated, and do not include factors that may cause systematic biases [@swoffordProbabilisticReportingAlgorithms2022]. 
<!-- [@swoffordProbabilisticReportingAlgorithms2022, 16].  -->
@swoffordProbabilisticReportingAlgorithms2022 demonstrates the variety of opinions on algorithm implementation throughout forensic science, although most support the use of algorithms. 

### Reporting Quantitative Results

Quantitative methods of examination can lead to quantitative results.
In this case, the quantitative results are in the form of a match score, but in other cases, results may be presented as a likelihood ratio. 
In many forensic science papers, there are two approaches that are commonly defined as using a "likelihood ratio" approach, although one is truly a Bayes Factor approach [@ommenProblemForensicScience2021].
According to @ommenProblemForensicScience2021, the usual form of "likelihood ratios" in forensic science are written as $\frac{P(E|H_p)}{P(E|H_d)}$, where E is the evidence, $H_p$ is the prosecution's hypothesis, and $H_d$ is the defense's hypothesis; however, there are two different methods for calculating the probability, $P$: either as a Bayes Factor or as a frequentist likelihood ratio. 
<!-- (pp. 348 - 349). -->
@ommenProblemForensicScience2021 states that there is a relationship between the two approaches: "...the Bayes Factor is the expected value of the likelihood ratio function with respect to the posterior distribution of the parameters given the entire collection of evidence generated under the defense model." (p. 351).
For the purposes of this literature review, the use of "likelihood ratio" reflects the source material, which in both cases relies on the equation $\frac{P(E|H_p)}{P(E|H_d)}$.

@EvaluatingLikelihoodRatio proposed a likelihood approach for a cartridge case comparison method that uses congruently matching cells. 
This is a change from the match language usually used by forensic examiners, but these likelihood ratios may have some benefit.
@marquis2016 suggest that the use of a likelihood ratio requires examiners to consider what they are including when weighing the strength of evidence, and provides a value that can be consistent across disciplines.
<!-- (p. 4).  -->
This quantitative approach, even while conducting a subjective evaluation, is meant to prevent examiners from being biased by information provided outside of their direct examination [@bunch2013]. 
<!-- [@bunch2013, 223].  -->
The approach also asks examiners to consider both the null and alternative hypotheses, and may make it less likely for either the examiners or jurors to transpose the conditional, which may happen in the case of reporting probabilities [@evett1998;@marquis2016].
<!-- [@evett1998;@marquis2016, 3]. -->

As an example of transposing the conditional, suppose an examiner were comparing a bullet from the suspect's gun to a bullet recovered from the crime scene.
The examiner may find that there is significant agreement between the two bullets.
They may then conclude that "the probability of seeing such significant agreement between the two bullets *given that another gun fired the bullet at the crime scene* is small".
In this case, the examiner is making a statement regarding the amount of agreement between the two bullets (P(Correspondence|Different Source)).
If the conditional is transposed, the statement would become "the probability that another gun fired the bullet at the crime scene *given the significant agreement between the two bullets* is small" (P(Different Source|Correspondence)).
Here, the conditional part of the statement has been switched, so that the examiner is making a statement regarding the gun being fired at the crime scene instead of a statement with regards to the bullet comparison.
By stating a likelihood ratio, such as "the bullet comparison provides strong support for the proposition that the two bullets came from the same gun rather than the proposition that the two bullets came from different guns", both the null and alternative hypothesis are clearly stated, and the focus of the statement is on the bullet comparison, rather than the gun's presence at the crime scene.

As shown above, the likelihood ratio directly compares the null and alternative hypotheses in a case, which is not accomplished by considering a single probability [@nordgaard2012]. 
<!-- [@nordgaard2012, 6]. -->
Likelihood ratios also allow for the integration of evidence from multiple sources, and individual likelihood ratios may be multiplied together to calculate an overall likelihood ratio [@nordgaard2012;@marquis2016].
@meuwly2017a suggested guidelines for validating both score- and feature-based likelihood ratio methods, so this form of result presentation could be more widely accepted.
These methods included validation criteria for all variables considered in a likelihood ratio, where validation criteria can be considered in several ways: a comparison with current “state of the art” methods, detection error trade-off graphs to measure discriminating power, and performing validation on a validation data set.

### Explainability in the Courtroom

One roadblock in the use of quantitative methods and results, such as likelihood ratios, in the forensic sciences is the necessity to explain such methods to jurors so that they can understand the method and results well enough to make informed decisions in court cases.
@afs2009 stated that opinions and conclusions should be expressed in likelihood ratios when outlining guidelines for forensic experts.
<!-- (p. 163).  -->
Participants in @swoffordProbabilisticReportingAlgorithms2022 expressed concern that probabilistic reporting would be confusing and easily misinterpreted, when compared to the alternative of categorical reporting.
<!-- (p. 18). -->
In order to gauge how quantitative methods are perceived in the courtroom, we can consider fingerprint and DNA analysis.
@garrettComparingCategoricalProbabilistic2018 studied the use of FRStat for fingerprint matching in the courtroom.
The FRStat language presented to the study participants is as follows: "The probability of observing this amount of correspondence is approximately [XXX] times greater when the impressions are made by the same source rather than by different sources" [language from @DFSCLPInformation2018].
@garrettComparingCategoricalProbabilistic2018 used ratios from 10 times greater to 100,000 times greater, and found that the likelihood that the subject committed the crime according to the participants did not change significantly.
This demonstrates that potential jurors may have trouble accurately interpreting statistical results, including likelihood ratios.

In DNA research, @koehler2001 found that participants were more likely to believe the subject was the source of the DNA when presented with a probability instead of a frequency.
When asked about how many individuals would match DNA for a given match proportion in a population of 500,000, 60.7\% of participants who received a frequency and 42.1\% of individuals who received a probability answered correctly [@koehler2001].
<!-- [@koehler2001, 503]. -->
When asking these individuals to determine the guilt or innocence of a suspect, the percentage of participants who correctly interpreted the DNA results seems relatively low.
Jurors are also prone to find evidence to be weaker when frequencies are presented as whole number (1 out of 100,000) compared to a decimal number (0.1 out of 10,000), even though the frequencies represent the same value [@koehlerThinkingLowProbabilityEvents2004].
<!-- [@koehlerThinkingLowProbabilityEvents2004, 544]. -->
These studies show that individuals struggle with interpreting numerical results, and assigning appropriate weight to likelihood ratios.

In addressing these issues of interpretation, attempts have been made to assign a verbal scale to be used alongside a likelihood ratio, such as the European recommendations for reporting forensic science put forth by the European Network of Forensic Science Institutes [@ENFSIGuidelineEvaluative2016].
A sample of phrasing from their recommended scale has been recreated in Table \@ref(tab:enfsi).
Verbal values range from no support to extremely strong support, and correspond to a numerical output. 
This type of scale would present jurors with both the quantitative result as well as a brief interpretation, so they are not solely relying on their own perception of what the likelihood ratio qualitatively means.
It also offers a numerical scale that may encourage consistency across examiner reports.

--------------------------------------------------------------------------------------------------------------------------------
  Likelihood Ratio                                 Verbal Equivalent
------------------------- ------------------------------------------------------------------------------------------------------
  1                         The forensic findings do not support one proposition over the other                    
  
  2 - 10                    The forensic findings provide weak support for the first proposition relative to the alternative                      
  
  10 - 100                  ...provide moderate support for the first proposition rather than the alternative                    
  
  100 - 1,000               ...provide moderately strong support for the first proposition rather than the alternative                    
  
  1,000 - 10,000            ...provide strong support for the first proposition rather than the alternative
  
  10,000 - 1,000,000        ...provide very strong support for the first proposition rather than the alternative
  
  1,000,000 and above       ...provide extremely strong support for the first proposition rather than the alternative
------------------------- ------------------------------------------------------------------------------------------------------
Table: (\#tab:enfsi) Sample language from @ENFSIGuidelineEvaluative2016 (17)

A similar scale is also suggested by @evett1998, with verbal values of “Limited support”, “Moderate support”, “Strong support”, and “Very strong support”.
<!-- (p. 201).  -->
These values approximately correspond to the scale above, but consists of fewer categories.
@marquis2016 suggest against providing the full verbal scale to participants, because participants may use other terms on the scale in order to orient the likelihood ratio in comparison to the full scale.
<!-- (p. 7).  -->
However, this ability to orient relative to other values may assist jurors in accurately judging the strength of evidence presented.

While verbal scales are useful for clarification, individuals may not always interpret them consistently.
For example, @budescu1985a asked participants to rank common probabilistic words (such are "rarely" or "usually") on three separate occasions, finding that individuals typically gave consistent rankings across time points, but there was variation in rankings between individuals.
In an earlier study, @lichtensteinEmpiricalScalingCommon1967 asked individuals to assign numerical probabilities to probabilistic words, finding that eight of the eleven mirror terms (such as “quite likely” and “quite unlikely”) demonstrated asymmetry - positive terms (such as likely) scored lower than their mirrored negative terms (unlikely). For example, "quite likely" and "quite unlikely" had median values of 0.8 and 0.1, respectively [@lichtensteinEmpiricalScalingCommon1967, 564].

@martire2013 studied the use of a verbal scale similar to @ENFSIGuidelineEvaluative2016 in a courtroom setting, in the case of shoe print evidence.
They found a "weak evidence effect", in which findings that provided "weak support" for the proposition that the defendant's shoe left the print resulted in decreased likelihood scores of guilt, compared to likelihood scores collected before the introduction of forensic evidence.
@martire2013 did not find this trend when the evidence was presented numerically as a likelihood ratio; in this case, participants tended to think of the defendant as more guilty after hearing the forensic evidence (as expected).
They did not, however find that the reverse was true: those who received "weak support" for the proposition that the defendant did not leave the shoe print resulted in increased likelihood scores in favor of innocence.
@martire2013 hypothesize that this demonstrates that the participants are engaging in a "criminal justice perspective" by weighting the evidence toward the defendant's innocence (since the burden of proof rests on the prosecution).
This inconsistency of interpretation between the numerical scale and the verbal equivalent may act as evidence against implementation of a solely verbal scale.

Examiners also show evidence of inconsistency in scale interpretation.
@mattijssen2020 asked examiners to use a verbal scale for degree of similarity and support when digitally comparing cartridge cases.
When using a verbal scale for degree of similarity/support, @mattijssen2020 found that the between-subject and within-subject reliability was moderate to high.
<!-- (p. 11).  -->
@mattijssen2020 asked 10 examiners who regularly used likelihood ratios to provide both a verbal degree of support along with a likelihood ratio.
They found that the verbal degrees of support were in general an overestimation when compared to the likelihood ratios.
<!-- (p. 8).  -->
While this is a small non-representative sample, it suggests that verbal scales do not always correspond to quantitative scales across subjects.
@thompsonLayUnderstanding investigated the amount of weight participants gave either DNA or shoe print evidence when it was presented as a likelihood ratio, verbal equivalent, or random match probability.
While individuals updated their response as expected when the strength of evidence was changed in all cases of the DNA condition, participants did not produce significantly different estimates when shoe print evidence was presented as a likelihood ratio or verbal equivalent, but they did produce different estimates in the case of the random match probability.
These results indicate that participants may weigh evidence differently depending on the presentation method.
@thompsonLayUnderstanding hypothesize that the difference between the DNA results and the shoe print results may stem from the perception of DNA analysis as highly scientific.
By combining the quantitative analysis with a verbal translation, it may be possible to present quantitative results in a manner that is understood consistently by laypeople.

This mix of quantitative language alongside more categorical terms is supported by subjects in @swoffordProbabilisticReportingAlgorithms2022. 
Almost all participants expressed concerns about the interpretation of the probabilistic language, and many recommended a combination of both methods [@swoffordProbabilisticReportingAlgorithms2022]. 
The prosecutors interviewed thought that match language was sufficient by itself, and did not wish to complicate testimony with probabilistic language [@swoffordProbabilisticReportingAlgorithms2022].
<!-- [@swoffordProbabilisticReportingAlgorithms2022, 8]. -->
@mcquistonsurrett2009 studied the use of match language versus probabilities with both judges and juries.
Both groups assigned higher probabilities to the defendant committing the crime when presented with qualitative evaluation or a single probability for the hair comparison, as compared to frequency methods of reporting.
@mcquistonsurrett2009 found that participants were more sure of the guilt of the defendant when qualitative language was used as opposed to a subjective probability.
While match language may give jurors more confidence, it could have the effect of shifting the consideration of evidence from the jury (who in a quantitative approach would need to determine whether or not they feel a likelihood ratio is large enough to indicate that the subject is the source) to the forensic expert, who can declare a match.

### Demonstrative Evidence

Aside from result language or scales used by forensics witnesses to convey their findings, another important factor in jury decision making is demonstrative evidence. 
Images can supply helpful context in describing complex forms of analysis, but they may also introduce bias in the form of "truthiness", an official term introduced by comedian Stephen Colbert.
He described truthiness as the quality of something *seeming* true [@WordTruthinessColbert2005]. 
In this segment, Colbert talks about “thinking with your head vs. knowing with your heart”, where some information may feel true, regardless of the facts. 
@bornsteinJuryDecisionMaking2011 found a "truthiness" effect in the courtroom - jurors tend to remember evidence that aligned with their previous beliefs.
<!-- (65).  -->
This concept can apply to images that make people more likely to feel like a statement is true, even when they do not contribute new evidence.
@kellermannTrialAdvocacyTruthiness2013 suggests that the truthiness or "falsiness" of non-probative visual images should be carefully considered before using images in the courtroom "...both to prevent backfire effects and to capitalize on every possible tactic that can be used to persuade jurors..." (p. 40).
While the concept of truthiness can be used to benefit either side in an adversarial justice system, trial outcomes should be based on factual evidence, rather than feelings that evidence is factual.
In the case of images in trials, it is important to balance the benefit of providing additional information to the jury via images without increasing truthiness. 

The impact of photos on an individual's perception can be rather large, as investigated by @cardwellNonprobativePhotosRapidly2016. 
They asked individuals to “give” or “take” food from an animal, represented by a word. 
Subjects were later asked to identify whether or not they gave food to an animal – either accompanied by an image or not. 
Individuals were more likely to say that they gave food to an animal if it was accompanied by an image [@cardwellNonprobativePhotosRapidly2016].
<!-- [@cardwellNonprobativePhotosRapidly2016, 887] -->
They found that images had an effect for positive associations such as giving food, but not negative associations such as taking food.
<!-- [@cardwellNonprobativePhotosRapidly2016, 883]. -->
In this case, the use of images may make it easier for individuals to visualize a scenario (giving food), and thus makes them more likely to remember the event - whether it happened or not.

In a study of perception, @mccabeSeeingBelievingEffect2008 presented a variety of graphics alongside articles relating to cognitive neuroscience.
The graphics all contained the same information, which was already presented the accompanying article.
They found that participants gave higher ratings of scientific reasoning to articles that included a brain image with activated areas, as opposed to a bar chart, topographical brain graphic, or no graphic.
Participants were also more likely to agree with the conclusion of an article when the brain image was present, compared to when it was absent.
Although the information presented in the articles did not change throughout these conditions, the brain image appears to add more "truthiness" to the articles, causing individuals to find them more scientific than articles with other graphics or no graphics.

In the courtroom, studies have been conducted to evaluate the effect of images of the brain.
@gurleyEffectsNeuroimagingBrain2008 studied the use of brain images when arguing the defendant is not guilty by reason of insanity. 
In a study on introduction to psychology university students, they found that the inclusion of images showing a brain lesion increased the odds of the participants finding the defendant not guilty by reason of insanity.
MRI scans were presented with additional information regarding impulse control for the damaged area, so the effect may not be caused by the images themselves but rather by the additional information.
@schweitzerNeuroimagesEvidenceMens2011 investigated whether images of the brain presented alongside expert testimony with regards to a mental disorder effected the participant's verdict of the defendant's mental state.
In this case, no additional information was provided alongside the neuroimage, allowing the effect of the image itself to be studied. 
They found that the inclusion of neuroimages did not significantly effect the judgement of the participants.
The influence of images on individuals remains unclear, and may be situationally dependent.

## Response Methods

When conducting studies that measure participant thoughts, such as the credibility of a witness, reliability of a forensic method, or chance that the defendant committed a crime, the format of response questions themselves can have an impact on the results.
While the overall ideas, or constructs, that we wish to measure remain the same, different response types may provide more or less useful mappings to the concept to be measured [@grovesSurveyMethodology2009].
Verbal scales, such as those used in the ENFSI guidelines described in the last section, are often used to record participant responses.
Likert scales can be used to evaluate several factors, such as the reliability, understanding, and scientific quality of the forensics expert, the algorithm, and the testimony in general.
It is therefore important to ensure that participants' views are accurately recorded with Likert type scales, which can vary in the number of categories used.
Several researchers found that the 7-point scale may perform or represent the participants’ true views better than the 5-point scale [@joshiLikertScaleExplored2015;@finstadResponseInterpolationScale2010]. 
While @grovesSurveyMethodology2009 recommends using a 5 or 7-point Likert scale with a label for every scale point for attitude questions, participants often preferred more categories, such as 7, 9, or 10 [@prestonOptimalNumberResponse2000;@komoritaNumberScalePoints1965]. 
However, @prestonOptimalNumberResponse2000 found that reliability decreased with more than 10 categories. 
This indicates that 7, 9, or 10 point scales should be adequate for reliable responses that accurately represent the participant's views.
In addition, @decastellarnau2018 found that asking participants to respond to specific items (such as reliable - unreliable) is preferred over using a generalized agree - disagree scale in terms of measurement quality, cognitive effort, and bias. 

Other methods have also been used to evaluate participant responses, such as asking participants to give a likelihood ratio, probability, or chance of guilt.
@thompsonJurorsGiveAppropriate2013 asked participants to rate the chance of guilt with values generally on a log 10 scale (ex. 1 in 100; 1 in 1,000); a middle value of a fifty-fifty chance; and extreme values of "Certain to be guilty" and "Impossible that he is guilty".
They compared results from participants to Bayesian conditional probabilities based on provided likelihoods of a match and error rates in order to consider how much weight participants were giving DNA evidence.
Their results mainly showed no significant difference between Bayesian and participant estimates (meaning that the estimates were "in the right ballpark"), with some conditions producing estimates either greater than or less than what was expected.

In another study of the relationship between how examiners present evidence (likelihood ratio, verbal equivalent, or random match probability), @thompsonLayUnderstanding applied the same multiple choice scale for selecting the chance that the defendant committed the crime for half of the participants, while the other half of the participants were asked to supply how many more times likely it was that the defendant was guilty as compared to not guilty, or vice versa (based on a response scale found in @martire2013).
The likelihood statement was worded as follows: "Based on the available evidence I believe it is ___ times more likely that Mr. Kelly is guilty than not guilty." [@thompsonLayUnderstanding].
<!-- [@thompsonLayUnderstanding, 5]. -->
In comparing responses from these two scales, @thompsonLayUnderstanding found that individuals were more likely to give higher estimates on the categorical log scale than when they were asked to provide a likelihood - resulting in the categorical log scale being more consistent with the expected result when using Bayesian methods of calculation.
This indicates an inconsistency in results based on the response method used, so response method must be carefully considered as a part of study design.

## Conclusion

While bullet matching, like many forensic sciences, have been widely accepted, in recent years their scientific validity has been called into question.
These validity issues revolve around both the subjectivity of the bullet matching process and the difficulty in calculating accurate error rates.
These concerns have led to the development of more quantitative methods of analysis, such as bullet matching algorithms.
With these quantitative methods comes the need for quantitative reporting, which may be difficult for the general public to understand.
This concern was raised by several subject matter experts, along with the suggestion that quantitative evidence should be presented alongside a firearms examiner's determination in order to facilitate understanding.
While the use of demonstrative evidence, such as images, may help individuals in interpretation, these images may unintentionally bias jurors.
In order to investigate the impact of quantitative methods in the courtroom, we conducted a series of studies.

In order to ensure that the US justice system is just, we must be confident that the evidence presented in the courtroom is scientifically valid, and that the evidence is presented in a way that gives jurors the ability to appropriately judge its strength. 
This includes the use of accurate error rates as well as quantitative responses.
One way to facilitate error rate calculation and quantitative results is through the use of algorithmic or statistical methods.
These methods must, however, be explained in a manner that is understandable to those without a statistical background, while limiting any of the potentially biasing effects of "truthiness" when using demonstrative evidence.