---
# UNL thesis fields
title: "DISSERTATION TITLE IN ALL CAPS"
author: "Rachel Edie Sparks Rogers"
month: "Month"
year: "Year"
location: "Lincoln, Nebraska"
major: "Statistics"
adviser: "Susan Vanderplas"
abstract: |
  Here is my abstract. *(350 word limit)*
acknowledgments: |
  Thank you to all my people!
dedication: |
  Dedicated to...
# End of UNL thesis fields
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    pandoc_args: --top-level-division=chapter
    keep_tex: yes
    latex_engine: xelatex
    template: template.tex
  huskydown::thesis_gitbook: 
    style: style.css
#  huskydown::thesis_word: default
#  huskydown::thesis_epub: default
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: bib/apa.csl
# Download your specific csl file and refer to it in the line above.
lot: true
lof: true
#header-includes:
#- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r setup, include = F}
options(width = 60)
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 4, 
  out.width = "\\linewidth", dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45)
)
```

```{r include_packages, include = FALSE}
# This chunk ensures that the huskydown package is
# installed and loaded. This huskydown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", 
                   repos = "http://cran.rstudio.com")
if(!require(huskydown))
  devtools::install_github(
    "benmarwick/huskydown"
  )
library(huskydown)
```

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

<!-- # Introduction {.unnumbered} -->

# Literature Review

The practice of bullet matching is based on the concept that rifling in a gun barrel can produce uniquely identifying marks on bullets fired from the gun, due to random variation [@pcast, 104]. 
An early record of scientific bullet matching in the court system can be found in two issues of The Saturday Evening Post from 1925. 
In an article entitled “Fingerprinting Bullets”, Stout describes the wrongful conviction and subsequent exoneration of a person accused of murder [@SaturdayEveningPost]. 
The alleged gun was missing a land – leaving a distinctive pattern – while the bullet showed no such evidence [@SaturdayEveningPost]. 
This trial is set as the motivation for Charles Waite to attempt to start a catalog of the manufacturing methods of guns by various companies around the country, which was soon exacerbated by the multitude of guns produced abroad, as well as “cheap knock-offs” with no clear record [@SaturdayEveningPosta].
Throughout the development of firearm identification, methods have turned from attempting to match barrel markings to specific brands to matching barrels based on a more local bullet to bullet comparison. **Citation for not creating catalog of all bullets? Also issues with using large databases?**

One of the issues listed in the article still resonates with an issue of today – that of disagreement among experts; this can be especially seen in the adversarial judicial system, where experts can testify on the side of the prosecution or the defense [@SaturdayEveningPost, 6]. 
This in turn relates to the subjectivity of the field - there is no quantitative set of conditions for determining the strength of a bullet match, so it is possible for experts to come to different conclusions when evaluating the same evidence. 
Disagreement among experts concerning strength of evidence can also be found within a single lab, as demonstrated by @montani2019. 
In this article, @montani2019 discuss the importance of independent verification, and how to present results when experts do not agree.
They suggest that, when experts disagree, a third expert should be consulted, and all conclusions should be documented and presented as evidence.

There are also procedural issues which may lead to differing expert opinions across laboratories, where one system may not allow an exclusion to be stated if the class characteristics match, while another system may not have such a limitation [@baldwin2014, 6]. 
Without a standardized procedure, experts may reach the same strength of evidence when comparing two bullets, but must draw different conclusions based on the procedures of their laboratory. 
This results in inconsistencies in results based on location, which are compounded with inconsistencies that may arise due to the subjectivity of bullet comparisons. 

Such subjectivity issues led to attempts to quantify bullet matching, as seen in @biasotti1959. 
@biasotti1959 suggests that statistical methods can be used in order to determine whether or not two bullets were fired from the same gun. 
He suggests that evaluation of individual striation marks is not sufficient, but counting the number of consecutively matching lines may provide enough information to distinguish between matching and non-matching bullets, in the case of the .38 Special Smith and Wesson revolvers used in this study [@biasotti1959, 37 – 47].
@biasotti1959 hoped that such methods may lead to a statistical model for evaluating firearms (47). 
The idea of consecutively matching striae was addressed again in @bunch2000, but a completely standardized or quantifiable method of examination has yet to be developed.
According to @AuimatagiHearing, consecutively matching striae is still used today by some examiners (pg 48). However, the use of consecutively matching striae has not been universally adopted. 
@biasotti1959's idea of using consecutively matching lines is used and extended in the bullet matching algorithm developed by @hare2017automatic (2340), indicating that he was on the right track. 

Outside of the attempts to quantify bullet comparisons with consecutively matching striae, research has been conducted to better understand the factors that drive firearms examiners to make their conclusions.
The use of virtual comparisons allowed @chapnick2021 to investigate what examiners considered important when evaluating cartridge cases by including a feature to highlight important aspects of comparisons. 
They found that examiners generally tended to highlight the same features when making match conclusions [@chapnick2021, 8].
However, not all examiners who correctly identified the samples used irregularly shaped marks: the number was around 60%-70% [@chapnick2021, 6].
This is far from a unanimous consensus on important features for determining a match. **There is no legend on their heatmaps, so it is difficult to tell what percentage marked certain areas**


In recent years, the scientific validity of many forensic science methods have been called into question, as shown in @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast on feature comparison methods.
General concerns include foundational evidence for scientific validity, general inability to determine error rates for conclusions, and subjectivity in analysis methods.
@nationalresearchcouncilusStrengtheningForensicScience2009 states that subjectivity is involved in determining “sufficient agreement” among firearms [@nationalresearchcouncilusStrengtheningForensicScience2009, 155].
This issues is also reflected in @imwinkelried, who suggest that bullet identification is based more on personal experience instead of a scientific method. 
In plain terms, there is not a set standard for what constitutes sufficient agreement between bullets.
Some issues outlined by @pcast is the circular nature of AFTE’s identification guidelines and a lack of appropriately designed error rate studies [@pcast, 104]. 
Sufficient agreement is "defined as the examiner being convinced that the items are extremely unlikely to have a different origin" [@pcast, 104].
Thus, items are classified as having sufficient agreement if they agree sufficiently enough to conclude that they did not come from different sources. 
This guideline is in itself subjective - there are no benchmarks for determining what one means by "extremely unlikely to have a different origin".
Due to these issues in firearms comparisons, both @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast suggest a move away from subjective methods. 
@pcast states the importance of the development of an objective method for firearms comparisons (113). 

These reports also highlighted the lack of studies that produced accurate error rates due to a multitude of issues, such as the reporting of inconclusive decisions as well as simple design issues [@pcast, 104 - 112]. 
This debate regarding scientific validity continues to this day, as @vanderplas2022 shows. 
In this declaration, they discuss the current issues with error rate studies, in which they conclude that, until valid error rate studies have been conducted, they cannot support the use of firearms analysis in the courtroom [@vanderplas2022, 10].
For example, @chapnick2021's report does not include inconclusive results as a potential source of error. 
They reported three errors (false positives) in their study, and reported a false negative error rate of 0%, ignoring the wide range of inconclusive decisions.
The issues of error rate calculation are addressed by @hofmannTreatmentInconclusivesAFTE2021, particularly with regards to whether or not inconclusive decisions are treated as errors in calculations (325). 
They suggest to not consider inconclusive decisions errors for the calculation of examiner error rates, used in the lab setting; but to consider inconclusive decisions as errors for process error rates, used for determining if the evidence is relevant enough for judging the guilt of the suspect (343).

@drorMisUseScientific2020 further describe this inconclusive issue as it pertains to the forensic sciences. 
They differentiate between inconclusive decisions based on the amount of evidence; an examiner reaching an inconclusive decision when there is not sufficient evidence to reach a conclusion would be correct, whereas an examiner reaching an inconclusive decision when there is sufficient evidence to reach a conclusion would be incorrect (334). 
By failing to distinguish between correct and incorrect inconclusive decisions, examiners may opt for the inconclusive choice on hard conclusive comparisons, resulting in error rates that are only valid for easy comparisons (336). 
These inconclusive choices would be seen as "incorrect" - there is enough evidence to reach a conclusion.
Similarly, another potential error would be the examiner reaching a conclusive decision when there is not sufficient evidence to reach a conclusion (334). 
Both inconclusive and conclusive results for the same analysis cannot be correct – there either is enough evidence to make a conclusion, or there is not enough evidence to make a conclusion; to say otherwise may deflate actual error rates [@drorMisUseScientific2020, 334-335].
@drorMisUseScientific2020 indicate that this inconclusive issue is more prominent in studies than in casework, as inconclusive decisions are more common in error rate studies (336).
However, this distinction between correct and incorrect decisions are not commonly made in studies, leading to inconclusive decisions contributing lowering the error rate in some studies.
Without a sufficient way to separate correct and incorrect inconclusive decisions, these inconclusives will continue to pose an issue in the accurate computation of error rates.

Other issues in the computation of error rates include the use of closed set studies (the source gun is always present, and examiners are asked to match across two sets - allowing for them to use a process of elimination to make matches), as opposed to open set studies (the source gun may not be present), where closed set studies result in significantly lower fates of inconclusive decisions and false positives [@pcast, 109].
The Ames study is described by @pcast as an appropriate black box study [@baldwin2014, 11]. 
@baldwin2014 found that most errors occurring in their study were produced by a small number of examiners, and that inconclusive decisions were also rather heterogeneous across examiners (16). 

Some sources have called to scale back conclusions that attest to “individualization”, due to the inability to tie a specific piece of evidence to a specific source, to the exclusion of all other sources [@nationalresearchcouncilusStrengtheningForensicScience2009, 7;@imwinkelried]. 
There has been some resistance to scaling back conclusions, however, as demonstrated in a memo sent out by Jim Agar II, an FBI attorney, who stated that less conclusive language would not be truthful on the part of the firearm expert [@agarmemo]. 
This indicates that the scale to which firearm evidence is used in the courtroom is rather contentious, and individuals with different stakes are likely to have different views.
These differing views are represented in @swoffordProbabilisticReportingAlgorithms2022. 
They found that prosecutors interviewed by Swofford and Champod also supported the use of match terms such as “Identification” and “Individualization”, instead of probabilistic terms, but two of the prosecutors also rejected the use of terms that convey “absolute certainty” (7). All participants felt that that examiner testimony should accurately reflect scientific limitations, although there was not a consensus on whether this practice was in fact followed in the courtroom (19).
In general, it seems that individuals want to guarantee that the information presented in the courtroom is accurate and valid, without giving too much weight to either the limitations or the categorical conclusion.
However, there does not appear to be agreement on where the balancing point between these two factors are - somewhere between giving too much credit with a match of "absolute certainty", and introducing unfounded reasonable doubt by overstating the limitations.
If accurate numerical information on error rates, or an objective method of evaluation that can be described to the court were available, it may be possible to have factual statements of limitations that both sides of the argument would find satisfactory.

The development of quantitative limitations and processes could simplify the balancing act to focus on the language used by experts when presenting evidence.
While there is much debate over the language used in these conclusions, @garrett2013 found that the specific language used to describe the strength of the match (when a match was present) had little effect on participants’ judgement of guilt.
@garrett2013 suggests that this lack of difference may relate to the overall view of firearms evidence as reliable, meaning that even a weak match is considered the same as a match (507).
They also found some limiting effect of the expert admitting the possibility that their conclusion was made in error (507).
Thus, perhaps the most important factor in finding a way to accurately portray evidence rests in moderating the reliability of experts through the use of accurate error rates.

**Move paragraph closer to pcast and NRC discussions?**
While many of these changes are suggested due to @pcast and @nationalresearchcouncilusStrengtheningForensicScience2009, the statements of these reports are not without debate. 
There has been some pushback against the conclusions of @pcast, as shown in @osacresponse.
@osacresponse argue that a false positive error rate is not necessary for determining foundational validity, and that closed set error rate studies are fine for calculating error rates.
@osacresponse also suggests that @pcast ignores the importance of peer review in studies that instruct firearms examiners to work alone, and that black box studies may not properly reflect casework.
@osacresponse argues that AFTE language is not circular - the basis of a practical impossibility is based on knowledge of best known non-matches conveyed through training. This again relates to the idea of decisions being based on "knowledge and experience", and does not counteract the subjective nature of this decision making.
Despite these disagreements @osacresponse also promotes the use of more objective methods.

Errors in pattern recognition are not limited to firearms comparisons. 
The need for an unbiased method for these methods is aptly demonstrated by the case of the Madrid bombing, in which the FBI incorrectly matched a latent fingerprint to the incorrect individual [@stacey]. 
@kassinForensicConfirmationBias2013 suggest that this incorrect conclusion may relate to confirmation bias, and outline other contributing factors, such as extraneous details, external pressures, and order of presentation. 
While this example is based on fingerprint analysis, the same issues of subjectivity are present in bullet matching.
One way to fix these issues of subjectivity is by developing a more objective method of evaluation.
One objective method introduced to the forensic sciences is bullet matching algorithms, such as the one described in @hare2017automatic.

This bullet matching algorithm was developed by @hare2017automatic and uses a random forest in order to determine a match score [@hare2017automatic, 2352]. 
The algorithm proposed by @hare2017automatic can be described as follows. 
The algorithm first takes a 3D scan of the two bullet lands, then identifies a cross section. 
This cross section is used in order to identify the signatures of the bullets, where there is a signature for each land. 
First the shoulders must be removed from the cross section. 
Then a loess smoothing is applied twice in order to remove the curvature. 
What is left is the bullet’s signature (which will show the peaks and valleys produced by the barrel of the gun). 
The two signatures are compared using a variety of attributes, such as consecutively matching striae and the relative height of the peaks/valleys.
A random forest is used to compare the two signatures and come up with a score for the land. 
Lands are then aligned across the bullet for the maximal random forest score, and this number would be considered the match score. 
In multiple tests, it was found that the algorithm is able to successfully distinguish between known matches and known non-matches [@vanderplasComparisonThreeSimilarity2020, 10].
The use of an algorithm allows for quantification of bullet matching, for cases where the algorithm has been verified.
**Need More Description Above, Like Chapter 1**

One roadblock in the use of quantitative methods in the forensic sciences is the necessity to explain such methods to jurors so that they can understand the method and results well enough to make informed decisions in court cases.
One quantitative method that is often discussed is likelihood ratios.
@afs2009 stated that opinions and conclusions should be expressed in likelihood ratios when outlining guidelines for forensic experts (163). 
@EvaluatingLikelihoodRatio proposed a likelihood approach for a firearms method that uses congruently matching cells, in order to introduce a quantitative approach. 

Participants in @swoffordProbabilisticReportingAlgorithms2022 expressed concern that probabilistic reporting would be confusing and easily misinterpreted, when compared to the alternative of categorical reporting (18).
In order to gauge how bullet matching algorithms may be perceived in the courtroom, we can turn to other disciplines that use algorithms - such as fingerprint and DNA analysis.
@garrettComparingCategoricalProbabilistic2018 studied the use of FRStat for fingerprint matching in the courtroom.
The FRStat language presented to the participants is as follows: "The probability of observing this amount of correspondence is approximately [XXX] times greater when the impressions are made by the same source rather than by different sources" [language from @DFSCLPInformation2018].
@garrettComparingCategoricalProbabilistic2018 used ratios from 10 times greater to 100,000 times greater, and found that participants' likelihood that the subject committed the crime did not change significantly.
This demonstrates that potential jurors may have trouble in accurately interpreting statistical results, such as likelihood ratios.

In DNA research, @koehler2001 found that participants were more likely to believe the subject was the source of the DNA when presented with a probability instead of a frequency.
When asked about how many individuals would match DNA in a population of 500,000, 60.7\% of participants who received a frequency and 42.1\% of individuals who received a probability answered correctly [@koehler2001, 503].
Here, there is a significant difference at the 0.05 level.
In either case, however, we can see that around half of the individuals were able to answer correctly.
When asking these individuals to determine the guilt or innocence of a suspect, the percentage of participants who correctly interpreted the DNA results is relatively low.
Jurors are also prone to find evidence to be weaker when frequencies are presented as whole number (1 out of 100,000) compared to a decimal number (0.1 out of 10,000), even though the frequencies represent the same value [@koehlerThinkingLowProbabilityEvents2004, 544].
These studies show that individuals struggle with interpreting numerical results, and assigning appropriate weight to likelihood ratios.

In addressing these issues of interpretation, attempts have been made to assign a scale to be used alongside a likelihood ratio.
This can be seen in the European recommendations for reporting forensic science [@ENFSIGuidelineEvaluative2016].
A sample of phrasing from their recommended scale has been recreated in Table \@ref(tab:enfsi).
Verbal values range from no support to extremely strong support, and correspond to a numerical output. 
This type of scale would present jurors with both the quantitative result as well as a brief interpretation, so they are not solely relying on their own perception of what the likelihood ratio qualitatively means.

--------------------------------------------------------------------------------------------------------------------------------
  Likelihood Ratio                                 Verbal Equivalent
------------------------- ------------------------------------------------------------------------------------------------------
  1                         The forensic findings do not support one proposition over the other                    
  
  2 - 10                    The forensic findings provide weak support for the first proposition relative to the alternative                      
  
  10 - 100                  ...provide moderate support for the first proposition rather than the alternative                    
  
  100 - 1,000               ...provide moderately strong support for the first proposition rather than the alternative                    
  
  1,000 - 10,000            ...provide strong support for the first proposition rather than the alternative
  
  10,000 - 1,000,000        ...provide very strong support for the first proposition rather than the alternative
  
  1,000,000 and above       ...provide extremely strong support for the first proposition rather than the alternative
------------------------- ------------------------------------------------------------------------------------------------------
Table: (\#tab:enfsi) Sample language from @ENFSIGuidelineEvaluative2016 (pg 17)

A similar scale is also suggested by @evett1998, with verbal values of “Limited support”, “Moderate support”, “Strong support”, and “Very strong support” (201). 
These values approximately correspond to the scale above, but consists of fewer categories.

While the use of quantitative results has its own merits in terms of consistency, these scales may also benefit forensic examiners.
@marquis2016 suggest that the use of a likelihood ratio requires examiners to consider what they are including when weighing the strength of evidence, and provides a value that can be consistent across disciplines (4). 
This quantitative approach, even while conducting a subjective evaluation, is meant to prevent examiners from being biased by information provided outside of their direct examination [@bunch2013, 223]. 
The approach also asks examiners to consider both the null and alternative hypotheses, and may make it less likely for them to transpose the conditional [@marquis2016, 3;@evett1998]. **Define transposing the conditional and why this is a concern**
@nordgaard2012 state that a likelihood ratio is necessary in order to compare the null and alternative hypotheses in a case, which is not accomplished by only considering a single probability (6). 

Due to the difficulty interpreting a likelihood ratio, words can be used instead. 
However, verbal scales which may be used can suffer from inconsistency in interpretation. 
For example, @budescu1985a asked individuals to assign probabilities to commonly used probabilistic words, such as “likely”, “unlikely”, “rarely”, and “never”. 
Subjects were divided into groups, so that they would not receive paired words such as “unlikely” and “likely” (394). 
They cite Lichtenstein and Newman, who found that the terms “quite likely” and “quite unlikely” had median values of 0.8 and 0.1, respectively; this indicates a lack of symmetry in the interpretation of some probabilistic terms (392). 
@marquis2016 suggest against providing the full verbal scale to participants, because participants may use other terms on the scale in order to orient the likelihood ratio in comparison to the full scale (7). 
When using a verbal scale for degree of similarity/support, @mattijssen2020. found that the between-subject and within-subject reliability was moderate to high (11). 
However, for the 10 examiners in the study who regularly use likelihood ratios, @mattijssen2020 found that the degree of support were in general an overestimation (8). 
@meuwly2017a suggested guidelines for validating likelihood ratio methods, which included validation criteria for all variables considered in a likelihood ratio, where validation criteria can be considered in several ways: a comparison with current “state of the art” methods, detection error tradeoff graphs to measure discriminating power, and performing validation on a validation dataset.

This leads us to the next topic: combining the examiner’s opinion with the algorithm. 
In @montani2019, they suggest presenting the algorithm as a “Second independent expert”. 
@swoffordProbabilisticReportingAlgorithms2022 interviewed 15 individuals with a range of expertise in forensic science and the court system in order to assess their feelings about the use of algorithms and the presentation of probabilistic reporting, as opposed to categorical reporting and traditional analysis methods. 
The three laboratory managers interviewed suggested using algorithms as a supplement to the forensic examiner [@swoffordProbabilisticReportingAlgorithms2022, 6]. 
This suggestion is similar to @montani2019’s suggestion to view algorithms as another expert. 
The three prosecutors varied in their feelings of algorithms in the courtroom: one thought that they would add unnecessary complications, while two suggested that the algorithms may be useful for the forensic expert [@swoffordProbabilisticReportingAlgorithms2022, 8]. 
The three defense attorneys, as well as the three judges, also supported the use of algorithms as empirical evidence, while stating concerns of transparency [@swoffordProbabilisticReportingAlgorithms2022, 10-13]. 
Three other academic scholars also supported the use of algorithms, so long as the algorithms can be understood, have been validated, and do not include factors that may cause systematic biases [@swoffordProbabilisticReportingAlgorithms2022, 16]. 
As can be seen, there are various opinions of throughout the field, in terms of algorithms. 
Several individuals have indicated the use of the algorithm as a supplement to the forensic expert, instead of as a replacement to the examiner.
This use of both the expert and the algorithm is the approach used in our study.

By combining the quantitative analysis with a verbal translation, it may be possible to present quantitative results in a manner that is understood by laypeople. 
This is kind of what we are looking at with the algorithm, where we combine the examiner’s analysis with the algorithm’s numbers, in the hope of presenting even stronger (and perhaps less biased) evidence. 
Another aspect of this, however, is how the match language is presented. 
Using our current standards, we went with the AFTE guidelines. 
These have their own issues, as addressed in the @pcast report. 
This mix of algorithm and examiner would also seem to be supported by @swoffordProbabilisticReportingAlgorithms2022, where they interviewed 15 people and asked them about opting for clearer probabilistic language over match language. 
They mostly all expressed concerns about the interpretation of the probabilistic language, and many recommended a combination of both methods approach [@swoffordProbabilisticReportingAlgorithms2022]. 
The prosecutors interviewed thought that match language was sufficient by itself, and didn’t see the need to complicate things with probabilistic language [@swoffordProbabilisticReportingAlgorithms2022, 8].
@mcquistonsurrett2009 studied the use of match language versus probabilities. 
They found that participants preferred match language over probabilities when determining the guilt of the defendant, but experts indicated that match language may be misleading [@mcquistonsurrett2009].

The other really important topic for this research is, of course, demonstrative evidence. 
Images can affect what people believe. 
This is known as “truthiness” and is explored by Stephen Colbert in 2005 [@WordTruthinessColbert2005]. 
In this video, Colbert talks about “thinking with your head vs. knowing with your heart”. 
@bornsteinJuryDecisionMaking2011 also found this effect in the courtroom, as they state that jurors tend to remember evidence that align with their previous beliefs (65). 
So when applying this concept to images, we would be saying that images make people feel like what is being said is true, even if the images do not provide any additional evidence. 
In our study, we need to balance the benefit of providing additional information to the jury without increasing “truthiness”. 
The impact of photos can be rather large, as investigated by @cardwellNonprobativePhotosRapidly2016. 
They asked individuals to “give” or “take” food from an animal, represented by a word; subjects were later asked to identify whether or not they gave food to an animal – either accompanied by an image or not. 
Individuals were more likely to say that they gave food to an animal if it were accompanied by an image [@cardwellNonprobativePhotosRapidly2016, 887]. 
They found that this appeared to be true for positive associations, but not negative associations [@cardwellNonprobativePhotosRapidly2016, 883].

In order to evaluate how participants feel about several factors, such as the reliability, understanding, and scientific quality of the forensics expert, the algorithm, and the testimony in general, several Likert scales were implemented. 
In terms of Likert scales, several researchers found that the 7-point scale may perform better or represent the participants’ true views than the 5-point scale [@joshiLikertScaleExplored2015;@finstadResponseInterpolationScale2010]. 
Participants often preferred more categories, such as 7, 9, or 10 [@prestonOptimalNumberResponse2000;@komoritaNumberScalePoints1965]. 
@prestonOptimalNumberResponse2000 found that reliability decreased with more than 10 categories. 
For our study, we use between a 7 and a 9 point scale, which appears to be acceptable.
