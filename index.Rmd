---
# UNL thesis fields
title: "DISSERTATION TITLE IN ALL CAPS"
author: "Rachel Edie Sparks Rogers"
month: "Month"
year: "Year"
location: "Lincoln, Nebraska"
major: "Statistics"
adviser: "Susan Vanderplas"
abstract: |
  Here is my abstract. *(350 word limit)*
acknowledgments: |
  Thank you to all my people!
dedication: |
  Dedicated to...
# End of UNL thesis fields
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    pandoc_args: --top-level-division=chapter
    keep_tex: yes
    latex_engine: xelatex
    template: template.tex
  huskydown::thesis_gitbook: 
    style: style.css
#  huskydown::thesis_word: default
#  huskydown::thesis_epub: default
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: bib/apa.csl
# Download your specific csl file and refer to it in the line above.
lot: true
lof: true
#header-includes:
#- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r setup, include = F}
options(width = 60)
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 4, 
  out.width = "\\linewidth", dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45)
)
```

```{r include_packages, include = FALSE}
# This chunk ensures that the huskydown package is
# installed and loaded. This huskydown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", 
                   repos = "http://cran.rstudio.com")
if(!require(huskydown))
  devtools::install_github(
    "benmarwick/huskydown"
  )
library(huskydown)
```

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters. -->

<!-- # Introduction {.unnumbered} -->

# Literature Review

The practice of bullet matching is based on the concept that rifling in a gun barrel can produce uniquely identifying marks on bullets fired from the gun, due to random variation [@pcast, 104]. 
An early record of scientific bullet matching in the court system can be found in two issues of The Saturday Evening Post from 1925. 
In an article entitled “Fingerprinting Bullets”, Stout describes the wrongful conviction and subsequent exoneration of a person accused of murder [@SaturdayEveningPost]. 
The alleged gun was missing a land – leaving a distinctive pattern – while the bullet showed no such evidence [@SaturdayEveningPost]. 
This trial is set as the motivation for Charles Waite to attempt to start a catalog of the manufacturing methods of guns by various companies around the country, which was soon exacerbated by the multitude of guns produced abroad, as well as “cheap knock-offs” with no clear record [@SaturdayEveningPosta].
One of the issues listed in the article still resonates with an issue of today – that of disagreement among experts; this can be especially seen in the adversarial judicial system, where experts can testify on the side of the prosecution or the defense [@SaturdayEveningPost, 6]. 
This in turn relates to the subjectivity of the field. Disagreement among experts concerning strength of evidence can also be found within a single lab, as demonstrated by @montani2019. 
In this article, @montani2019 discuss the importance of independent verification, and how to present results when experts do not agree. 
This issue is also seen across different laboratories, where one system may not allow an exclusion to be stated if the class characteristics match, while another system may not have such a limitation [@baldwin2014, 6]. 

Subjectivity issues led to attempts to quantify bullet matching, as seen in @biasotti1959. 
@biasotti1959 suggests that statistical methods can be used in order to determine whether or not two bullets were fired from the same gun. 
He suggests that evaluation of individual striation marks is not sufficient, but counting the number of consecutively matching lines may provide enough information to distinguish between matching and non-matching bullets, in the case of the .38 Special Smith and Wesson revolvers used in this study [@biasotti1959, 37 – 47].
@biasotti1959 hoped that such methods may lead to a statistical model for evaluating firearms (47). 
This idea of using consecutively matching lines is used and extended in the bullet matching algorithm [@hare2017automatic, 2340]. 
The idea of Consecutively Matching Striae was addressed again in @bunch2000. 
I find it interesting that this article was published in 2000, yet there appears to be no progress since 1959 toward creating a completely standardized or quantifiable method of examination. 
According to @AuimatagiHearing, consecutively matching striae is still used today by some examiners (pg 48). 
The use of virtual comparisons allowed @chapnick2021 to investigate what examiners considered important when evaluating cartridge cases. 
They found that examiners generally tended to highlight the same features when making match conclusions [@chapnick2021, 8].

In recent years, the scientific validity of many forensic science methods have been called into question, as shown in @nationalresearchcouncilusStrengtheningForensicScience2009 and @pcast on feature comparison methods. @nationalresearchcouncilusStrengtheningForensicScience2009 mentions that subjectivity is involved in determining “sufficient agreement” among firearms [@nationalresearchcouncilusStrengtheningForensicScience2009, 155]. 
Some issues outlined by @pcast is the circular nature of AFTE’s identification guidelines and a lack of appropriately designed error rate studies [@pcast, 104 – 112]. 
Similar to @nationalresearchcouncilusStrengtheningForensicScience2009, @pcast suggests a move away from subjective methods, by discussing the importance of the development of an objective method for firearms comparisons [@pcast, 113]. 
These reports pointed out the lack of scientific validity found in the forensic sciences in general, as well as issues with firearms examination specifically. 
@imwinkelried suggests that bullet identification is based more on personal experience instead of a scientific method. 
These reports highlighted the lack of studies that produced accurate error rates, due to a multitude of issues, such as the reporting of inconclusives as well as simple design issues [@pcast, 104 - 112]. 
There has also been a call to scale back conclusions that attest to “individualization”, due to the inability to tie a specific piece of evidence to a specific source, to the exclusion of all other sources [@nationalresearchcouncilusStrengtheningForensicScience2009, 7;@imwinkelried]. 
There has been some resistance to scaling back conclusions, however, as demonstrated in a memo sent out by Jim Agar II, an FBI attorney, who stated that less conclusive language would not be truthful on the part of the firearm expert[@agarmemo]. 
Prosecutors interviewed by Swofford and Champod also supported the use of match terms such as “Identification” and “Individualization”, instead of probabilistic terms, but two of the prosecutors also rejected the use of terms that convey “absolute certainty” [@swoffordProbabilisticReportingAlgorithms2022, 7]. 
@garrett2013 found that the language used to describe the strength of the match had little effect on participants’ judgement of guilt.

The issues of error rate calculation are addressed by @hofmannTreatmentInconclusivesAFTE2021, particularly in relation to how inconclusive values are reported (325). 
@drorMisUseScientific2020 further describe this inconclusive issue as it pertains to the forensic sciences. 
They differentiate between inconclusive decisions based on the amount of evidence; an examiner reaching an inconclusive decision when there is not sufficient evidence to reach a conclusion would be correct, whereas an examiner reaching an inconclusive decision when there is sufficient evidence to reach a conclusion would be incorrect (334). 
By failing to distinguish between correct and incorrect inconclusive decisions, examiners may opt for the inconclusive choice on hard conclusive comparisons, resulting in error rates that are only valid for easy comparisons (336). 
Similarly, another potential error would be the examiner reaching a conclusive decision when there is not sufficient evidence to reach a conclusion (334). 
Both inconclusive and conclusive results for the same analysis cannot be correct – there either is enough evidence to make a conclusion, or there is not enough evidence to make a conclusion; to say otherwise may deflate actual error rates [@drorMisUseScientific2020, 334-335].
@drorMisUseScientific2020 indicate that this inconclusive issue is more prominent in studies than in casework, as inconclusive decisions are more common in error rate studies (336).

There has been some pushback against the conclusions of @pcast, as shown in @osacresponse. 
This debate regarding scientific validity continues, as @vanderplas2022 shows. 
In this declaration, they discuss the current issues with error rate studies, in which they conclude that, until valid error rate studies have been conducted, they cannot support the use of firearms analysis in the courtroom [@vanderplas2022, 10]. 
There is also an issue with there being no national standard for labs, and the fact that the analysis is in itself subjective. 
The Ames study is described by @pcast as an appropriate black box study [@baldwin2014, 11]. 
@baldwin2014 found that most errors occurring in their study were produced by a small number of examiners, and that inconclusive decisions were also rather heterogeneous across examiners (16). 
Errors in pattern recognition are not limited to forensics, and the need for an unbiased method for these methods is aptly demonstrated by the case of the Madrid bombing, in which the FBI incorrectly matched a latent fingerprint to the incorrect individual [@stacey]. @kassinForensicConfirmationBias2013 suggest that this incorrect conclusion may relate to confirmation bias, and outline other contributing factors, such as extraneous details, external pressures, and order of presentation. 
While this example is based on fingerprint analysis, the same issues of subjectivity can be seen in bullet matching. One objective method introduced to the forensic sciences is a bullet matching 23, described in @hare2017automatic.

This bullet matching algorithm was developed by @hare2017automatic and uses a random forest in order to determine a match score [@hare2017automatic, 2352]. 
The algorithm proposed by @hare2017automatic can be described as follows: The algorithm first takes a 3D scan of the two bullets, then identifies a cross section. 
This cross section is used in order to identify the signatures of the bullets, where there is a signature for each land. 
First the shoulders must be removed from the cross section. Then a loess smoothing is applied twice in order to remove the curvature. 
What is left is the bullet’s signature (which will show the peaks and valleys produced by the barrel of the gun). 
The two signatures are compared using a variety of attributes, such as consecutively matching straie and the relative height of the peaks/valleys.
A random forest is used to compare the two signatures and come up with a score for the land. 
Lands are then aligned across the bullet for the maximal random forest score, and this number would be considered the match score. 
In multiple tests, it was found that the algorithm is able to successfully distinguish between known matches and known non-matches [@vanderplasComparisonThreeSimilarity2020, 10]. 
The use of an algorithm allows for quantification of bullet matching.  

The main issue of presenting a quantitative method to jurors is making sure that the jurors understand the method and results well enough to make informed decisions in a presented case. 
Algorithms can already be found in several other realms of forensic science in the courtroom, such as FRStat used in fingerprint analysis. The use of FRStat in the courtroom was studied by @garrettComparingCategoricalProbabilistic2018. 

Likelihood ratios are often used to present quantitative evidence. 
In fact, @afs2009 stated that opinions and conclusions should be expressed in likelihood ratios when outlining guidelines for forensic experts (163). 
@EvaluatingLikelihoodRatio proposed a likelihood approach for a firearms method that uses congruently matching cells, in order to introduce a quantitative approach. 
The unfortunate issue with likelihoods is that people often struggle to understand and distinguish between various levels of likelihood, so their understanding is not great (SOURCE). 
Attempts have been made to assign a scale to be used alongside a likelihood ratio, and this can be seen in the European recommendations for reporting forensic science [@ENFSIGuidelineEvaluative2016]. 
@evett1998 suggests a likelihood ratio scale as well, with verbal values of “Limited support”, “Moderate support”, “Strong support”, and “Very strong support” (201). 
It is suggested that the use of a likelihood ratio requires examiners to consider what they are including when weighing the strength of evidence, and provides a value that can be consistent across disciplines [@marquis2016, 4]. 
This quantitative approach, even while conducting a subjective evaluation, is meant to prevent examiners from being biased by information provided outside of their direct examination [@bunch2013, 223]. 
It also asks examiners to consider both the null and alternative hypotheses, and may make it less likely for them to transpose the conditional [@marquis2016, 3;@evett1998.]. 
@nordgaard2012 state that a likelihood ratio is necessary in order to compare the null and alternative hypotheses in a case, which is not accomplished by only considering a single probability (6). 

Due to the difficulty interpreting a likelihood ratio, words can be used instead. 
However, verbal scales which may be used can suffer from inconsistency in interpretation. 
For example, @budescu1985a asked individuals to assign probabilities to commonly used probabilistic words, such as “likely”, “unlikely”, “rarely”, and “never”. 
Subjects were divided into groups, so that they would not receive paired words such as “unlikely” and “likely” (394). 
They cite Lichtenstein and Newman, who found that the terms “quite likely” and “quite unlikely” had median values of 0.8 and 0.1, respectively; this indicates a lack of symmetry in the interpretation of some probabilistic terms (392). 
@marquis2016 suggest against providing the full verbal scale to participants, because participants may use other terms on the scale in order to orient the likelihood ratio in comparison to the full scale (7). 
When using a verbal scale for degree of similarity/support, @mattijssen2020. found that the between-subject and within-subject reliability was moderate to high (11). 
However, for the 10 examiners in the study who regularly use likelihood ratios, @mattijssen2020 found that the degree of support were in general an overestimation (8). 
@meuwly2017a suggested guidelines for validating likelihood ratio methods, which included validation criteria for all variables considered in a likelihood ratio, where validation criteria can be considered in several ways: a comparison with current “state of the art” methods, detection error tradeoff graphs to measure discriminating power, and performing validation on a validation dataset.

This leads us to the next topic: combining the examiner’s opinion with the algorithm. 
In @montani2019, they suggest presenting the algorithm as a “Second independent expert”. 
@swoffordProbabilisticReportingAlgorithms2022 interviewed 15 individuals with a range of expertise in forensic science and the court system in order to assess their feelings about the use of algorithms and the presentation of probabilistic reporting, as opposed to categorical reporting and traditional analysis methods. 
The three laboratory managers interviewed suggested using algorithms as a supplement to the forensic examiner [@swoffordProbabilisticReportingAlgorithms2022, 6]. 
This suggestion is similar to @montani2019’s suggestion to view algorithms as another expert. 
The three prosecutors varied in their feelings of algorithms in the courtroom: one thought that they would add unnecessary complications, while two suggested that the algorithms may be useful for the forensic expert [@swoffordProbabilisticReportingAlgorithms2022, 8]. 
The three defense attorneys, as well as the three judges, also supported the use of algorithms as empirical evidence, while stating concerns of transparency [@swoffordProbabilisticReportingAlgorithms2022, 10-13]. 
Three other academic scholars also supported the use of algorithms, so long as the algorithms can be understood, have been validated, and do not include factors that may cause systematic biases [@swoffordProbabilisticReportingAlgorithms2022, 16]. 
As can be seen, there are various opinions of throughout the field, in terms of algorithms. 
Several individuals have indicated the use of the algorithm as a supplement to the forensic expert, instead of as a replacement to the examiner.
This use of both the expert and the algorithm is the approach used in our study.

By combining the quantitative analysis with a verbal translation, it may be possible to present quantitative results in a manner that is understood by laypeople. 
This is kind of what we are looking at with the algorithm, where we combine the examiner’s analysis with the algorithm’s numbers, in the hope of presenting even stronger (and perhaps less biased) evidence. 
Another aspect of this, however, is how the match language is presented. 
Using our current standards, we went with the AFTE guidelines. 
These have their own issues, as addressed in the @pcast report. 
This mix of algorithm and examiner would also seem to be supported by @swoffordProbabilisticReportingAlgorithms2022, where they interviewed 15 people and asked them about opting for clearer probabilistic language over match language. 
They mostly all expressed concerns about the interpretation of the probabilistic language, and many recommended a combination of both methods approach [@swoffordProbabilisticReportingAlgorithms2022]. 
The prosecutors interviewed thought that match language was sufficient by itself, and didn’t see the need to complicate things with probabilistic language [@swoffordProbabilisticReportingAlgorithms2022, 8].
@mcquistonsurrett2009 studied the use of match language versus probabilities. 
They found that participants preferred match language over probabilities when determining the guilt of the defendant, but experts indicated that match language may be misleading [@mcquistonsurrett2009].

The other really important topic for this research is, of course, demonstrative evidence. 
Images can affect what people believe. 
This is known as “truthiness” and is explored by Stephen Colbert in 2005 [@WordTruthinessColbert2005]. 
In this video, Colbert talks about “thinking with your head vs. knowing with your heart”. 
@bornsteinJuryDecisionMaking2011 also found this effect in the courtroom, as they state that jurors tend to remember evidence that align with their previous beliefs (65). 
So when applying this concept to images, we would be saying that images make people feel like what is being said is true, even if the images do not provide any additional evidence. 
In our study, we need to balance the benefit of providing additional information to the jury without increasing “truthiness”. 
The impact of photos can be rather large, as investigated by @cardwellNonprobativePhotosRapidly2016. 
They asked individuals to “give” or “take” food from an animal, represented by a word; subjects were later asked to identify whether or not they gave food to an animal – either accompanied by an image or not. 
Individuals were more likely to say that they gave food to an animal if it were accompanied by an image [@cardwellNonprobativePhotosRapidly2016, 887]. 
They found that this appeared to be true for positive associations, but not negative associations [@cardwellNonprobativePhotosRapidly2016, 883].

In order to evaluate how participants feel about several factors, such as the reliability, understanding, and scientific quality of the forensics expert, the algorithm, and the testimony in general, several Likert scales were implemented. 
In terms of Likert scales, several researchers found that the 7-point scale may perform better or represent the participants’ true views than the 5-point scale [@joshiLikertScaleExplored2015;@finstadResponseInterpolationScale2010]. 
Participants often preferred more categories, such as 7, 9, or 10 [@prestonOptimalNumberResponse2000;@komoritaNumberScalePoints1965]. 
@prestonOptimalNumberResponse2000 found that reliability decreased with more than 10 categories. 
For our study, we use between a 7 and a 9 point scale, which appears to be acceptable.
